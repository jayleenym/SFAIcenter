{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "import tools.ProcessFiles as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf.get_filelist(1, data_path='/Users/jinym/Library/CloudStorage/OneDrive-á„€á…¢á„‹á…µá†«/á„ƒá…¦á„‹á…µá„á…¥L/selectstar/data/FIN_workbook')\n",
    "\n",
    "FINAL_DATA_PATH = '/Users/jinym/Library/CloudStorage/OneDrive-á„€á…¢á„‹á…µá†«/á„ƒá…¦á„‹á…µá„á…¥L/selectstar/data/FIN_workbook'\n",
    "json_files = []\n",
    "for root, _, files in os.walk(FINAL_DATA_PATH):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\") and ('merged' not in f):\n",
    "        #  and ('Lv5' not in root):\n",
    "            json_files.append(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple, short, essay = [],[],[]\n",
    "\n",
    "for file in json_files: \n",
    "    origin = json.load(open(file, 'r', encoding='utf-8'))\n",
    "    for qna in origin:\n",
    "        if len(qna['qna_data']['description']['answer']) > 0:\n",
    "            if qna.get('qna_type') == \"multiple-choice\":\n",
    "                multiple.append(qna)\n",
    "            elif qna.get('qna_type') == \"short-answer\":\n",
    "                short.append(qna)\n",
    "            elif qna.get('qna_type') == \"essay\":\n",
    "                essay.append(qna)\n",
    "# data_dir = os.path.join(base_dir,'Lv2', 're')\n",
    "# origin = json.load(open(os.path.join(base_dir, 'merged_qna_set.json'), 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiple), len(short), len(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_multiple = random.sample(multiple, 100)\n",
    "\n",
    "from tools.ProcessQnA import replace_tags_in_qna_data\n",
    "\n",
    "for qna in random_multiple:\n",
    "    if len(qna.get('additional_tags_found', [])) > 0:\n",
    "        print(\"ì›ë³¸\", qna.get('qna_data'))\n",
    "        result_qna_data = replace_tags_in_qna_data(qna.get('qna_data'), qna.get('additional_tag_data', []))\n",
    "        # print(\"ì²˜ë¦¬ ì¤‘\", result_qna_data)\n",
    "        qna['qna_data'] = result_qna_data\n",
    "        print(\"ì²˜ë¦¬ëœ\", qna.get('qna_data'))\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Iterable, Set\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 0) ë¡œê¹… ì„¤ì •\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('evaluation.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "# -----------------------------\n",
    "CIRCLED_MAP = {\"â‘ \":\"1\",\"â‘¡\":\"2\",\"â‘¢\":\"3\",\"â‘£\":\"4\",\"â‘¤\":\"5\"}\n",
    "\n",
    "def normalize_option_text(s: str) -> str:\n",
    "    \"\"\"ì„ ì§€ ì•ì— ë¶™ì€ â‘ ~â‘¤, 1), (1), 1. ë“± ë²ˆí˜¸ í‘œê¸°ë¥¼ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë‚¨ê¹€.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    # â‘ ~â‘¤ ì œê±°\n",
    "    s = re.sub(r\"^\\s*[â‘ -â‘¤]\\s*\", \"\", s)\n",
    "    # 1), (1), 1. ë“± ì œê±°\n",
    "    s = re.sub(r\"^\\s*(?:\\(?([1-5])\\)?[.)])\\s*\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_answer_set(ans: str) -> Set[int]:\n",
    "    \"\"\"'â‘ , â‘¤' ê°™ì€ ë³µìˆ˜ì •ë‹µë„ {1,5}ë¡œ íŒŒì‹±. ë¹ˆ/ì´ìƒê°’ì€ ë¹ˆ set.\"\"\"\n",
    "    if not ans:\n",
    "        return set()\n",
    "    s = str(ans)\n",
    "    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜\n",
    "    for k, v in CIRCLED_MAP.items():\n",
    "        s = s.replace(k, v)\n",
    "    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ\n",
    "    nums = re.findall(r\"[1-5]\", s)\n",
    "    return set(int(n) for n in nums)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) JSON â†’ df_all ë³€í™˜\n",
    "# -----------------------------\n",
    "def json_to_df_all(json_list: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì…ë ¥ JSON(list[dict])ì„ íŒŒì‹±í•´ df_all ìƒì„±.\n",
    "    ì»¬ëŸ¼: book_id, tag, id, question, opt1..opt5, answer_set\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for item in json_list:\n",
    "        book_id = str(item.get(\"file_id\", \"\"))\n",
    "        qna = item.get(\"qna_data\", {}) or {}\n",
    "        tag  = qna.get(\"tag\", \"\")\n",
    "        desc = qna.get(\"description\", {}) or {}\n",
    "        q    = (desc.get(\"question\") or \"\").strip()\n",
    "        opts = desc.get(\"options\") or []\n",
    "        # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •\n",
    "        opts = list(opts)[:5] + [\"\"] * max(0, 5 - len(opts))\n",
    "        opts = [normalize_option_text(x) for x in opts]\n",
    "        ans_set = parse_answer_set(desc.get(\"answer\", \"\"))\n",
    "\n",
    "        rows.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"tag\": tag,\n",
    "            \"id\": f\"{book_id}_{tag}\",\n",
    "            \"question\": q,\n",
    "            \"opt1\": opts[0], \"opt2\": opts[1], \"opt3\": opts[2], \"opt4\": opts[3], \"opt5\": opts[4],\n",
    "            \"answer_set\": ans_set\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)\n",
    "    df = df.drop_duplicates(\"id\", keep=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 3) ë°°ì¹˜ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„± (50ë¬¸ì œ)\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµì „ë¬¸ê°€ì´ì ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "ì—¬ëŸ¬ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•´, ê° ë¬¸ì œì˜ ì •ë‹µ \"ë²ˆí˜¸ë§Œ\" í•˜ë‚˜ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "ê·œì¹™\n",
    "- ê° ë¬¸ì œëŠ” ê³ ìœ  IDì™€ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.\n",
    "- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë‹¹ \"ID<TAB>ë²ˆí˜¸\" í˜•ì‹ìœ¼ë¡œë§Œ í•©ë‹ˆë‹¤. (ì˜ˆ: SS0000_q_0377_0001<TAB>3)\n",
    "- ë‹¤ë¥¸ ê¸€ì, ë§ˆí¬ë‹¤ìš´, ì´ìœ , ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "- ëª¨ë“  ë¬¸ì œëŠ” ë³´ê¸°(1~5) ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¦…ë‹ˆë‹¤.\n",
    "- ì¶œë ¥ ì¤„ ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(batch_df: pd.DataFrame) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"ë‹¤ìŒì€ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´ ì •ë‹µ ë²ˆí˜¸ë§Œ ê³ ë¥´ì„¸ìš”.\\n\")\n",
    "    lines.append(\"ë¬¸ì œë“¤\")\n",
    "    for _, r in batch_df.iterrows():\n",
    "        lines.append(f\"ID: {r['id']}\")\n",
    "        lines.append(f\"Q: {r['question']}\")\n",
    "        lines.append(f\"1) {r['opt1']}\")\n",
    "        lines.append(f\"2) {r['opt2']}\")\n",
    "        lines.append(f\"3) {r['opt3']}\")\n",
    "        lines.append(f\"4) {r['opt4']}\")\n",
    "        lines.append(f\"5) {r['opt5']}\\n\")\n",
    "    lines.append(\"ì¶œë ¥ í˜•ì‹(ì¤‘ìš”)\")\n",
    "    for _, r in batch_df.iterrows():\n",
    "        lines.append(f\"{r['id']}\\\\t{{ë²ˆí˜¸}}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) LLM í˜¸ì¶œ ì¶”ìƒí™” (ëª¨ì˜/ì‹¤ì œ) - ê°œì„ ëœ ë²„ì „\n",
    "# -----------------------------\n",
    "def call_llm(model_name: str, system_prompt: str, user_prompt: str, mock_mode: bool=False, max_retries: int=3) -> str:\n",
    "    \"\"\"\n",
    "    - mock_mode=Trueë©´ ì„ì˜ ë²ˆí˜¸(1~5)ë¥¼ ìƒì„±í•´ íŒŒì´í”„ë¼ì¸ ê²€ì¦ìš© ì¶œë ¥ ë°˜í™˜.\n",
    "    - ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ OpenAI/ì‚¬ë‚´ì—”ì§„ í˜¸ì¶œë¡œ êµì²´.\n",
    "    - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨\n",
    "    \"\"\"\n",
    "    if mock_mode:\n",
    "        logger.info(f\"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘\")\n",
    "        # ì…ë ¥ user_promptì—ì„œ ID ëª©ë¡ íšŒìˆ˜\n",
    "        ids = [ln.split(\"\\t\")[0] for ln in user_prompt.splitlines() if \"\\t{ë²ˆí˜¸}\" in ln]\n",
    "        # ë¬´ì‘ìœ„ ì˜ˆì¸¡(1~5)\n",
    "        rng = np.random.default_rng(42)\n",
    "        preds = rng.integers(1, 6, size=len(ids))\n",
    "        result = \"\\n\".join(f\"{_id}\\t{int(a)}\" for _id, a in zip(ids, preds))\n",
    "        logger.info(f\"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - {len(ids)}ê°œ ë¬¸ì œ ì²˜ë¦¬\")\n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        import tools.Openrouter as Openrouter\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                ans = Openrouter.query_model_openrouter(system_prompt, user_prompt, model_name)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                logger.info(f\"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n",
    "                \n",
    "                return ans\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"[API] ëª¨ë¸ {model_name} ìµœì¢… ì‹¤íŒ¨ - ëª¨ë“  ì¬ì‹œë„ ì†Œì§„\")\n",
    "                    raise e\n",
    "                else:\n",
    "                    wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„\n",
    "                    logger.info(f\"[API] {wait_time}ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) ëª¨ë¸ ì¶œë ¥ íŒŒì‹± (ID<TAB>ë²ˆí˜¸)\n",
    "# -----------------------------\n",
    "def parse_model_output(raw: str, expected_ids: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì›ì‹œ ì¶œë ¥(raw)ì„ {id: answer(1~5)}ë¡œ ë³€í™˜.\n",
    "    - 'ID\\\\të²ˆí˜¸' í¬ë§· ê¸°ì¤€\n",
    "    - ì˜ëª»ëœ ì¤„/ëˆ„ë½ ì¤„ì€ NaN ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    id_set = set(expected_ids)\n",
    "    out: Dict[str, float] = {k: np.nan for k in expected_ids}\n",
    "\n",
    "    for ln in (raw or \"\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or \"\\t\" not in ln:\n",
    "            continue\n",
    "        left, right = ln.split(\"\\t\", 1)\n",
    "        _id = left.strip()\n",
    "        if _id not in id_set:\n",
    "            continue\n",
    "        # ì²« ë²ˆì§¸ 1~5 ì¶”ì¶œ\n",
    "        m = re.search(r\"[1-5]\", right)\n",
    "        if m:\n",
    "            out[_id] = float(m.group(0))\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 6) íŒŒì´í”„ë¼ì¸: nê°œ ìƒ˜í”Œ â†’ 50ê°œì”© Ã— ëª¨ë¸ í˜¸ì¶œ â†’ DF ì •ë¦¬/ì •í™•ë„ (ê°œì„ ëœ ë²„ì „)\n",
    "# -----------------------------\n",
    "def run_eval_pipeline(\n",
    "    json_list: List[dict],\n",
    "    models: List[str],\n",
    "    sample_size: int = 300,\n",
    "    batch_size: int = 50,\n",
    "    seed: int = 42,\n",
    "    mock_mode: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    ë°˜í™˜:\n",
    "      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set)\n",
    "      pred_long   : (id, model_name, answer) ë¡± í¬ë§·\n",
    "      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ\n",
    "      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)\n",
    "    \"\"\"\n",
    "    logger.info(f\"í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}\")\n",
    "    \n",
    "    # (1) JSON â†’ df_all\n",
    "    logger.info(\"1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...\")\n",
    "    df_all = json_to_df_all(json_list)\n",
    "    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)\n",
    "    logger.info(f\"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ\")\n",
    "\n",
    "    # (2) ìƒ˜í”Œë§\n",
    "    logger.info(f\"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\")\n",
    "    df_sample = df_all.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "    logger.info(f\"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ\")\n",
    "\n",
    "    # (3) ë°°ì¹˜ ë¶„í• \n",
    "    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]\n",
    "    logger.info(f\"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ\")\n",
    "\n",
    "    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì \n",
    "    logger.info(\"4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "    rows = []\n",
    "    total_calls = len(batches) * len(models)\n",
    "    \n",
    "    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "    with tqdm(total=total_calls, desc=\"ëª¨ë¸ í˜¸ì¶œ ì§„í–‰\", unit=\"call\") as pbar:\n",
    "        for bidx, bdf in enumerate(batches, 1):\n",
    "            user_prompt = build_user_prompt(bdf)\n",
    "            ids = bdf[\"id\"].tolist()\n",
    "            \n",
    "            for model in models:\n",
    "                try:\n",
    "                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "                    pbar.set_description(f\"ë°°ì¹˜ {bidx}/{len(batches)} - {model}\")\n",
    "                    \n",
    "                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode)\n",
    "                    parsed = parse_model_output(raw, ids)\n",
    "                    \n",
    "                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦\n",
    "                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))\n",
    "                    logger.info(f\"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡\")\n",
    "                    \n",
    "                    for _id in ids:\n",
    "                        rows.append({\"id\": _id, \"model_name\": model, \"answer\": parsed[_id]})\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
    "                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€\n",
    "                    for _id in ids:\n",
    "                        rows.append({\"id\": _id, \"model_name\": model, \"answer\": np.nan})\n",
    "                    pbar.update(1)\n",
    "\n",
    "    logger.info(\"5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...\")\n",
    "    pred_long = pd.DataFrame(rows)\n",
    "    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # (5) ì™€ì´ë“œ í¬ë§·\n",
    "    pred_wide = pred_long.pivot(index=\"id\", columns=\"model_name\", values=\"answer\").reset_index()\n",
    "    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # (6) ì •í™•ë„ ê³„ì‚°\n",
    "    logger.info(\"6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...\")\n",
    "    key = df_sample[[\"id\", \"answer_set\"]].copy()\n",
    "    \n",
    "    def _is_correct(pred: float, s: Set[int]) -> float:\n",
    "        if np.isnan(pred) or not s:\n",
    "            return np.nan\n",
    "        return float(int(pred) in s)\n",
    "\n",
    "    merged = pred_long.merge(key, on=\"id\", how=\"left\")\n",
    "    merged[\"correct\"] = merged.apply(lambda r: _is_correct(r[\"answer\"], r[\"answer_set\"]), axis=1)\n",
    "\n",
    "    acc_by_model = (\n",
    "        merged.groupby(\"model_name\", dropna=False)[\"correct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"correct\": \"accuracy\"})\n",
    "        .sort_values(\"accuracy\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…\n",
    "    logger.info(\"í‰ê°€ ì™„ë£Œ!\")\n",
    "    logger.info(\"ëª¨ë¸ë³„ ì •í™•ë„:\")\n",
    "    for _, row in acc_by_model.iterrows():\n",
    "        logger.info(f\"  {row['model_name']}: {row['accuracy']:.3f}\")\n",
    "    \n",
    "    return df_all, pred_long, pred_wide, acc_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# O, X ë¬¸ì œ ì²˜ë¦¬ ê°œì„ \n",
    "# -----------------------------\n",
    "\n",
    "def is_ox_question(question: str, options: list) -> bool:\n",
    "    \"\"\"O, X ë¬¸ì œì¸ì§€ íŒë‹¨\"\"\"\n",
    "    if not options or len(options) == 0:\n",
    "        return False\n",
    "    # optionsê°€ ë¹„ì–´ìˆê±°ë‚˜ 2ê°œ ì´í•˜ì´ê³ , O/X í˜•íƒœì¸ì§€ í™•ì¸\n",
    "    if len(options) <= 2:\n",
    "        option_text = \" \".join(options).upper()\n",
    "        return \"O\" in option_text or \"X\" in option_text\n",
    "    return False\n",
    "\n",
    "def parse_answer_set_improved(ans: str, question: str = \"\", options: list = None) -> Set[int]:\n",
    "    \"\"\"ê°œì„ ëœ ì •ë‹µ íŒŒì‹± í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬\"\"\"\n",
    "    if not ans:\n",
    "        return set()\n",
    "    s = str(ans).strip()\n",
    "    \n",
    "    # O, X ë¬¸ì œ ì²˜ë¦¬\n",
    "    if s.upper() in ['O', 'X']:\n",
    "        # O, X ë¬¸ì œëŠ” 1ë²ˆ(O), 2ë²ˆ(X)ìœ¼ë¡œ ë³€í™˜\n",
    "        return {1} if s.upper() == 'O' else {2}\n",
    "    \n",
    "    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜\n",
    "    for k, v in CIRCLED_MAP.items():\n",
    "        s = s.replace(k, v)\n",
    "    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ\n",
    "    nums = re.findall(r\"[1-5]\", s)\n",
    "    return set(int(n) for n in nums)\n",
    "\n",
    "def json_to_df_all_improved(json_list: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ JSON â†’ df_all ë³€í™˜ í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬\n",
    "    ì»¬ëŸ¼: book_id, tag, id, question, opt1..opt5, answer_set, is_ox_question\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for item in json_list:\n",
    "        book_id = str(item.get(\"file_id\", \"\"))\n",
    "        qna = item.get(\"qna_data\", {}) or {}\n",
    "        tag  = qna.get(\"tag\", \"\")\n",
    "        desc = qna.get(\"description\", {}) or {}\n",
    "        q    = (desc.get(\"question\") or \"\").strip()\n",
    "        opts = desc.get(\"options\") or []\n",
    "        \n",
    "        # O, X ë¬¸ì œì¸ì§€ íŒë‹¨\n",
    "        is_ox = is_ox_question(q, opts)\n",
    "        \n",
    "        if is_ox:\n",
    "            # O, X ë¬¸ì œëŠ” 2ê°œ ì„ ì§€ë¡œ ê³ ì •\n",
    "            opts = [\"O\", \"X\"] + [\"\"] * 3\n",
    "        else:\n",
    "            # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •\n",
    "            opts = list(opts)[:5] + [\"\"] * max(0, 5 - len(opts))\n",
    "        \n",
    "        opts = [normalize_option_text(x) for x in opts]\n",
    "        ans_set = parse_answer_set_improved(desc.get(\"answer\", \"\"), q, opts)\n",
    "\n",
    "        rows.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"tag\": tag,\n",
    "            \"id\": f\"{book_id}_{tag}\",\n",
    "            \"question\": q,\n",
    "            \"opt1\": opts[0], \"opt2\": opts[1], \"opt3\": opts[2], \"opt4\": opts[3], \"opt5\": opts[4],\n",
    "            \"answer_set\": ans_set,\n",
    "            \"is_ox_question\": is_ox\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)\n",
    "    df = df.drop_duplicates(\"id\", keep=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# O, X ë¬¸ì œ í†µê³„ í™•ì¸ í•¨ìˆ˜\n",
    "def analyze_ox_questions(df: pd.DataFrame):\n",
    "    \"\"\"O, X ë¬¸ì œ ë¶„ì„\"\"\"\n",
    "    ox_questions = df[df['is_ox_question'] == True]\n",
    "    regular_questions = df[df['is_ox_question'] == False]\n",
    "    \n",
    "    print(f\"ğŸ“Š ë¬¸ì œ ìœ í˜• ë¶„ì„\")\n",
    "    print(f\"   - O, X ë¬¸ì œ: {len(ox_questions)}ê°œ\")\n",
    "    print(f\"   - ì¼ë°˜ ê°ê´€ì‹: {len(regular_questions)}ê°œ\")\n",
    "    print(f\"   - ì „ì²´: {len(df)}ê°œ\")\n",
    "    \n",
    "    if len(ox_questions) > 0:\n",
    "        print(f\"\\nğŸ” O, X ë¬¸ì œ ì •ë‹µ ë¶„í¬:\")\n",
    "        ox_answers = ox_questions['answer_set'].apply(lambda x: list(x) if x else [])\n",
    "        answer_counts = {}\n",
    "        for answers in ox_answers:\n",
    "            for ans in answers:\n",
    "                answer_counts[ans] = answer_counts.get(ans, 0) + 1\n",
    "        \n",
    "        for ans, count in sorted(answer_counts.items()):\n",
    "            answer_text = \"O\" if ans == 1 else \"X\" if ans == 2 else str(ans)\n",
    "            print(f\"   - {answer_text}: {count}ê°œ\")\n",
    "    \n",
    "    return ox_questions, regular_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸ (O, X ë¬¸ì œ ì§€ì›)\n",
    "# -----------------------------\n",
    "\n",
    "def run_eval_pipeline_improved(\n",
    "    json_list: List[dict],\n",
    "    models: List[str],\n",
    "    sample_size: int = 300,\n",
    "    batch_size: int = 50,\n",
    "    seed: int = 42,\n",
    "    mock_mode: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    O, X ë¬¸ì œë¥¼ ì§€ì›í•˜ëŠ” ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸\n",
    "    ë°˜í™˜:\n",
    "      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set + is_ox_question)\n",
    "      pred_long   : (id, model_name, answer) ë¡± í¬ë§·\n",
    "      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ\n",
    "      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)\n",
    "    \"\"\"\n",
    "    logger.info(f\"ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}\")\n",
    "    \n",
    "    # (1) JSON â†’ df_all (O, X ë¬¸ì œ ì§€ì›)\n",
    "    logger.info(\"1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...\")\n",
    "    df_all = json_to_df_all_improved(json_list)\n",
    "    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)\n",
    "    logger.info(f\"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ\")\n",
    "\n",
    "    # O, X ë¬¸ì œ ë¶„ì„\n",
    "    ox_questions, regular_questions = analyze_ox_questions(df_all)\n",
    "\n",
    "    # (2) ìƒ˜í”Œë§\n",
    "    logger.info(f\"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\")\n",
    "    df_sample = df_all.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "    logger.info(f\"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ\")\n",
    "\n",
    "    # ìƒ˜í”Œì—ì„œ O, X ë¬¸ì œ ë¹„ìœ¨ í™•ì¸\n",
    "    sample_ox = df_sample[df_sample['is_ox_question'] == True]\n",
    "    sample_regular = df_sample[df_sample['is_ox_question'] == False]\n",
    "    logger.info(f\"ìƒ˜í”Œ ë‚´ O, X ë¬¸ì œ: {len(sample_ox)}ê°œ, ì¼ë°˜ ê°ê´€ì‹: {len(sample_regular)}ê°œ\")\n",
    "\n",
    "    # (3) ë°°ì¹˜ ë¶„í• \n",
    "    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]\n",
    "    logger.info(f\"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ\")\n",
    "\n",
    "    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì \n",
    "    logger.info(\"4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "    rows = []\n",
    "    total_calls = len(batches) * len(models)\n",
    "    \n",
    "    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "    with tqdm(total=total_calls, desc=\"ëª¨ë¸ í˜¸ì¶œ ì§„í–‰\", unit=\"call\") as pbar:\n",
    "        for bidx, bdf in enumerate(batches, 1):\n",
    "            user_prompt = build_user_prompt(bdf)\n",
    "            ids = bdf[\"id\"].tolist()\n",
    "            \n",
    "            for model in models:\n",
    "                try:\n",
    "                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "                    pbar.set_description(f\"ë°°ì¹˜ {bidx}/{len(batches)} - {model}\")\n",
    "                    \n",
    "                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode)\n",
    "                    parsed = parse_model_output(raw, ids)\n",
    "                    \n",
    "                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦\n",
    "                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))\n",
    "                    logger.info(f\"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡\")\n",
    "                    \n",
    "                    for _id in ids:\n",
    "                        rows.append({\"id\": _id, \"model_name\": model, \"answer\": parsed[_id]})\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
    "                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€\n",
    "                    for _id in ids:\n",
    "                        rows.append({\"id\": _id, \"model_name\": model, \"answer\": np.nan})\n",
    "                    pbar.update(1)\n",
    "\n",
    "    logger.info(\"5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...\")\n",
    "    pred_long = pd.DataFrame(rows)\n",
    "    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # (5) ì™€ì´ë“œ í¬ë§·\n",
    "    pred_wide = pred_long.pivot(index=\"id\", columns=\"model_name\", values=\"answer\").reset_index()\n",
    "    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # (6) ì •í™•ë„ ê³„ì‚°\n",
    "    logger.info(\"6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...\")\n",
    "    key = df_sample[[\"id\", \"answer_set\"]].copy()\n",
    "    \n",
    "    def _is_correct(pred: float, s: Set[int]) -> float:\n",
    "        if np.isnan(pred) or not s:\n",
    "            return np.nan\n",
    "        return float(int(pred) in s)\n",
    "\n",
    "    merged = pred_long.merge(key, on=\"id\", how=\"left\")\n",
    "    merged[\"correct\"] = merged.apply(lambda r: _is_correct(r[\"answer\"], r[\"answer_set\"]), axis=1)\n",
    "\n",
    "    acc_by_model = (\n",
    "        merged.groupby(\"model_name\", dropna=False)[\"correct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"correct\": \"accuracy\"})\n",
    "        .sort_values(\"accuracy\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    # O, X ë¬¸ì œì™€ ì¼ë°˜ ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„\n",
    "    sample_with_type = df_sample[[\"id\", \"is_ox_question\"]].copy()\n",
    "    merged_with_type = merged.merge(sample_with_type, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # O, X ë¬¸ì œ ì •í™•ë„\n",
    "    ox_accuracy = merged_with_type[merged_with_type['is_ox_question'] == True].groupby(\"model_name\")[\"correct\"].mean()\n",
    "    regular_accuracy = merged_with_type[merged_with_type['is_ox_question'] == False].groupby(\"model_name\")[\"correct\"].mean()\n",
    "    \n",
    "    logger.info(\"í‰ê°€ ì™„ë£Œ!\")\n",
    "    logger.info(\"ëª¨ë¸ë³„ ì „ì²´ ì •í™•ë„:\")\n",
    "    for _, row in acc_by_model.iterrows():\n",
    "        logger.info(f\"  {row['model_name']}: {row['accuracy']:.3f}\")\n",
    "    \n",
    "    if len(ox_accuracy) > 0:\n",
    "        logger.info(\"O, X ë¬¸ì œ ì •í™•ë„:\")\n",
    "        for model, acc in ox_accuracy.items():\n",
    "            logger.info(f\"  {model}: {acc:.3f}\")\n",
    "    \n",
    "    return df_all, pred_long, pred_wide, acc_by_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# O, X ë¬¸ì œ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦\n",
    "# -----------------------------\n",
    "\n",
    "# O, X ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "print(\"ğŸ” O, X ë¬¸ì œ í™•ì¸ ì¤‘...\")\n",
    "df_test = json_to_df_all_improved(random_multiple)\n",
    "ox_questions, regular_questions = analyze_ox_questions(df_test)\n",
    "\n",
    "# O, X ë¬¸ì œê°€ ìˆë‹¤ë©´ ìƒ˜í”Œ ì¶œë ¥\n",
    "if len(ox_questions) > 0:\n",
    "    print(f\"\\nğŸ“ O, X ë¬¸ì œ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):\")\n",
    "    for i, (_, row) in enumerate(ox_questions.head(3).iterrows()):\n",
    "        print(f\"\\n{i+1}. ID: {row['id']}\")\n",
    "        print(f\"   ë¬¸ì œ: {row['question']}\")\n",
    "        print(f\"   ì„ ì§€: 1) {row['opt1']} 2) {row['opt2']}\")\n",
    "        print(f\"   ì •ë‹µ: {row['answer_set']} ({'O' if 1 in row['answer_set'] else 'X' if 2 in row['answer_set'] else 'Unknown'})\")\n",
    "else:\n",
    "    print(\"âŒ O, X ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Mock ëª¨ë“œë¡œ O, X ë¬¸ì œ í¬í•¨ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸ§ª Mock ëª¨ë“œë¡œ O, X ë¬¸ì œ í¬í•¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "test_data = random_multiple[:20]  # 20ê°œ í…ŒìŠ¤íŠ¸\n",
    "test_models = ['test-model']\n",
    "\n",
    "try:\n",
    "    df_all_test, pred_long_test, pred_wide_test, acc_test = run_eval_pipeline_improved(\n",
    "        test_data, test_models, sample_size=20, batch_size=5, seed=42, mock_mode=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Mock í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "    print(f\"   - ì²˜ë¦¬ëœ ë¬¸ì œ ìˆ˜: {len(df_all_test)}\")\n",
    "    print(f\"   - O, X ë¬¸ì œ ìˆ˜: {len(df_all_test[df_all_test['is_ox_question'] == True])}\")\n",
    "    print(f\"   - ì¼ë°˜ ê°ê´€ì‹ ìˆ˜: {len(df_all_test[df_all_test['is_ox_question'] == False])}\")\n",
    "    print(f\"   - ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜: {len(pred_long_test)}\")\n",
    "    print(f\"   - ì •í™•ë„: {acc_test.iloc[0]['accuracy']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Mock í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}\")\n",
    "    logger.error(f\"Mock í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ O, X ë¬¸ì œë¥¼ í¬í•¨í•œ ì‹¤ì œ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# O, X ë¬¸ì œë¥¼ í¬í•¨í•œ ì‹¤ì œ í‰ê°€ ì‹¤í–‰\n",
    "# -----------------------------\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "data = random_multiple\n",
    "models = ['anthropic/claude-sonnet-4.5']\n",
    "# , 'google/gemini-2.5-flash', 'openai/gpt-5']  # ì‹¤ì œ ëª¨ë¸ëª…\n",
    "\n",
    "# O, X ë¬¸ì œë¥¼ í¬í•¨í•œ í‰ê°€ ì‹¤í–‰\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"O, X ë¬¸ì œë¥¼ í¬í•¨í•œ í‰ê°€ ì‹œì‘\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬\n",
    "df_all_temp = json_to_df_all_improved(data)\n",
    "quality_issues = check_data_quality(df_all_temp, df_all_temp.sample(n=min(50, len(df_all_temp)), random_state=42))\n",
    "\n",
    "# O, X ë¬¸ì œ ë¶„ì„\n",
    "ox_questions, regular_questions = analyze_ox_questions(df_all_temp)\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰ (O, X ë¬¸ì œ ì§€ì›)\n",
    "df_all, pred_long, pred_wide, acc = run_eval_pipeline_improved(\n",
    "    data, models, sample_size=50, batch_size=10, seed=42, mock_mode=False\n",
    ")\n",
    "\n",
    "# ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
    "print_evaluation_summary(acc, pred_long)\n",
    "\n",
    "# O, X ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„\n",
    "sample_with_type = df_all[df_all['id'].isin(pred_long['id'])][['id', 'is_ox_question']].copy()\n",
    "merged_with_type = pred_long.merge(sample_with_type, on='id', how='left')\n",
    "\n",
    "if len(merged_with_type[merged_with_type['is_ox_question'] == True]) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š O, X ë¬¸ì œ vs ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„ ë¹„êµ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # O, X ë¬¸ì œ ì •í™•ë„\n",
    "    ox_correct = merged_with_type[merged_with_type['is_ox_question'] == True]['answer'].notna().sum()\n",
    "    ox_total = len(merged_with_type[merged_with_type['is_ox_question'] == True])\n",
    "    ox_acc = ox_correct / ox_total if ox_total > 0 else 0\n",
    "    \n",
    "    # ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„\n",
    "    regular_correct = merged_with_type[merged_with_type['is_ox_question'] == False]['answer'].notna().sum()\n",
    "    regular_total = len(merged_with_type[merged_with_type['is_ox_question'] == False])\n",
    "    regular_acc = regular_correct / regular_total if regular_total > 0 else 0\n",
    "    \n",
    "    print(f\"O, X ë¬¸ì œ: {ox_correct}/{ox_total} ({ox_acc:.1%})\")\n",
    "    print(f\"ì¼ë°˜ ê°ê´€ì‹: {regular_correct}/{regular_total} ({regular_acc:.1%})\")\n",
    "\n",
    "# ìƒì„¸ ë¡œê·¸ ì €ì¥\n",
    "save_detailed_logs(pred_long, \"evaluation_ox\")\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"O, X ë¬¸ì œë¥¼ í¬í•¨í•œ í‰ê°€ ì™„ë£Œ\")\n",
    "logger.info(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "# -----------------------------\n",
    "\n",
    "def print_evaluation_summary(acc_df: pd.DataFrame, pred_long_df: pd.DataFrame):\n",
    "    \"\"\"í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š í‰ê°€ ê²°ê³¼ ìƒì„¸ ìš”ì•½\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ê¸°ë³¸ í†µê³„\n",
    "    total_predictions = len(pred_long_df)\n",
    "    valid_predictions = len(pred_long_df.dropna(subset=['answer']))\n",
    "    invalid_predictions = total_predictions - valid_predictions\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ì „ì²´ ì˜ˆì¸¡ ìˆ˜: {total_predictions:,}\")\n",
    "    print(f\"âœ… ìœ íš¨ ì˜ˆì¸¡: {valid_predictions:,} ({valid_predictions/total_predictions*100:.1f}%)\")\n",
    "    print(f\"âŒ ë¬´íš¨ ì˜ˆì¸¡: {invalid_predictions:,} ({invalid_predictions/total_predictions*100:.1f}%)\")\n",
    "    \n",
    "    # ëª¨ë¸ë³„ ì •í™•ë„\n",
    "    print(f\"\\nğŸ† ëª¨ë¸ë³„ ì •í™•ë„ ìˆœìœ„:\")\n",
    "    for i, (_, row) in enumerate(acc_df.iterrows(), 1):\n",
    "        accuracy = row['accuracy']\n",
    "        if pd.isna(accuracy):\n",
    "            print(f\"  {i}. {row['model_name']}: N/A (ë°ì´í„° ì—†ìŒ)\")\n",
    "        else:\n",
    "            print(f\"  {i}. {row['model_name']}: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def save_detailed_logs(pred_long_df: pd.DataFrame, filename_prefix: str = \"evaluation\"):\n",
    "    import datetime as dt\n",
    "    \"\"\"ìƒì„¸í•œ ë¡œê·¸ë¥¼ CSVë¡œ ì €ì¥\"\"\"\n",
    "    timestamp = dt.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¡œê·¸\n",
    "    pred_log_filename = f\"{filename_prefix}_predictions_{timestamp}.csv\"\n",
    "    pred_long_df.to_csv(pred_log_filename, index=False, encoding='utf-8-sig')\n",
    "    logger.info(f\"ìƒì„¸ ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥: {pred_log_filename}\")\n",
    "    \n",
    "    # ëª¨ë¸ë³„ í†µê³„\n",
    "    model_stats = pred_long_df.groupby('model_name').agg({\n",
    "        'answer': ['count', lambda x: x.notna().sum(), lambda x: x.isna().sum()]\n",
    "    }).round(3)\n",
    "    model_stats.columns = ['ì´_ì˜ˆì¸¡ìˆ˜', 'ìœ íš¨_ì˜ˆì¸¡ìˆ˜', 'ë¬´íš¨_ì˜ˆì¸¡ìˆ˜']\n",
    "    model_stats['ìœ íš¨ìœ¨'] = (model_stats['ìœ íš¨_ì˜ˆì¸¡ìˆ˜'] / model_stats['ì´_ì˜ˆì¸¡ìˆ˜'] * 100).round(1)\n",
    "    \n",
    "    stats_filename = f\"{filename_prefix}_model_stats_{timestamp}.csv\"\n",
    "    model_stats.to_csv(stats_filename, encoding='utf-8-sig')\n",
    "    logger.info(f\"ëª¨ë¸ í†µê³„ ì €ì¥: {stats_filename}\")\n",
    "\n",
    "def check_data_quality(df_all: pd.DataFrame, df_sample: pd.DataFrame):\n",
    "    \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬\"\"\"\n",
    "    logger.info(\"ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. ë¹ˆ ë¬¸ì œ ê²€ì‚¬\n",
    "    empty_questions = df_all[df_all['question'].str.strip() == '']\n",
    "    if len(empty_questions) > 0:\n",
    "        issues.append(f\"ë¹ˆ ë¬¸ì œ: {len(empty_questions)}ê°œ\")\n",
    "    \n",
    "    # 2. ë¹ˆ ì„ ì§€ ê²€ì‚¬\n",
    "    empty_options = df_all[(df_all['opt1'].str.strip() == '') & \n",
    "                          (df_all['opt2'].str.strip() == '') & \n",
    "                          (df_all['opt3'].str.strip() == '') & \n",
    "                          (df_all['opt4'].str.strip() == '') & \n",
    "                          (df_all['opt5'].str.strip() == '')]\n",
    "    if len(empty_options) > 0:\n",
    "        issues.append(f\"ë¹ˆ ì„ ì§€ ë¬¸ì œ: {len(empty_options)}ê°œ\")\n",
    "    \n",
    "    # 3. ì •ë‹µ ì—†ëŠ” ë¬¸ì œ ê²€ì‚¬\n",
    "    no_answer = df_all[df_all['answer_set'].apply(len) == 0]\n",
    "    if len(no_answer) > 0:\n",
    "        issues.append(f\"ì •ë‹µ ì—†ëŠ” ë¬¸ì œ: {len(no_answer)}ê°œ\")\n",
    "    \n",
    "    # 4. ì¤‘ë³µ ë¬¸ì œ ê²€ì‚¬\n",
    "    duplicates = df_all[df_all.duplicated(subset=['question'], keep=False)]\n",
    "    if len(duplicates) > 0:\n",
    "        issues.append(f\"ì¤‘ë³µ ë¬¸ì œ: {len(duplicates)}ê°œ\")\n",
    "    \n",
    "    if issues:\n",
    "        logger.warning(\"ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œê²¬:\")\n",
    "        for issue in issues:\n",
    "            logger.warning(f\"  - {issue}\")\n",
    "    else:\n",
    "        logger.info(\"ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ âœ…\")\n",
    "    \n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8) ì‚¬ìš© ì˜ˆì‹œ (ê°œì„ ëœ ë²„ì „)\n",
    "# -----------------------------\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "data = random_multiple\n",
    "models = ['anthropic/claude-sonnet-4.5']\n",
    "# , 'google/gemini-2.5-flash', 'openai/gpt-5']  # ì‹¤ì œ ëª¨ë¸ëª…\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰ (ì§„í–‰ìƒí™©ê³¼ ë¡œê¹…ì´ í¬í•¨ëœ ë²„ì „)\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"í‰ê°€ ì‹œì‘\")\n",
    "logger.info(\"=\" * 50)\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬\n",
    "df_all_temp = json_to_df_all(data)\n",
    "quality_issues = check_data_quality(df_all_temp, df_all_temp.sample(n=min(50, len(df_all_temp)), random_state=42))\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "df_all, pred_long, pred_wide, acc = run_eval_pipeline(\n",
    "    data, models, sample_size=50, batch_size=10, seed=42, mock_mode=False\n",
    ")\n",
    "\n",
    "# ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
    "print_evaluation_summary(acc, pred_long)\n",
    "\n",
    "# ìƒì„¸ ë¡œê·¸ ì €ì¥\n",
    "save_detailed_logs(pred_long, \"evaluation\")\n",
    "\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"í‰ê°€ ì™„ë£Œ\")\n",
    "logger.info(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detailed_logs(pred_long, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ (ê°œì„ ëœ ë²„ì „)\n",
    "import datetime as dt\n",
    "\n",
    "# íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€\n",
    "timestamp = dt.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "filename = f\"evaluation_results_{timestamp}.xlsx\"\n",
    "\n",
    "logger.info(f\"ê²°ê³¼ë¥¼ {filename}ì— ì €ì¥ ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as w:\n",
    "        df_all.to_excel(w, index=False, sheet_name=\"ì „ì²´ë°ì´í„°\")   # í†µí•© ë·°\n",
    "        pred_wide.to_excel(w, index=False, sheet_name=\"ëª¨ë¸ë³„ì˜ˆì¸¡\")         # ëª¨ë¸ë³„ ì˜ˆì¸¡(ê°€ë¡œ)\n",
    "        acc.to_excel(w, index=False, sheet_name=\"ì •í™•ë„\")       # ëª¨ë¸ë³„ ì •í™•ë„ ìš”ì•½\n",
    "        \n",
    "    logger.info(f\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì´ë¯¸ print_evaluation_summaryì—ì„œ ì²˜ë¦¬ë¨)\n",
    "    print(f\"\\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
    "    print(f\"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 9) í…ŒìŠ¤íŠ¸ ë° ê²€ì¦\n",
    "# # -----------------------------\n",
    "\n",
    "# # Mock ëª¨ë“œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "# print(\"ğŸ§ª Mock ëª¨ë“œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "# test_data = random_multiple[:10]  # 10ê°œë§Œ í…ŒìŠ¤íŠ¸\n",
    "# test_models = ['test-model']\n",
    "\n",
    "# try:\n",
    "#     df_all_test, pred_long_test, pred_wide_test, acc_test = run_eval_pipeline(\n",
    "#         test_data, test_models, sample_size=10, batch_size=5, seed=42, mock_mode=True\n",
    "#     )\n",
    "    \n",
    "#     print(\"âœ… Mock í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "#     print(f\"   - ì²˜ë¦¬ëœ ë¬¸ì œ ìˆ˜: {len(df_all_test)}\")\n",
    "#     print(f\"   - ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜: {len(pred_long_test)}\")\n",
    "#     print(f\"   - ì •í™•ë„: {acc_test.iloc[0]['accuracy']:.3f}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Mock í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}\")\n",
    "#     logger.error(f\"Mock í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"ğŸš€ ì‹¤ì œ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ìœ„ì˜ Cell 8ì„ ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "# print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
