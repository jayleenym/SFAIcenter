#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM í‰ê°€ ì‹œìŠ¤í…œ - í†µí•© ë²„ì „
O, X ë¬¸ì œë¥¼ í¬í•¨í•œ ê°ê´€ì‹ ë¬¸ì œ í‰ê°€ ì‹œìŠ¤í…œ

ì‚¬ìš©ë²•:
    python llm_evaluation_system.py --data_path /path/to/data --sample_size 100 --mock_mode
"""

import os
import pandas as pd
import numpy as np
import re
import time
import logging
import random
import json
import datetime as dt
from typing import List, Dict, Tuple, Iterable, Set
from dataclasses import dataclass
from tqdm import tqdm
import argparse

# -----------------------------
# ë¡œê¹… ì„¤ì •
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('evaluation_result/log/evaluation.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# -----------------------------
# ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ê·œí™”
# -----------------------------
CIRCLED_MAP = {"â‘ ":"1","â‘¡":"2","â‘¢":"3","â‘£":"4","â‘¤":"5"}

def normalize_option_text(s: str) -> str:
    """ì„ ì§€ ì•ì— ë¶™ì€ â‘ ~â‘¤, 1), (1), 1. ë“± ë²ˆí˜¸ í‘œê¸°ë¥¼ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë‚¨ê¹€."""
    if s is None:
        return ""
    s = str(s).strip()
    # â‘ ~â‘¤ ì œê±°
    s = re.sub(r"^\s*[â‘ -â‘¤]\s*", "", s)
    # 1), (1), 1. ë“± ì œê±°
    s = re.sub(r"^\s*(?:\(?([1-5])\)?[.)])\s*", "", s)
    return s.strip()

def parse_answer_set(ans: str) -> Set[int]:
    """'â‘ , â‘¤' ê°™ì€ ë³µìˆ˜ì •ë‹µë„ {1,5}ë¡œ íŒŒì‹±. ë¹ˆ/ì´ìƒê°’ì€ ë¹ˆ set."""
    if not ans:
        return set()
    s = str(ans)
    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
    for k, v in CIRCLED_MAP.items():
        s = s.replace(k, v)
    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ
    nums = re.findall(r"[1-5]", s)
    return set(int(n) for n in nums)

# -----------------------------
# O, X ë¬¸ì œ ì²˜ë¦¬ ê°œì„ 
# -----------------------------

def is_ox_question(question: str, options: list) -> bool:
    """O, X ë¬¸ì œì¸ì§€ íŒë‹¨"""
    if not options or len(options) == 0:
        return False
    # optionsê°€ ë¹„ì–´ìˆê±°ë‚˜ 2ê°œ ì´í•˜ì´ê³ , O/X í˜•íƒœì¸ì§€ í™•ì¸
    if len(options) <= 2:
        option_text = " ".join(options).upper()
        return "O" in option_text or "X" in option_text
    return False

def parse_answer_set_improved(ans: str, question: str = "", options: list = None) -> Set[int]:
    """ê°œì„ ëœ ì •ë‹µ íŒŒì‹± í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬"""
    if not ans:
        return set()
    s = str(ans).strip()
    
    # O, X ë¬¸ì œ ì²˜ë¦¬
    if s.upper() in ['O', 'X']:
        # O, X ë¬¸ì œëŠ” 1ë²ˆ(O), 2ë²ˆ(X)ìœ¼ë¡œ ë³€í™˜
        return {1} if s.upper() == 'O' else {2}
    
    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
    for k, v in CIRCLED_MAP.items():
        s = s.replace(k, v)
    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ
    nums = re.findall(r"[1-5]", s)
    return set(int(n) for n in nums)

# -----------------------------
# JSON â†’ df_all ë³€í™˜
# -----------------------------

def json_to_df_all(json_list: List[dict]) -> pd.DataFrame:
    """
    ì…ë ¥ JSON(list[dict])ì„ íŒŒì‹±í•´ df_all ìƒì„±.
    ì»¬ëŸ¼: book_id, tag, id, question, opt1..opt5, answer_set
    """
    rows = []
    for item in json_list:
        book_id = str(item.get("file_id", ""))
        qna = item.get("qna_data", {}) or {}
        tag  = qna.get("tag", "")
        desc = qna.get("description", {}) or {}
        q    = (desc.get("question") or "").strip()
        opts = desc.get("options") or []
        # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
        opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        opts = [normalize_option_text(x) for x in opts]
        ans_set = parse_answer_set(desc.get("answer", ""))

        rows.append({
            "book_id": book_id,
            "tag": tag,
            "id": f"{book_id}_{tag}",
            "question": q,
            "opt1": opts[0], "opt2": opts[1], "opt3": opts[2], "opt4": opts[3], "opt5": opts[4],
            "answer_set": ans_set
        })
    df = pd.DataFrame(rows)
    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)
    df = df.drop_duplicates("id", keep="last").reset_index(drop=True)
    return df

def json_to_df_all_improved(json_list: List[dict]) -> pd.DataFrame:
    """
    ê°œì„ ëœ JSON â†’ df_all ë³€í™˜ í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬
    ì»¬ëŸ¼: book_id, tag, id, question, opt1..opt5, answer_set, is_ox_question
    """
    rows = []
    for item in json_list:
        book_id = str(item.get("file_id", ""))
        qna = item.get("qna_data", {}) or {}
        tag  = qna.get("tag", "")
        desc = qna.get("description", {}) or {}
        q    = (desc.get("question") or "").strip()
        opts = desc.get("options") or []
        
        # O, X ë¬¸ì œì¸ì§€ íŒë‹¨
        is_ox = is_ox_question(q, opts)
        
        if is_ox:
            # O, X ë¬¸ì œëŠ” 2ê°œ ì„ ì§€ë¡œ ê³ ì •
            opts = ["O", "X"] + [""] * 3
        else:
            # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
            opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        
        opts = [normalize_option_text(x) for x in opts]
        ans_set = parse_answer_set_improved(desc.get("answer", ""), q, opts)

        rows.append({
            "book_id": book_id,
            "tag": tag,
            "id": f"{book_id}_{tag}",
            "question": q,
            "opt1": opts[0], "opt2": opts[1], "opt3": opts[2], "opt4": opts[3], "opt5": opts[4],
            "answer_set": ans_set,
            "is_ox_question": is_ox
        })
    df = pd.DataFrame(rows)
    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)
    df = df.drop_duplicates("id", keep="last").reset_index(drop=True)
    return df

# -----------------------------
# ë°°ì¹˜ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
# -----------------------------

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸ˆìœµì „ë¬¸ê°€ì´ì ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•´, ê° ë¬¸ì œì˜ ì •ë‹µ "ë²ˆí˜¸ë§Œ" í•˜ë‚˜ ì„ íƒí•©ë‹ˆë‹¤.

ê·œì¹™
- ê° ë¬¸ì œëŠ” ê³ ìœ  IDì™€ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë‹¹ "ID<TAB>ë²ˆí˜¸" í˜•ì‹ìœ¼ë¡œë§Œ í•©ë‹ˆë‹¤. (ì˜ˆ: SS0000_q_0377_0001<TAB>3)
- ë‹¤ë¥¸ ê¸€ì, ë§ˆí¬ë‹¤ìš´, ì´ìœ , ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì œëŠ” ë³´ê¸°(1~5) ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¦…ë‹ˆë‹¤.
- ì¶œë ¥ ì¤„ ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
"""

def build_user_prompt(batch_df: pd.DataFrame) -> str:
    lines = []
    lines.append("ë‹¤ìŒì€ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´ ì •ë‹µ ë²ˆí˜¸ë§Œ ê³ ë¥´ì„¸ìš”.\n")
    lines.append("ë¬¸ì œë“¤")
    for _, r in batch_df.iterrows():
        lines.append(f"ID: {r['id']}")
        lines.append(f"Q: {r['question']}")
        lines.append(f"1) {r['opt1']}")
        lines.append(f"2) {r['opt2']}")
        lines.append(f"3) {r['opt3']}")
        lines.append(f"4) {r['opt4']}")
        lines.append(f"5) {r['opt5']}\n")
    lines.append("ì¶œë ¥ í˜•ì‹(ì¤‘ìš”)")
    for _, r in batch_df.iterrows():
        lines.append(f"{r['id']}\\t{{ë²ˆí˜¸}}")
    return "\n".join(lines)

# -----------------------------
# LLM í˜¸ì¶œ ì¶”ìƒí™”
# -----------------------------

def call_llm(model_name: str, system_prompt: str, user_prompt: str, mock_mode: bool=False, max_retries: int=3) -> str:
    """
    - mock_mode=Trueë©´ ì„ì˜ ë²ˆí˜¸(1~5)ë¥¼ ìƒì„±í•´ íŒŒì´í”„ë¼ì¸ ê²€ì¦ìš© ì¶œë ¥ ë°˜í™˜.
    - ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ OpenAI/ì‚¬ë‚´ì—”ì§„ í˜¸ì¶œë¡œ êµì²´.
    - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨
    """
    if mock_mode:
        logger.info(f"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘")
        # ì…ë ¥ user_promptì—ì„œ ID ëª©ë¡ íšŒìˆ˜
        ids = [ln.split("\t")[0] for ln in user_prompt.splitlines() if "\t{ë²ˆí˜¸}" in ln]
        if not ids:
            # IDë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            ids = [ln.split("ID: ")[1].strip() for ln in user_prompt.splitlines() if ln.startswith("ID: ")]
        
        # ë¬´ì‘ìœ„ ì˜ˆì¸¡(1~5)
        rng = np.random.default_rng(42)
        preds = rng.integers(1, 6, size=len(ids))
        result = "\n".join(f"{_id}\t{int(a)}" for _id, a in zip(ids, preds))
        logger.info(f"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - {len(ids)}ê°œ ë¬¸ì œ ì²˜ë¦¬")
        return result
    
    else:
        try:
            import sys
            import os
            # tools ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
            tools_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)))
            if tools_dir not in sys.path:
                sys.path.insert(0, tools_dir)
            import Openrouter
        except ImportError as e:
            logger.error(f"Openrouter ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            logger.error("í•´ê²° ë°©ë²•:")
            logger.error("1. mock_mode=Trueë¡œ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸")
            logger.error("2. pip install openai ëª…ë ¹ìœ¼ë¡œ ì˜ì¡´ì„± ì„¤ì¹˜")
            logger.error("3. tools/Openrouter.py íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸")
            raise ImportError(f"Openrouter ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤: {str(e)}")
        
        for attempt in range(max_retries):
            try:
                logger.info(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})")
                start_time = time.time()
                
                ans = Openrouter.query_model_openrouter(system_prompt, user_prompt, model_name)
                
                elapsed_time = time.time() - start_time
                logger.info(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                
                return ans
                
            except Exception as e:
                logger.warning(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    logger.error(f"[API] ëª¨ë¸ {model_name} ìµœì¢… ì‹¤íŒ¨ - ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")
                    raise e
                else:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    logger.info(f"[API] {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)

# -----------------------------
# ëª¨ë¸ ì¶œë ¥ íŒŒì‹±
# -----------------------------

def parse_model_output(raw: str, expected_ids: List[str]) -> Dict[str, float]:
    """
    ëª¨ë¸ ì›ì‹œ ì¶œë ¥(raw)ì„ {id: answer(1~5)}ë¡œ ë³€í™˜.
    - 'ID\\të²ˆí˜¸' í¬ë§· ê¸°ì¤€
    - ì˜ëª»ëœ ì¤„/ëˆ„ë½ ì¤„ì€ NaN ì²˜ë¦¬
    """
    id_set = set(expected_ids)
    out: Dict[str, float] = {k: np.nan for k in expected_ids}
    
    if not raw or not raw.strip():
        logger.warning("ëª¨ë¸ ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return out

    lines = raw.splitlines()
    logger.debug(f"íŒŒì‹±í•  ì¤„ ìˆ˜: {len(lines)}")
    
    for i, ln in enumerate(lines):
        ln = ln.strip()
        logger.debug(f"ì¤„ {i+1}: '{ln}'")
        
        if not ln:
            logger.debug(f"ì¤„ {i+1}: ë¹ˆ ì¤„, ìŠ¤í‚µ")
            continue
        
        # ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬: \\t -> \t
        ln = ln.replace("\\t", "\t")
        logger.debug(f"ì¤„ {i+1} (ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ í›„): '{ln}'")
            
        if "\t" not in ln:
            logger.debug(f"ì¤„ {i+1}: íƒ­ì´ ì—†ìŒ, ìŠ¤í‚µ")
            continue
            
        left, right = ln.split("\t", 1)
        _id = left.strip()
        logger.debug(f"ì¤„ {i+1}: ID='{_id}', ë‹µë³€='{right}'")
        
        if _id not in id_set:
            logger.debug(f"ì¤„ {i+1}: ID '{_id}'ê°€ ì˜ˆìƒ ëª©ë¡ì— ì—†ìŒ, ìŠ¤í‚µ")
            continue
            
        # ì²« ë²ˆì§¸ 1~5 ì¶”ì¶œ
        m = re.search(r"[1-5]", right)
        if m:
            answer = float(m.group(0))
            out[_id] = answer
            logger.debug(f"ì¤„ {i+1}: ID '{_id}' -> ë‹µë³€ {answer}")
        else:
            logger.debug(f"ì¤„ {i+1}: ID '{_id}'ì˜ ë‹µë³€ì—ì„œ 1~5 ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    return out

# -----------------------------
# í‰ê°€ íŒŒì´í”„ë¼ì¸
# -----------------------------

def run_eval_pipeline(
    json_list: List[dict],
    models: List[str],
    sample_size: int = 300,
    batch_size: int = 50,
    seed: int = 42,
    mock_mode: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ë°˜í™˜:
      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set)
      pred_long   : (id, model_name, answer) ë¡± í¬ë§·
      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ
      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)
    """
    logger.info(f"í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}")
    
    # (1) JSON â†’ df_all
    logger.info("1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    df_all = json_to_df_all(json_list)
    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)
    logger.info(f"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ")

    # (2) ìƒ˜í”Œë§
    logger.info(f"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
    df_sample = df_all.sample(n=sample_size, random_state=seed).reset_index(drop=True)
    logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ")

    # (3) ë°°ì¹˜ ë¶„í• 
    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]
    logger.info(f"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ")

    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì 
    logger.info("4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...")
    rows = []
    invalid_responses = []  # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ìš©
    total_calls = len(batches) * len(models)
    
    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ
    with tqdm(total=total_calls, desc="ëª¨ë¸ í˜¸ì¶œ ì§„í–‰", unit="call") as pbar:
        for bidx, bdf in enumerate(batches, 1):
            user_prompt = build_user_prompt(bdf)
            ids = bdf["id"].tolist()
            
            for model in models:
                try:
                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ
                    pbar.set_description(f"ë°°ì¹˜ {bidx}/{len(batches)} - {model}")
                    
                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode)
                    parsed = parse_model_output(raw, ids)
                    
                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦
                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))
                    logger.info(f"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡")
                    
                    # ë¬´íš¨ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ë° ì €ì¥
                    if valid_predictions < len(ids):
                        logger.warning(f"ë°°ì¹˜ {bidx} - {model}: ë¬´íš¨ ì˜ˆì¸¡ ê°ì§€!")
                        logger.warning(f"ì˜ˆìƒ ID: {ids}")
                        logger.warning(f"ëª¨ë¸ ì›ì‹œ ì¶œë ¥:\n{raw}")
                        logger.warning(f"íŒŒì‹±ëœ ê²°ê³¼: {parsed}")
                        
                        # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)
                        for _id in ids:
                            if np.isnan(parsed[_id]):
                                # ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                question_info = bdf[bdf['id'] == _id].iloc[0] if len(bdf[bdf['id'] == _id]) > 0 else None
                                
                                invalid_response = {
                                    "model_name": model,
                                    "batch_id": bidx,
                                    "question_id": _id,
                                    "question": question_info['question'] if question_info is not None else "ì •ë³´ ì—†ìŒ",
                                    "options": {
                                        "opt1": question_info['opt1'] if question_info is not None else "",
                                        "opt2": question_info['opt2'] if question_info is not None else "",
                                        "opt3": question_info['opt3'] if question_info is not None else "",
                                        "opt4": question_info['opt4'] if question_info is not None else "",
                                        "opt5": question_info['opt5'] if question_info is not None else ""
                                    },
                                    "correct_answer": list(question_info['answer_set']) if question_info is not None else [],
                                    "model_raw_output": raw,
                                    "parsed_result": parsed[_id],
                                    "timestamp": dt.datetime.now().isoformat()
                                }
                                invalid_responses.append(invalid_response)
                    
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": parsed[_id]})
                    
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": np.nan})
                    pbar.update(1)

    logger.info("5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    pred_long = pd.DataFrame(rows)
    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)
    
    # (5) ì™€ì´ë“œ í¬ë§·
    pred_wide = pred_long.pivot(index="id", columns="model_name", values="answer").reset_index()
    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)

    # (6) ì •í™•ë„ ê³„ì‚°
    logger.info("6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...")
    key = df_sample[["id", "answer_set"]].copy()
    
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)

    merged = pred_long.merge(key, on="id", how="left")
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)

    acc_by_model = (
        merged.groupby("model_name", dropna=False)["correct"]
        .mean()
        .reset_index()
        .rename(columns={"correct": "accuracy"})
        .sort_values("accuracy", ascending=False)
    )
    
    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    logger.info("í‰ê°€ ì™„ë£Œ!")
    logger.info("ëª¨ë¸ë³„ ì •í™•ë„:")
    for _, row in acc_by_model.iterrows():
        logger.info(f"  {row['model_name']}: {row['accuracy']:.3f}")
    
    # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥
    if 'invalid_responses' in locals() and invalid_responses:
        save_invalid_responses(invalid_responses, "evaluation")
    
    return df_all, pred_long, pred_wide, acc_by_model

def run_eval_pipeline_improved(
    json_list: List[dict],
    models: List[str],
    sample_size: int = 300,
    batch_size: int = 50,
    seed: int = 42,
    mock_mode: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    O, X ë¬¸ì œë¥¼ ì§€ì›í•˜ëŠ” ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸
    ë°˜í™˜:
      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set + is_ox_question)
      pred_long   : (id, model_name, answer) ë¡± í¬ë§·
      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ
      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)
    """
    logger.info(f"ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}")
    
    # (1) JSON â†’ df_all (O, X ë¬¸ì œ ì§€ì›)
    logger.info("1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    df_all = json_to_df_all_improved(json_list)
    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)
    logger.info(f"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ")

    # O, X ë¬¸ì œ ë¶„ì„
    ox_questions, regular_questions = analyze_ox_questions(df_all)

    # (2) ìƒ˜í”Œë§
    logger.info(f"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
    df_sample = df_all.sample(n=sample_size, random_state=seed).reset_index(drop=True)
    logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ")

    # ìƒ˜í”Œì—ì„œ O, X ë¬¸ì œ ë¹„ìœ¨ í™•ì¸
    sample_ox = df_sample[df_sample['is_ox_question'] == True]
    sample_regular = df_sample[df_sample['is_ox_question'] == False]
    logger.info(f"ìƒ˜í”Œ ë‚´ O, X ë¬¸ì œ: {len(sample_ox)}ê°œ, ì¼ë°˜ ê°ê´€ì‹: {len(sample_regular)}ê°œ")

    # (3) ë°°ì¹˜ ë¶„í• 
    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]
    logger.info(f"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ")

    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì 
    logger.info("4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...")
    rows = []
    invalid_responses = []  # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ìš©
    total_calls = len(batches) * len(models)
    
    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ
    with tqdm(total=total_calls, desc="ëª¨ë¸ í˜¸ì¶œ ì§„í–‰", unit="call") as pbar:
        for bidx, bdf in enumerate(batches, 1):
            user_prompt = build_user_prompt(bdf)
            ids = bdf["id"].tolist()
            
            for model in models:
                try:
                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ
                    pbar.set_description(f"ë°°ì¹˜ {bidx}/{len(batches)} - {model}")
                    
                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode)
                    parsed = parse_model_output(raw, ids)
                    
                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦
                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))
                    logger.info(f"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡")
                    
                    # ë¬´íš¨ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ë° ì €ì¥
                    if valid_predictions < len(ids):
                        logger.warning(f"ë°°ì¹˜ {bidx} - {model}: ë¬´íš¨ ì˜ˆì¸¡ ê°ì§€!")
                        logger.warning(f"ì˜ˆìƒ ID: {ids}")
                        logger.warning(f"ëª¨ë¸ ì›ì‹œ ì¶œë ¥:\n{raw}")
                        logger.warning(f"íŒŒì‹±ëœ ê²°ê³¼: {parsed}")
                        
                        # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)
                        for _id in ids:
                            if np.isnan(parsed[_id]):
                                # ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                question_info = bdf[bdf['id'] == _id].iloc[0] if len(bdf[bdf['id'] == _id]) > 0 else None
                                
                                invalid_response = {
                                    "model_name": model,
                                    "batch_id": bidx,
                                    "question_id": _id,
                                    "question": question_info['question'] if question_info is not None else "ì •ë³´ ì—†ìŒ",
                                    "options": {
                                        "opt1": question_info['opt1'] if question_info is not None else "",
                                        "opt2": question_info['opt2'] if question_info is not None else "",
                                        "opt3": question_info['opt3'] if question_info is not None else "",
                                        "opt4": question_info['opt4'] if question_info is not None else "",
                                        "opt5": question_info['opt5'] if question_info is not None else ""
                                    },
                                    "correct_answer": list(question_info['answer_set']) if question_info is not None else [],
                                    "model_raw_output": raw,
                                    "parsed_result": parsed[_id],
                                    "timestamp": dt.datetime.now().isoformat()
                                }
                                invalid_responses.append(invalid_response)
                    
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": parsed[_id]})
                    
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": np.nan})
                    pbar.update(1)

    logger.info("5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    pred_long = pd.DataFrame(rows)
    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)
    
    # (5) ì™€ì´ë“œ í¬ë§·
    pred_wide = pred_long.pivot(index="id", columns="model_name", values="answer").reset_index()
    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)

    # (6) ì •í™•ë„ ê³„ì‚°
    logger.info("6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...")
    key = df_sample[["id", "answer_set"]].copy()
    
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)

    merged = pred_long.merge(key, on="id", how="left")
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)

    acc_by_model = (
        merged.groupby("model_name", dropna=False)["correct"]
        .mean()
        .reset_index()
        .rename(columns={"correct": "accuracy"})
        .sort_values("accuracy", ascending=False)
    )
    
    # O, X ë¬¸ì œì™€ ì¼ë°˜ ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„
    sample_with_type = df_sample[["id", "is_ox_question"]].copy()
    merged_with_type = merged.merge(sample_with_type, on="id", how="left")
    
    # O, X ë¬¸ì œ ì •í™•ë„
    ox_accuracy = merged_with_type[merged_with_type['is_ox_question'] == True].groupby("model_name")["correct"].mean()
    regular_accuracy = merged_with_type[merged_with_type['is_ox_question'] == False].groupby("model_name")["correct"].mean()
    
    logger.info("í‰ê°€ ì™„ë£Œ!")
    logger.info("ëª¨ë¸ë³„ ì „ì²´ ì •í™•ë„:")
    for _, row in acc_by_model.iterrows():
        logger.info(f"  {row['model_name']}: {row['accuracy']:.3f}")
    
    if len(ox_accuracy) > 0:
        logger.info("O, X ë¬¸ì œ ì •í™•ë„:")
        for model, acc in ox_accuracy.items():
            logger.info(f"  {model}: {acc:.3f}")
    
    # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥
    if 'invalid_responses' in locals() and invalid_responses:
        save_invalid_responses(invalid_responses, "evaluation")
    
    return df_all, pred_long, pred_wide, acc_by_model

# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -----------------------------

def analyze_ox_questions(df: pd.DataFrame):
    """O, X ë¬¸ì œ ë¶„ì„"""
    ox_questions = df[df['is_ox_question'] == True]
    regular_questions = df[df['is_ox_question'] == False]
    
    print(f"ğŸ“Š ë¬¸ì œ ìœ í˜• ë¶„ì„")
    print(f"   - O, X ë¬¸ì œ: {len(ox_questions)}ê°œ")
    print(f"   - ì¼ë°˜ ê°ê´€ì‹: {len(regular_questions)}ê°œ")
    print(f"   - ì „ì²´: {len(df)}ê°œ")
    
    if len(ox_questions) > 0:
        print(f"\nğŸ” O, X ë¬¸ì œ ì •ë‹µ ë¶„í¬:")
        ox_answers = ox_questions['answer_set'].apply(lambda x: list(x) if x else [])
        answer_counts = {}
        for answers in ox_answers:
            for ans in answers:
                answer_counts[ans] = answer_counts.get(ans, 0) + 1
        
        for ans, count in sorted(answer_counts.items()):
            answer_text = "O" if ans == 1 else "X" if ans == 2 else str(ans)
            print(f"   - {answer_text}: {count}ê°œ")
    
    return ox_questions, regular_questions

def print_evaluation_summary(acc_df: pd.DataFrame, pred_long_df: pd.DataFrame):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìƒì„¸ ìš”ì•½")
    print("="*80)
    
    # ê¸°ë³¸ í†µê³„
    total_predictions = len(pred_long_df)
    valid_predictions = len(pred_long_df.dropna(subset=['answer']))
    invalid_predictions = total_predictions - valid_predictions
    
    print(f"ğŸ“ˆ ì „ì²´ ì˜ˆì¸¡ ìˆ˜: {total_predictions:,}")
    print(f"âœ… ìœ íš¨ ì˜ˆì¸¡: {valid_predictions:,} ({valid_predictions/total_predictions*100:.1f}%)")
    print(f"âŒ ë¬´íš¨ ì˜ˆì¸¡: {invalid_predictions:,} ({invalid_predictions/total_predictions*100:.1f}%)")
    
    # ëª¨ë¸ë³„ ì •í™•ë„
    print(f"\nğŸ† ëª¨ë¸ë³„ ì •í™•ë„ ìˆœìœ„:")
    for i, (_, row) in enumerate(acc_df.iterrows(), 1):
        accuracy = row['accuracy']
        if pd.isna(accuracy):
            print(f"  {i}. {row['model_name']}: N/A (ë°ì´í„° ì—†ìŒ)")
        else:
            print(f"  {i}. {row['model_name']}: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    print("="*80)

def save_invalid_responses(invalid_responses: List[Dict], filename_prefix: str = "evaluation"):
    """ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)"""
    if not invalid_responses:
        logger.info("ë¬´íš¨ ì˜ˆì¸¡ì´ ì—†ì–´ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M%S")
    invalid_filename = f"evaluation_result/{filename_prefix}_invalid_responses_{timestamp}.json"
    
    try:
        with open(invalid_filename, 'w', encoding='utf-8') as f:
            json.dump(invalid_responses, f, ensure_ascii=False, indent=2)
        logger.info(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥: {invalid_filename}")
        logger.info(f"ì´ {len(invalid_responses)}ê°œì˜ ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½ ì •ë³´ë„ ì¶œë ¥
        model_counts = {}
        for resp in invalid_responses:
            model = resp.get('model_name', 'unknown')
            model_counts[model] = model_counts.get(model, 0) + 1
        
        logger.info("ëª¨ë¸ë³„ ë¬´íš¨ ì˜ˆì¸¡ ìˆ˜:")
        for model, count in model_counts.items():
            logger.info(f"  {model}: {count}ê°œ")
            
    except Exception as e:
        logger.error(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def save_detailed_logs(pred_long_df: pd.DataFrame, filename_prefix: str = "evaluation"):
    """ìƒì„¸í•œ ë¡œê·¸ë¥¼ CSVë¡œ ì €ì¥"""
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
    
    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
    pred_log_filename = f"evaluation_result/log/{filename_prefix}_predictions_{timestamp}.csv"
    pred_long_df.to_csv(pred_log_filename, index=False, encoding='utf-8-sig')
    logger.info(f"ìƒì„¸ ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥: {pred_log_filename}")
    
    # ëª¨ë¸ë³„ í†µê³„
    model_stats = pred_long_df.groupby('model_name').agg({
        'answer': [lambda x: x.count(), lambda x: x.notna().sum(), lambda x: x.isna().sum()]
    }).round(3)
    model_stats.columns = ['ì´_ì˜ˆì¸¡ìˆ˜', 'ìœ íš¨_ì˜ˆì¸¡ìˆ˜', 'ë¬´íš¨_ì˜ˆì¸¡ìˆ˜']
    model_stats['ìœ íš¨ìœ¨'] = (model_stats['ìœ íš¨_ì˜ˆì¸¡ìˆ˜'] / model_stats['ì´_ì˜ˆì¸¡ìˆ˜'] * 100).round(1)
    
    stats_filename = f"evaluation_result/log/{filename_prefix}_model_stats_{timestamp}.csv"
    model_stats.to_csv(stats_filename, encoding='utf-8-sig')
    logger.info(f"ëª¨ë¸ í†µê³„ ì €ì¥: {stats_filename}")

def check_data_quality(df_all: pd.DataFrame, df_sample: pd.DataFrame):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")
    
    issues = []
    
    # 1. ë¹ˆ ë¬¸ì œ ê²€ì‚¬
    empty_questions = df_all[df_all['question'].str.strip() == '']
    if len(empty_questions) > 0:
        issues.append(f"ë¹ˆ ë¬¸ì œ: {len(empty_questions)}ê°œ")
    
    # 2. ë¹ˆ ì„ ì§€ ê²€ì‚¬
    empty_options = df_all[(df_all['opt1'].str.strip() == '') & 
                          (df_all['opt2'].str.strip() == '') & 
                          (df_all['opt3'].str.strip() == '') & 
                          (df_all['opt4'].str.strip() == '') & 
                          (df_all['opt5'].str.strip() == '')]
    if len(empty_options) > 0:
        issues.append(f"ë¹ˆ ì„ ì§€ ë¬¸ì œ: {len(empty_options)}ê°œ")
    
    # 3. ì •ë‹µ ì—†ëŠ” ë¬¸ì œ ê²€ì‚¬
    no_answer = df_all[df_all['answer_set'].apply(len) == 0]
    if len(no_answer) > 0:
        issues.append(f"ì •ë‹µ ì—†ëŠ” ë¬¸ì œ: {len(no_answer)}ê°œ")
    
    # 4. ì¤‘ë³µ ë¬¸ì œ ê²€ì‚¬
    duplicates = df_all[df_all.duplicated(subset=['question'], keep=False)]
    if len(duplicates) > 0:
        issues.append(f"ì¤‘ë³µ ë¬¸ì œ: {len(duplicates)}ê°œ")
    
    if issues:
        logger.warning("ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œê²¬:")
        for issue in issues:
            logger.warning(f"  - {issue}")
    else:
        logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ âœ…")
    
    return issues

def save_results_to_excel(df_all: pd.DataFrame, pred_wide: pd.DataFrame, acc: pd.DataFrame, filename: str = None):
    """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
        filename = f"evaluation_results_{timestamp}.xlsx"
    
    logger.info(f"ê²°ê³¼ë¥¼ {filename}ì— ì €ì¥ ì¤‘...")
    
    try:
        with pd.ExcelWriter(filename, engine="openpyxl") as w:
            df_all.to_excel(w, index=False, sheet_name="ì „ì²´ë°ì´í„°")
            pred_wide.to_excel(w, index=False, sheet_name="ëª¨ë¸ë³„ì˜ˆì¸¡")
            acc.to_excel(w, index=False, sheet_name="ì •í™•ë„")
            
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# -----------------------------
# íƒœê·¸ ëŒ€ì¹˜ í•¨ìˆ˜ë“¤
# -----------------------------

def replace_tags_in_text(text: str, additional_tag_data: list) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ {f_0000_0000}ì´ë‚˜ {tb_0000_0000} ê°™ì€ íƒœê·¸ë¥¼ additional_tag_dataì—ì„œ ì°¾ì•„ì„œ ëŒ€ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        text: ëŒ€ì¹˜í•  í…ìŠ¤íŠ¸
        additional_tag_data: íƒœê·¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        íƒœê·¸ê°€ ëŒ€ì¹˜ëœ í…ìŠ¤íŠ¸
    """
    if not text or not additional_tag_data:
        return text
    
    # íƒœê·¸ íŒ¨í„´ ë§¤ì¹­: {f_0000_0000}, {tb_0000_0000}, {img_0000_0000}, {etc_0000_0000}, {note_0000_0000}
    tag_pattern = r'\{(f_\d{4}_\d{4}|tb_\d{4}_\d{4}|note_\d{4}_\d{4})\}'
    
    def replace_tag(match):
        tag_with_braces = match.group(0)  # {f_0000_0000}
        tag_without_braces = match.group(1)  # f_0000_0000
        
        # additional_tag_dataì—ì„œ í•´ë‹¹ íƒœê·¸ ì°¾ê¸°
        for tag_data in additional_tag_data:
            if tag_data.get('tag') == tag_with_braces:
                # data í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                if 'data' in tag_data:
                    data = tag_data.get('data', {})
                    if isinstance(data, dict):
                        # dataì—ì„œ ì ì ˆí•œ í•„ë“œ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: content, text, description, caption)
                        for field in ['content', 'text', 'description', 'caption']:
                            if field in data and data[field]:
                                return str(data[field])
                        
                        # file_pathê°€ ìˆìœ¼ë©´ íŒŒì¼ëª… í‘œì‹œ
                        if 'file_path' in data and data['file_path']:
                            return f"[{os.path.basename(data['file_path'])}]"
                    
                    # dataê°€ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    elif isinstance(data, str) and data:
                        return data
                    
                    # dataê°€ ë¦¬ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                    elif isinstance(data, list) and data:
                        return str(data[0])
                
                # data í•„ë“œê°€ ì—†ëŠ” ê²½ìš°, ì§ì ‘ í•„ë“œì—ì„œ ì°¾ê¸°
                else:
                    # ì§ì ‘ í•„ë“œì—ì„œ ì ì ˆí•œ ë‚´ìš© ì°¾ê¸° (ìš°ì„ ìˆœìœ„: content, text, description, caption)
                    for field in ['content', 'text', 'description', 'caption']:
                        if field in tag_data and tag_data[field]:
                            return str(tag_data[field])
                    
                    # file_pathê°€ ìˆìœ¼ë©´ íŒŒì¼ëª… í‘œì‹œ
                    if 'file_path' in tag_data and tag_data['file_path']:
                        return f"[{os.path.basename(tag_data['file_path'])}]"
        
        # íƒœê·¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì›ë³¸ íƒœê·¸ ìœ ì§€
        return tag_with_braces
    
    return re.sub(tag_pattern, replace_tag, text)

def replace_tags_in_qna_data(qna_data: dict, additional_tag_data: list) -> dict:
    """
    Q&A ë°ì´í„°ì˜ questionê³¼ optionsì—ì„œ íƒœê·¸ë¥¼ ëŒ€ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        qna_data: Q&A ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (ì „ì²´ qna ê°ì²´ ë˜ëŠ” qna_data ë¶€ë¶„)
        additional_tag_data: ì¶”ê°€ íƒœê·¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        íƒœê·¸ê°€ ëŒ€ì¹˜ëœ Q&A ë°ì´í„°
    """
    if not qna_data:
        return qna_data
    
    if not additional_tag_data:
        return qna_data
    
    # qna_dataê°€ ì „ì²´ qna ê°ì²´ì¸ ê²½ìš° qna_data ë¶€ë¶„ì„ ì¶”ì¶œ
    if 'qna_data' in qna_data:
        qna_info = qna_data['qna_data']
    else:
        # ì´ë¯¸ qna_data ë¶€ë¶„ë§Œ ì „ë‹¬ëœ ê²½ìš°
        qna_info = qna_data
    
    if 'description' in qna_info:
        desc = qna_info['description']
        
        # question í•„ë“œ ì²˜ë¦¬
        if 'question' in desc and desc['question']:
            desc['question'] = replace_tags_in_text(desc['question'], additional_tag_data)
        
        # options í•„ë“œ ì²˜ë¦¬ (ë¦¬ìŠ¤íŠ¸)
        if 'options' in desc and desc['options']:
            if isinstance(desc['options'], list):
                desc['options'] = [replace_tags_in_text(option, additional_tag_data) for option in desc['options']]
            else:
                desc['options'] = replace_tags_in_text(desc['options'], additional_tag_data)
        
        # answer í•„ë“œ ì²˜ë¦¬
        if 'answer' in desc and desc['answer']:
            desc['answer'] = replace_tags_in_text(desc['answer'], additional_tag_data)
        
        # explanation í•„ë“œ ì²˜ë¦¬
        if 'explanation' in desc and desc['explanation']:
            desc['explanation'] = replace_tags_in_text(desc['explanation'], additional_tag_data)
    
    return qna_data

# -----------------------------
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# -----------------------------

def load_data_from_directory(data_path: str, apply_tag_replacement: bool = True) -> List[dict]:
    """ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    json_files = []
    for root, _, files in os.walk(data_path):
        for f in files:
            if f.endswith(".json") and ('merged' not in f):
                json_files.append(os.path.join(root, f))
    
    logger.info(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")
    
    all_data = []
    for file_path in json_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    all_data.extend(data)
                else:
                    all_data.append(data)
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file_path} - {str(e)}")
    
    logger.info(f"ë¡œë“œëœ ì´ ë°ì´í„° ìˆ˜: {len(all_data)}")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì ìš©
    if apply_tag_replacement:
        logger.info("íƒœê·¸ ëŒ€ì¹˜ ì ìš© ì¤‘...")
        processed_count = 0
        for item in all_data:
            if 'additional_tags_found' in item and item['additional_tags_found']:
                if 'additional_tag_data' in item:
                    item['qna_data'] = replace_tags_in_qna_data(
                        item['qna_data'], 
                        item['additional_tag_data']
                    )
                    processed_count += 1
        logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì™„ë£Œ: {processed_count}ê°œ í•­ëª© ì²˜ë¦¬")
    
    return all_data

def filter_multiple_choice_questions(data: List[dict]) -> List[dict]:
    """ê°ê´€ì‹ ë¬¸ì œë§Œ í•„í„°ë§"""
    multiple_choice = []
    for item in data:
        if item.get('qna_type') == "multiple-choice":
            multiple_choice.append(item)
    
    logger.info(f"ê°ê´€ì‹ ë¬¸ì œ ìˆ˜: {len(multiple_choice)}")
    return multiple_choice

# -----------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -----------------------------

def main():
    parser = argparse.ArgumentParser(description='LLM í‰ê°€ ì‹œìŠ¤í…œ')
    parser.add_argument('--data_path', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--sample_size', type=int, default=100, help='ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ê°’: 100)')
    parser.add_argument('--batch_size', type=int, default=5, help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--models', nargs='+', default=['anthropic/claude-sonnet-4.5'], help='í‰ê°€í•  ëª¨ë¸ ëª©ë¡')
    parser.add_argument('--mock_mode', action='store_true', help='Mock ëª¨ë“œë¡œ ì‹¤í–‰ (ì‹¤ì œ API í˜¸ì¶œ ì—†ìŒ)')
    parser.add_argument('--use_ox_support', action='store_true', help='O, X ë¬¸ì œ ì§€ì› í™œì„±í™”')
    parser.add_argument('--apply_tag_replacement', action='store_true', help='íƒœê·¸ ëŒ€ì¹˜ ì ìš© (ê¸°ë³¸ê°’: True)')
    parser.add_argument('--no_tag_replacement', action='store_true', help='íƒœê·¸ ëŒ€ì¹˜ ë¹„í™œì„±í™”')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    
    logger.info("=" * 60)
    logger.info("LLM í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    logger.info(f"ìƒ˜í”Œ í¬ê¸°: {args.sample_size}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"ëª¨ë¸: {args.models}")
    logger.info(f"Mock ëª¨ë“œ: {args.mock_mode}")
    logger.info(f"O, X ë¬¸ì œ ì§€ì›: {args.use_ox_support}")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì˜µì…˜ ì²˜ë¦¬
    apply_tag_replacement = not args.no_tag_replacement
    logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì ìš©: {apply_tag_replacement}")
    
    try:
        # ë°ì´í„° ë¡œë”©
        logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
        all_data = load_data_from_directory(args.data_path, apply_tag_replacement)
        multiple_choice_data = filter_multiple_choice_questions(all_data)
        
        if len(multiple_choice_data) == 0:
            logger.error("ê°ê´€ì‹ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìƒ˜í”Œë§
        if len(multiple_choice_data) > args.sample_size:
            random.seed(args.seed)
            sample_data = random.sample(multiple_choice_data, args.sample_size)
        else:
            sample_data = multiple_choice_data
            logger.info(f"ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(sample_data)}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        if args.use_ox_support:
            df_temp = json_to_df_all_improved(sample_data)
        else:
            df_temp = json_to_df_all(sample_data)
        
        quality_issues = check_data_quality(df_temp, df_temp.sample(n=min(50, len(df_temp)), random_state=args.seed))
        
        # O, X ë¬¸ì œ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support:
            ox_questions, regular_questions = analyze_ox_questions(df_temp)
        
        # í‰ê°€ ì‹¤í–‰
        logger.info("í‰ê°€ ì‹¤í–‰ ì¤‘...")
        if args.use_ox_support:
            df_all, pred_long, pred_wide, acc = run_eval_pipeline_improved(
                sample_data, args.models, args.sample_size, args.batch_size, args.seed, args.mock_mode
            )
        else:
            df_all, pred_long, pred_wide, acc = run_eval_pipeline(
                sample_data, args.models, args.sample_size, args.batch_size, args.seed, args.mock_mode
            )
        
        # ê²°ê³¼ ì¶œë ¥
        print_evaluation_summary(acc, pred_long)
        
        # O, X ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support and 'is_ox_question' in df_all.columns:
            sample_with_type = df_all[df_all['id'].isin(pred_long['id'])][['id', 'is_ox_question']].copy()
            merged_with_type = pred_long.merge(sample_with_type, on='id', how='left')
            
            if len(merged_with_type[merged_with_type['is_ox_question'] == True]) > 0:
                print("\n" + "="*60)
                print("ğŸ“Š O, X ë¬¸ì œ vs ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„ ë¹„êµ")
                print("="*60)
                
                # O, X ë¬¸ì œ ì •í™•ë„
                ox_correct = merged_with_type[merged_with_type['is_ox_question'] == True]['answer'].notna().sum()
                ox_total = len(merged_with_type[merged_with_type['is_ox_question'] == True])
                ox_acc = ox_correct / ox_total if ox_total > 0 else 0
                
                # ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„
                regular_correct = merged_with_type[merged_with_type['is_ox_question'] == False]['answer'].notna().sum()
                regular_total = len(merged_with_type[merged_with_type['is_ox_question'] == False])
                regular_acc = regular_correct / regular_total if regular_total > 0 else 0
                
                print(f"O, X ë¬¸ì œ: {ox_correct}/{ox_total} ({ox_acc:.1%})")
                print(f"ì¼ë°˜ ê°ê´€ì‹: {regular_correct}/{regular_total} ({regular_acc:.1%})")
        
        # ìƒì„¸ ë¡œê·¸ ì €ì¥
        save_detailed_logs(pred_long, "evaluation")
        
        # Excel íŒŒì¼ ì €ì¥
        save_results_to_excel(df_all, pred_wide, acc)
        
        logger.info("=" * 60)
        logger.info("í‰ê°€ ì™„ë£Œ")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    main()
