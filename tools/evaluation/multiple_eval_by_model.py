#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM í‰ê°€ ì‹œìŠ¤í…œ - í†µí•© ë²„ì „
O, X ë¬¸ì œë¥¼ í¬í•¨í•œ ê°ê´€ì‹ ë¬¸ì œ í‰ê°€ ì‹œìŠ¤í…œ

ì‚¬ìš©ë²•:
    # OpenRouter API ëª¨ë“œ (ê¸°ë³¸ê°’)
    python multiple_eval_by_model.py --data_path /path/to/data --sample_size 1000 --api --mock_mode
    
    # vLLM ì„œë²„ ëª¨ë“œ
    python multiple_eval_by_model.py --data_path /path/to/data --sample_size 1000 --server --mock_mode
"""

import os
import pandas as pd
import numpy as np
import re
import time
import logging
import random
import json
import datetime as dt
from typing import List, Dict, Tuple, Iterable, Set
from dataclasses import dataclass
from tqdm import tqdm
import argparse

# -----------------------------
# ë¡œê¹… ì„¤ì •
# -----------------------------
# í™ˆ ë””ë ‰í† ë¦¬ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
global home_dir
home_dir = os.path.expanduser("~")
global project_root
project_root = None

# í™ˆ ë””ë ‰í† ë¦¬ì—ì„œ SFAIcenter í”„ë¡œì íŠ¸ ì°¾ê¸°
for root, dirs, files in os.walk(home_dir):
    if 'SFAIcenter' in dirs:
        project_root = os.path.join(root, 'SFAIcenter')
        break

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
if project_root is None:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)

log_dir = os.path.join(project_root, 'logs')
log_file = os.path.join(log_dir, 'multiple_eval_by_model.log')

# logs ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# -----------------------------
# ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ê·œí™”
# -----------------------------
CIRCLED_MAP = {"â‘ ":"1","â‘¡":"2","â‘¢":"3","â‘£":"4","â‘¤":"5"}

def normalize_option_text(s: str) -> str:
    """ì„ ì§€ ì•ì— ë¶™ì€ â‘ ~â‘¤, 1), (1), 1. ë“± ë²ˆí˜¸ í‘œê¸°ë¥¼ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë‚¨ê¹€."""
    if s is None:
        return ""
    s = str(s).strip()
    # â‘ ~â‘¤ ì œê±°
    s = re.sub(r"^\s*[â‘ -â‘¤]\s*", "", s)
    # 1), (1), 1. ë“± ì œê±°
    s = re.sub(r"^\s*(?:\(?([1-5])\)?[.)])\s*", "", s)
    return s.strip()

def parse_answer_set(ans: str) -> Set[int]:
    """'â‘ , â‘¤' ê°™ì€ ë³µìˆ˜ì •ë‹µë„ {1,5}ë¡œ íŒŒì‹±. ë¹ˆ/ì´ìƒê°’ì€ ë¹ˆ set."""
    if not ans:
        return set()
    s = str(ans)
    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
    for k, v in CIRCLED_MAP.items():
        s = s.replace(k, v)
    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ
    nums = re.findall(r"[1-5]", s)
    return set(int(n) for n in nums)

# -----------------------------
# O, X ë¬¸ì œ ì²˜ë¦¬ ê°œì„ 
# -----------------------------

def is_ox_question(question: str, options: list) -> bool:
    """O, X ë¬¸ì œì¸ì§€ íŒë‹¨"""
    if not options or len(options) == 0:
        return False
    # optionsê°€ ë¹„ì–´ìˆê±°ë‚˜ 2ê°œ ì´í•˜ì´ê³ , O/X í˜•íƒœì¸ì§€ í™•ì¸
    if len(options) <= 2:
        option_text = " ".join(options).upper()
        return "O" in option_text or "X" in option_text
    return False

def parse_answer_set_improved(ans: str, question: str = "", options: list = None) -> Set[int]:
    """ê°œì„ ëœ ì •ë‹µ íŒŒì‹± í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬"""
    if not ans:
        return set()
    s = str(ans).strip()
    
    # O, X ë¬¸ì œ ì²˜ë¦¬
    if s.upper() in ['O', 'X']:
        # O, X ë¬¸ì œëŠ” 1ë²ˆ(O), 2ë²ˆ(X)ìœ¼ë¡œ ë³€í™˜
        return {1} if s.upper() == 'O' else {2}
    
    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
    for k, v in CIRCLED_MAP.items():
        s = s.replace(k, v)
    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ
    nums = re.findall(r"[1-5]", s)
    return set(int(n) for n in nums)

# -----------------------------
# JSON â†’ df_all ë³€í™˜
# -----------------------------

def json_to_df_all(json_list: List[dict]) -> pd.DataFrame:
    """
    ì…ë ¥ JSON(list[dict])ì„ íŒŒì‹±í•´ df_all ìƒì„±.
    ì»¬ëŸ¼: subject, domain, subdomain, book_id, tag, id, question, opt1..opt5, answer_set
    """
    rows = []
    for item in json_list:
        book_id = str(item.get("file_id", ""))
        
        # Mock exam íŒŒì¼ êµ¬ì¡° ì²˜ë¦¬ (qna_dataê°€ ì—†ëŠ” ê²½ìš°)
        if "qna_data" in item:
            # ì¼ë°˜ íŒŒì¼ êµ¬ì¡°
            qna = item.get("qna_data", {}) or {}
            tag  = qna.get("tag", "")
            desc = qna.get("description", {}) or {}
            q    = (desc.get("question") or "").strip()
            opts = desc.get("options") or []
            ans_set = parse_answer_set(desc.get("answer", ""))
            domain = item.get("qna_domain", "")
            subdomain = item.get("qna_subdomain", "")
        else:
            # Mock exam íŒŒì¼ êµ¬ì¡°
            tag = item.get("tag", "")
            q = (item.get("question") or "").strip()
            opts = item.get("options") or []
            ans_set = parse_answer_set(item.get("answer", ""))
            domain = item.get("domain", "")
            subdomain = item.get("subdomain", "")
        
        # subject ì •ë³´ ì¶”ì¶œ
        subject = item.get("subject", "")
        
        # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
        opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        opts = [normalize_option_text(x) for x in opts]

        rows.append({
            "subject": subject,
            "domain": domain,
            "subdomain": subdomain,
            "book_id": book_id,
            "tag": tag,
            "id": f"{book_id}_{tag}",
            "question": q,
            "opt1": opts[0], "opt2": opts[1], "opt3": opts[2], "opt4": opts[3], "opt5": opts[4],
            "answer_set": ans_set
        })
    df = pd.DataFrame(rows)
    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)
    df = df.drop_duplicates("id", keep="last").reset_index(drop=True)
    return df

def json_to_df_all_improved(json_list: List[dict], use_ox_support: bool = False) -> pd.DataFrame:
    """
    ê°œì„ ëœ JSON â†’ df_all ë³€í™˜ í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬
    ì»¬ëŸ¼: subject, domain, subdomain, book_id, tag, id, question, opt1..opt5, answer_set [, is_ox_question]
    """
    rows = []
    for item in json_list:
        book_id = str(item.get("file_id", ""))
        
        # Mock exam íŒŒì¼ êµ¬ì¡° ì²˜ë¦¬ (qna_dataê°€ ì—†ëŠ” ê²½ìš°)
        if "qna_data" in item:
            # ì¼ë°˜ íŒŒì¼ êµ¬ì¡°
            qna = item.get("qna_data", {}) or {}
            tag  = qna.get("tag", "")
            desc = qna.get("description", {}) or {}
            q    = (desc.get("question") or "").strip()
            opts = desc.get("options") or []
            ans_set = parse_answer_set_improved(desc.get("answer", ""), q, opts)
            domain = item.get("qna_domain", "")
            subdomain = item.get("qna_subdomain", "")
        else:
            # Mock exam íŒŒì¼ êµ¬ì¡°
            tag = item.get("tag", "")
            q = (item.get("question") or "").strip()
            opts = item.get("options") or []
            ans_set = parse_answer_set_improved(item.get("answer", ""), q, opts)
            domain = item.get("domain", "")
            subdomain = item.get("subdomain", "")
        
        # O, X ë¬¸ì œì¸ì§€ íŒë‹¨ (ox ëª¨ë“œê°€ ì¼œì§„ ê²½ìš°ì—ë§Œ)
        is_ox = False
        if use_ox_support:
            is_ox = is_ox_question(q, opts)
            
            if is_ox:
                # O, X ë¬¸ì œëŠ” 2ê°œ ì„ ì§€ë¡œ ê³ ì •
                opts = ["O", "X"] + [""] * 3
            else:
                # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
                opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        else:
            # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
            opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        
        opts = [normalize_option_text(x) for x in opts]
        
        # subject ì •ë³´ ì¶”ì¶œ
        subject = item.get("subject", "")

        # ê¸°ë³¸ ì»¬ëŸ¼ êµ¬ì„±
        row_data = {
            "subject": subject,
            "domain": domain,
            "subdomain": subdomain,
            "book_id": book_id,
            "tag": tag,
            "id": f"{book_id}_{tag}",
            "question": q,
            "opt1": opts[0], "opt2": opts[1], "opt3": opts[2], "opt4": opts[3], "opt5": opts[4],
            "answer_set": ans_set
        }
        
        # ox ëª¨ë“œê°€ ì¼œì§„ ê²½ìš°ì—ë§Œ is_ox_question ì¶”ê°€
        if use_ox_support:
            row_data["is_ox_question"] = is_ox
        
        rows.append(row_data)
    df = pd.DataFrame(rows)
    # í˜¹ì‹œ id ì¤‘ë³µì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ìœ ì§€(í•„ìš”ì‹œ ì •ì±… ë³€ê²½)
    df = df.drop_duplicates("id", keep="last").reset_index(drop=True)
    return df

# -----------------------------
# ë°°ì¹˜ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
# -----------------------------

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸ˆìœµì „ë¬¸ê°€ì´ì ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•´, ê° ë¬¸ì œì˜ ì •ë‹µ "ë²ˆí˜¸ë§Œ" í•˜ë‚˜ ì„ íƒí•©ë‹ˆë‹¤.

ê·œì¹™
- ê° ë¬¸ì œëŠ” ê³ ìœ  IDì™€ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë‹¹ "ID<TAB>ë²ˆí˜¸" í˜•ì‹ìœ¼ë¡œë§Œ í•©ë‹ˆë‹¤. (ì˜ˆ: SS0000_q_0377_0001<TAB>3)
- ë‹¤ë¥¸ ê¸€ì, ë§ˆí¬ë‹¤ìš´, ì´ìœ , ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì œëŠ” ë³´ê¸°(1~5) ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¦…ë‹ˆë‹¤.
- ì¶œë ¥ ì¤„ ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
"""

def build_user_prompt(batch_df: pd.DataFrame) -> str:
    lines = []
    lines.append("ë‹¤ìŒì€ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´ ì •ë‹µ ë²ˆí˜¸ë§Œ ê³ ë¥´ì„¸ìš”.\n")
    lines.append("ë¬¸ì œë“¤")
    for _, r in batch_df.iterrows():
        lines.append(f"ID: {r['id']}")
        lines.append(f"Q: {r['question']}")
        lines.append(f"1) {r['opt1']}")
        lines.append(f"2) {r['opt2']}")
        lines.append(f"3) {r['opt3']}")
        lines.append(f"4) {r['opt4']}")
        lines.append(f"5) {r['opt5']}\n")
    lines.append("ì¶œë ¥ í˜•ì‹(ì¤‘ìš”)")
    for _, r in batch_df.iterrows():
        lines.append(f"{r['id']}\\t{{ë²ˆí˜¸}}")
    return "\n".join(lines)

# -----------------------------
# LLM í˜¸ì¶œ ì¶”ìƒí™”
# -----------------------------

# ëª¨ë¸ ìºì‹œë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_model_cache = {}
_config_cache = None
_query_models_module = None

def _load_config():
    """Config íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œ"""
    global _config_cache
    if _config_cache is None:
        import configparser
        config_path = os.popen(f"find {home_dir} -type f -name 'llm_config.ini'").read().strip()
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        
        _config_cache = configparser.ConfigParser()
        _config_cache.read(config_path, encoding='utf-8')
        logger.info(f"[CACHE] Config íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
    
    return _config_cache

def _load_query_models():
    """QueryModels ëª¨ë“ˆì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œ"""
    global _query_models_module
    if _query_models_module is None:
        import sys
        import os
        tools_dir = os.popen(f"find {home_dir}/SFAIcenter/ -type d -name 'tools'").read().strip()
        sys.path.append(tools_dir)
        try:
            import QueryModels
            _query_models_module = QueryModels
            logger.info(f"[CACHE] QueryModels ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ: {tools_dir}")
        except Exception as e:
            logger.error(f"[CACHE] QueryModels ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    return _query_models_module

def _load_model_cached(model_name: str):
    """ëª¨ë¸ì„ ìºì‹œì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ"""
    global _model_cache
    
    if model_name not in _model_cache:
        logger.info(f"[CACHE] ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        config = _load_config()
        QueryModels = _load_query_models()
        
        llm, tokenizer, sampling_params = QueryModels.load_model(model_name, config)
        _model_cache[model_name] = (llm, tokenizer, sampling_params)
        logger.info(f"[CACHE] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    else:
        logger.debug(f"[CACHE] ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    
    return _model_cache[model_name]

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œë¥¼ ì •ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ"""
    global _model_cache
    if _model_cache:
        logger.info(f"[CACHE] {len(_model_cache)}ê°œ ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì¤‘...")
        _model_cache.clear()
        logger.info("[CACHE] ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

def get_cache_info():
    """í˜„ì¬ ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    global _model_cache, _config_cache, _query_models_module
    return {
        "cached_models": list(_model_cache.keys()),
        "config_loaded": _config_cache is not None,
        "query_models_loaded": _query_models_module is not None
    }

def call_llm(model_name: str, system_prompt: str, user_prompt: str, mock_mode: bool=False, use_server_mode: bool=False, max_retries: int=3) -> str:
    """
    - mock_mode=Trueë©´ ì„ì˜ ë²ˆí˜¸(1~5)ë¥¼ ìƒì„±í•´ íŒŒì´í”„ë¼ì¸ ê²€ì¦ìš© ì¶œë ¥ ë°˜í™˜.
    - use_server_mode=Trueë©´ vLLM ì„œë²„ ëª¨ë“œë¡œ í˜¸ì¶œ
    - use_server_mode=Falseë©´ OpenRouter APIë¡œ í˜¸ì¶œ
    - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨
    """
    if mock_mode:
        logger.info(f"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘")
        # ì…ë ¥ user_promptì—ì„œ ID ëª©ë¡ íšŒìˆ˜
        ids = [ln.split("\t")[0] for ln in user_prompt.splitlines() if "\t{ë²ˆí˜¸}" in ln]
        if not ids:
            # IDë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            ids = [ln.split("ID: ")[1].strip() for ln in user_prompt.splitlines() if ln.startswith("ID: ")]
        
        # ë¬´ì‘ìœ„ ì˜ˆì¸¡(1~5)
        rng = np.random.default_rng(42)
        preds = rng.integers(1, 6, size=len(ids))
        result = "\n".join(f"{_id}\t{int(a)}" for _id, a in zip(ids, preds))
        logger.info(f"[MOCK] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - {len(ids)}ê°œ ë¬¸ì œ ì²˜ë¦¬")
        return result
    
    else:
        for attempt in range(max_retries):
            try:
                if use_server_mode:
                    # vLLM ì„œë²„ ëª¨ë“œ - ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©
                    logger.info(f"[VLLM] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})")
                    start_time = time.time()
                    
                    # ìºì‹œëœ ëª¨ë¸ ë¡œë“œ
                    llm, tokenizer, sampling_params = _load_model_cached(model_name)
                    QueryModels = _load_query_models()
                    
                    ans = QueryModels.query_vllm(llm, tokenizer, sampling_params, system_prompt, user_prompt, model_name)
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"[VLLM] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    
                    return ans
                else:
                    # OpenRouter API ëª¨ë“œ
                    logger.info(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})")
                    start_time = time.time()
                    
                    # ìºì‹œëœ configì™€ ëª¨ë“ˆ ì‚¬ìš©
                    config = _load_config()
                    QueryModels = _load_query_models()
                    
                    ans = QueryModels.query_openrouter(config, system_prompt, user_prompt, model_name)
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    
                    return ans
                
            except Exception as e:
                mode_str = "[VLLM]" if use_server_mode else "[API]"
                logger.warning(f"{mode_str} ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    logger.error(f"{mode_str} ëª¨ë¸ {model_name} ìµœì¢… ì‹¤íŒ¨ - ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")
                    raise e
                else:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    logger.info(f"{mode_str} {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)

# -----------------------------
# ëª¨ë¸ ì¶œë ¥ íŒŒì‹± 
# -----------------------------

def parse_model_output(raw: str, expected_ids: List[str], reasoning: bool = False) -> Dict[str, float]:
    """
    ëª¨ë¸ ì›ì‹œ ì¶œë ¥(raw)ì„ {id: answer(1~5)}ë¡œ ë³€í™˜.
    - 'ID\\të²ˆí˜¸' í¬ë§· ê¸°ì¤€ (ì¼ë°˜ ëª¨ë¸)
    - reasoning=Trueì¼ ê²½ìš°, ë‹µë³€ì—ì„œ IDì™€ ì •ë‹µì„ ì§ì ‘ ì°¾ìŒ
    - ì˜ëª»ëœ ì¤„/ëˆ„ë½ ì¤„ì€ NaN ì²˜ë¦¬
    """
    id_set = set(expected_ids)
    out: Dict[str, float] = {k: np.nan for k in expected_ids}
    
    if not raw or not raw.strip():
        logger.warning("ëª¨ë¸ ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return out

    # ì¶”ë¡  ëª¨ë¸ì¼ ê²½ìš°, ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ IDì™€ ì •ë‹µì„ ì§ì ‘ ì°¾ê¸°
    if reasoning:
        logger.debug("ì¶”ë¡  ëª¨ë¸ ëª¨ë“œ: ë‹µë³€ì—ì„œ IDì™€ ì •ë‹µì„ ì§ì ‘ ì°¾ëŠ” ì¤‘...")
        for _id in expected_ids:
            # ID íŒ¨í„´ìœ¼ë¡œ í•´ë‹¹ IDê°€ í¬í•¨ëœ ë¶€ë¶„ ì°¾ê¸°
            id_pattern = re.escape(_id)
            # ì—¬ëŸ¬ íŒ¨í„´ì„ ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            patterns = [
                rf"{id_pattern}.*?ì •ë‹µì€\s*(?:ë³´ê¸°\s*)?([1-5])",  # "ì •ë‹µì€ 4" ë˜ëŠ” "ì •ë‹µì€ ë³´ê¸° 4"
                rf"{id_pattern}.*?ê°€ì¥\s*ê·¼ì ‘í•œ\s*ê²ƒì€\s*(?:ë³´ê¸°\s*)?([1-5])",  # "ê°€ì¥ ê·¼ì ‘í•œ ê²ƒì€ 4" ë˜ëŠ” "ê°€ì¥ ê·¼ì ‘í•œ ê²ƒì€ ë³´ê¸° 4"
                rf"{id_pattern}.*?ê°€ì¥\s*ê·¼ì ‘í•œ\s*ê²ƒì€\s*([1-5])\s*ë²ˆ",  # "ê°€ì¥ ê·¼ì ‘í•œ ê²ƒì€ 4ë²ˆ"
            ]
            
            found = False
            for pattern in patterns:
                match = re.search(pattern, raw, re.IGNORECASE | re.DOTALL)
                if match:
                    answer = float(match.group(1))
                    out[_id] = answer
                    logger.debug(f"ID '{_id}' -> ë‹µë³€ {answer} (ì¶”ë¡  ëª¨ë¸ì—ì„œ ì¶”ì¶œ)")
                    found = True
                    break
            
            if not found:
                # 'ì •ë‹µì€' ë˜ëŠ” 'ê°€ì¥ ê·¼ì ‘í•œ ê²ƒì€' í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ í‘œì‹œ
                out[_id] = 0.0
                logger.debug(f"ID '{_id}' -> 'ì •ë‹µì€' ë˜ëŠ” 'ê°€ì¥ ê·¼ì ‘í•œ ê²ƒì€' í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ 0ìœ¼ë¡œ ì„¤ì •")
        return out

    # ì¼ë°˜ ëª¨ë¸: íƒ­ êµ¬ë¶„ í¬ë§· ì²˜ë¦¬
    lines = raw.splitlines()
    logger.debug(f"íŒŒì‹±í•  ì¤„ ìˆ˜: {len(lines)}")
    
    for i, ln in enumerate(lines):
        ln = ln.strip()
        logger.debug(f"ì¤„ {i+1}: '{ln}'")
        
        if not ln:
            logger.debug(f"ì¤„ {i+1}: ë¹ˆ ì¤„, ìŠ¤í‚µ")
            continue
        
        # ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬: \\t -> \t
        ln = ln.replace("\\t", "\t")
        logger.debug(f"ì¤„ {i+1} (ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ í›„): '{ln}'")
            
        if "\t" not in ln:
            logger.debug(f"ì¤„ {i+1}: íƒ­ì´ ì—†ìŒ, ìŠ¤í‚µ")
            continue
            
        left, right = ln.split("\t", 1)
        _id = left.strip()
        logger.debug(f"ì¤„ {i+1}: ID='{_id}', ë‹µë³€='{right}'")
        
        if _id not in id_set:
            logger.debug(f"ì¤„ {i+1}: ID '{_id}'ê°€ ì˜ˆìƒ ëª©ë¡ì— ì—†ìŒ, ìŠ¤í‚µ")
            continue
            
        # ì²« ë²ˆì§¸ 1~5 ì¶”ì¶œ
        m = re.search(r"[1-5]", right)
        if m:
            answer = float(m.group(0))
            out[_id] = answer
            logger.debug(f"ì¤„ {i+1}: ID '{_id}' -> ë‹µë³€ {answer}")
        else:
            logger.debug(f"ì¤„ {i+1}: ID '{_id}'ì˜ ë‹µë³€ì—ì„œ 1~5 ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    return out

# -----------------------------
# í‰ê°€ íŒŒì´í”„ë¼ì¸
# -----------------------------

def run_eval_pipeline(
    json_list: List[dict],
    models: List[str],
    sample_size: int = 300,
    batch_size: int = 50,
    seed: int = 42,
    mock_mode: bool = False,
    use_server_mode: bool = False,
    reasoning: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ë°˜í™˜:
      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set)
      pred_long   : (id, model_name, answer) ë¡± í¬ë§·
      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ
      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)
    """
    logger.info(f"í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}")
    
    # (1) JSON â†’ df_all
    logger.info("1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    df_all = json_to_df_all(json_list)
    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)
    logger.info(f"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ")

    # (2) ìƒ˜í”Œë§
    logger.info(f"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
    # ìƒ˜í”Œ í¬ê¸°ê°€ ì „ì²´ ë°ì´í„°ë³´ë‹¤ í° ê²½ìš°, ì „ì²´ ë°ì´í„° í¬ê¸°ë¡œ ì¡°ì •
    actual_sample_size = min(sample_size, len(df_all))
    if actual_sample_size < sample_size:
        logger.warning(f"ìš”ì²­í•œ ìƒ˜í”Œ í¬ê¸°({sample_size})ê°€ ì „ì²´ ë°ì´í„°({len(df_all)})ë³´ë‹¤ í¼. {actual_sample_size}ê°œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    df_sample = df_all.sample(n=actual_sample_size, random_state=seed).reset_index(drop=True)
    logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ")

    # (3) ë°°ì¹˜ ë¶„í• 
    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]
    logger.info(f"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ")

    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì 
    logger.info("4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...")
    rows = []
    invalid_responses = []  # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ìš©
    total_calls = len(batches) * len(models)
    
    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ
    with tqdm(total=total_calls, desc="ëª¨ë¸ í˜¸ì¶œ ì§„í–‰", unit="call") as pbar:
        for bidx, bdf in enumerate(batches, 1):
            user_prompt = build_user_prompt(bdf)
            ids = bdf["id"].tolist()
            
            for model in models:
                try:
                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ
                    pbar.set_description(f"ë°°ì¹˜ {bidx}/{len(batches)} - {model}")
                    
                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode, use_server_mode=use_server_mode)
                    if reasoning:
                        logger.info(f"ì¶”ë¡  ëª¨ë¸ ì›ì‹œ ì¶œë ¥ ì €ì¥ ì™„ë£Œ")
                        with open(f"reasoning_model_output_{model}.txt", "w") as f:
                            f.write(raw)
                    else:
                        pass
                    parsed = parse_model_output(raw, ids, reasoning=reasoning)
                    
                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦
                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))
                    logger.info(f"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡")
                    
                    # ë¬´íš¨ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ë° ì €ì¥
                    if valid_predictions < len(ids):
                        logger.warning(f"ë°°ì¹˜ {bidx} - {model}: ë¬´íš¨ ì˜ˆì¸¡ ê°ì§€!")
                        logger.warning(f"ì˜ˆìƒ ID: {ids}")
                        logger.warning(f"ëª¨ë¸ ì›ì‹œ ì¶œë ¥:\n{raw}")
                        logger.warning(f"íŒŒì‹±ëœ ê²°ê³¼: {parsed}")
                        
                        # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)
                        for _id in ids:
                            if np.isnan(parsed[_id]):
                                # ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                question_info = bdf[bdf['id'] == _id].iloc[0] if len(bdf[bdf['id'] == _id]) > 0 else None
                                
                                invalid_response = {
                                    "model_name": model,
                                    "batch_id": bidx,
                                    "question_id": _id,
                                    "question": question_info['question'] if question_info is not None else "ì •ë³´ ì—†ìŒ",
                                    "options": {
                                        "opt1": question_info['opt1'] if question_info is not None else "",
                                        "opt2": question_info['opt2'] if question_info is not None else "",
                                        "opt3": question_info['opt3'] if question_info is not None else "",
                                        "opt4": question_info['opt4'] if question_info is not None else "",
                                        "opt5": question_info['opt5'] if question_info is not None else ""
                                    },
                                    "correct_answer": list(question_info['answer_set']) if question_info is not None else [],
                                    "model_raw_output": raw,
                                    "parsed_result": parsed[_id],
                                    "timestamp": dt.datetime.now().isoformat()
                                }
                                invalid_responses.append(invalid_response)
                    
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": parsed[_id]})
                    
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": np.nan})
                    pbar.update(1)

    logger.info("5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    pred_long = pd.DataFrame(rows)
    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)
    
    # (5) ì™€ì´ë“œ í¬ë§·
    pred_wide = pred_long.pivot(index="id", columns="model_name", values="answer").reset_index()
    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)

    # (6) ì •í™•ë„ ê³„ì‚°
    logger.info("6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...")
    key = df_sample[["id", "answer_set"]].copy()
    
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)

    merged = pred_long.merge(key, on="id", how="left")
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)

    acc_by_model = (
        merged.groupby("model_name", dropna=False)["correct"]
        .mean()
        .reset_index()
        .rename(columns={"correct": "accuracy"})
        .sort_values("accuracy", ascending=False)
    )
    
    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    logger.info("í‰ê°€ ì™„ë£Œ!")
    logger.info("ëª¨ë¸ë³„ ì •í™•ë„:")
    for _, row in acc_by_model.iterrows():
        logger.info(f"  {row['model_name']}: {row['accuracy']:.3f}")
    
    # ìºì‹œ ì •ë³´ ë¡œê¹…
    cache_info = get_cache_info()
    logger.info(f"[CACHE] í‰ê°€ ì™„ë£Œ í›„ ìºì‹œ ìƒíƒœ: {len(cache_info['cached_models'])}ê°œ ëª¨ë¸ ìºì‹œë¨")
    
    # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥
    if 'invalid_responses' in locals() and invalid_responses:
        save_invalid_responses(invalid_responses, "evaluation")
    
    return df_all, pred_long, pred_wide, acc_by_model

def run_eval_pipeline_improved(
    json_list: List[dict],
    models: List[str],
    sample_size: int = 300,
    batch_size: int = 50,
    seed: int = 42,
    mock_mode: bool = False,
    use_server_mode: bool = False,
    reasoning: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    O, X ë¬¸ì œë¥¼ ì§€ì›í•˜ëŠ” ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸
    ë°˜í™˜:
      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set + is_ox_question)
      pred_long   : (id, model_name, answer) ë¡± í¬ë§·
      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ
      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)
    """
    logger.info(f"ê°œì„ ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}")
    
    # (1) JSON â†’ df_all (O, X ë¬¸ì œ ì§€ì›)
    logger.info("1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    df_all = json_to_df_all_improved(json_list, use_ox_support=True)
    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)
    logger.info(f"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ")

    # O, X ë¬¸ì œ ë¶„ì„
    ox_questions, regular_questions = analyze_ox_questions(df_all)

    # (2) ìƒ˜í”Œë§
    logger.info(f"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
    # ìƒ˜í”Œ í¬ê¸°ê°€ ì „ì²´ ë°ì´í„°ë³´ë‹¤ í° ê²½ìš°, ì „ì²´ ë°ì´í„° í¬ê¸°ë¡œ ì¡°ì •
    actual_sample_size = min(sample_size, len(df_all))
    if actual_sample_size < sample_size:
        logger.warning(f"ìš”ì²­í•œ ìƒ˜í”Œ í¬ê¸°({sample_size})ê°€ ì „ì²´ ë°ì´í„°({len(df_all)})ë³´ë‹¤ í¼. {actual_sample_size}ê°œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    df_sample = df_all.sample(n=actual_sample_size, random_state=seed).reset_index(drop=True)
    logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ")

    # ìƒ˜í”Œì—ì„œ O, X ë¬¸ì œ ë¹„ìœ¨ í™•ì¸
    sample_ox = df_sample[df_sample['is_ox_question'] == True]
    sample_regular = df_sample[df_sample['is_ox_question'] == False]
    logger.info(f"ìƒ˜í”Œ ë‚´ O, X ë¬¸ì œ: {len(sample_ox)}ê°œ, ì¼ë°˜ ê°ê´€ì‹: {len(sample_regular)}ê°œ")

    # (3) ë°°ì¹˜ ë¶„í• 
    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]
    logger.info(f"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ")

    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì 
    logger.info("4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...")
    rows = []
    invalid_responses = []  # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ìš©
    total_calls = len(batches) * len(models)

    if reasoning:
        SYSTEM_PROMPT = SYSTEM_PROMPT + "<think> </think>"
    else:
        pass
    
    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ
    with tqdm(total=total_calls, desc="ëª¨ë¸ í˜¸ì¶œ ì§„í–‰", unit="call") as pbar:
        for bidx, bdf in enumerate(batches, 1):
            user_prompt = build_user_prompt(bdf)
            ids = bdf["id"].tolist()
            
            for model in models:
                try:
                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ
                    pbar.set_description(f"ë°°ì¹˜ {bidx}/{len(batches)} - {model}")
                    
                    raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode, use_server_mode=use_server_mode)
                    parsed = parse_model_output(raw, ids, reasoning=reasoning)
                    
                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦
                    valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))
                    logger.info(f"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡")
                    
                    # ë¬´íš¨ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ë° ì €ì¥
                    if valid_predictions < len(ids):
                        logger.warning(f"ë°°ì¹˜ {bidx} - {model}: ë¬´íš¨ ì˜ˆì¸¡ ê°ì§€!")
                        logger.warning(f"ì˜ˆìƒ ID: {ids}")
                        logger.warning(f"ëª¨ë¸ ì›ì‹œ ì¶œë ¥:\n{raw}")
                        logger.warning(f"íŒŒì‹±ëœ ê²°ê³¼: {parsed}")
                        
                        # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)
                        for _id in ids:
                            if np.isnan(parsed[_id]):
                                # ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                question_info = bdf[bdf['id'] == _id].iloc[0] if len(bdf[bdf['id'] == _id]) > 0 else None
                                
                                invalid_response = {
                                    "model_name": model,
                                    "batch_id": bidx,
                                    "question_id": _id,
                                    "question": question_info['question'] if question_info is not None else "ì •ë³´ ì—†ìŒ",
                                    "options": {
                                        "opt1": question_info['opt1'] if question_info is not None else "",
                                        "opt2": question_info['opt2'] if question_info is not None else "",
                                        "opt3": question_info['opt3'] if question_info is not None else "",
                                        "opt4": question_info['opt4'] if question_info is not None else "",
                                        "opt5": question_info['opt5'] if question_info is not None else ""
                                    },
                                    "correct_answer": list(question_info['answer_set']) if question_info is not None else [],
                                    "model_raw_output": raw,
                                    "parsed_result": parsed[_id],
                                    "timestamp": dt.datetime.now().isoformat()
                                }
                                invalid_responses.append(invalid_response)
                    
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": parsed[_id]})
                    
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ì±„ì›€
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": np.nan})
                    pbar.update(1)

    logger.info("5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    pred_long = pd.DataFrame(rows)
    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)
    
    # (5) ì™€ì´ë“œ í¬ë§·
    pred_wide = pred_long.pivot(index="id", columns="model_name", values="answer").reset_index()
    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)

    # (6) ì •í™•ë„ ê³„ì‚°
    logger.info("6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...")
    key = df_sample[["id", "answer_set"]].copy()
    
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)

    merged = pred_long.merge(key, on="id", how="left")
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)

    acc_by_model = (
        merged.groupby("model_name", dropna=False)["correct"]
        .mean()
        .reset_index()
        .rename(columns={"correct": "accuracy"})
        .sort_values("accuracy", ascending=False)
    )
    
    # O, X ë¬¸ì œì™€ ì¼ë°˜ ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„
    sample_with_type = df_sample[["id", "is_ox_question"]].copy()
    merged_with_type = merged.merge(sample_with_type, on="id", how="left")
    
    # O, X ë¬¸ì œ ì •í™•ë„
    ox_accuracy = merged_with_type[merged_with_type['is_ox_question'] == True].groupby("model_name")["correct"].mean()
    regular_accuracy = merged_with_type[merged_with_type['is_ox_question'] == False].groupby("model_name")["correct"].mean()
    
    logger.info("í‰ê°€ ì™„ë£Œ!")
    logger.info("ëª¨ë¸ë³„ ì „ì²´ ì •í™•ë„:")
    for _, row in acc_by_model.iterrows():
        logger.info(f"  {row['model_name']}: {row['accuracy']:.3f}")
    
    if len(ox_accuracy) > 0:
        logger.info("O, X ë¬¸ì œ ì •í™•ë„:")
        for model, acc in ox_accuracy.items():
            logger.info(f"  {model}: {acc:.3f}")
    
    # ìºì‹œ ì •ë³´ ë¡œê¹…
    cache_info = get_cache_info()
    logger.info(f"[CACHE] í‰ê°€ ì™„ë£Œ í›„ ìºì‹œ ìƒíƒœ: {len(cache_info['cached_models'])}ê°œ ëª¨ë¸ ìºì‹œë¨")
    
    # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥
    if 'invalid_responses' in locals() and invalid_responses:
        save_invalid_responses(invalid_responses, "evaluation")
    
    return df_all, pred_long, pred_wide, acc_by_model

# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -----------------------------

def analyze_ox_questions(df: pd.DataFrame):
    """O, X ë¬¸ì œ ë¶„ì„"""
    ox_questions = df[df['is_ox_question'] == True]
    regular_questions = df[df['is_ox_question'] == False]
    
    print(f"ğŸ“Š ë¬¸ì œ ìœ í˜• ë¶„ì„")
    print(f"   - O, X ë¬¸ì œ: {len(ox_questions)}ê°œ")
    print(f"   - ì¼ë°˜ ê°ê´€ì‹: {len(regular_questions)}ê°œ")
    print(f"   - ì „ì²´: {len(df)}ê°œ")
    
    if len(ox_questions) > 0:
        print(f"\nğŸ” O, X ë¬¸ì œ ì •ë‹µ ë¶„í¬:")
        ox_answers = ox_questions['answer_set'].apply(lambda x: list(x) if x else [])
        answer_counts = {}
        for answers in ox_answers:
            for ans in answers:
                answer_counts[ans] = answer_counts.get(ans, 0) + 1
        
        for ans, count in sorted(answer_counts.items()):
            answer_text = "O" if ans == 1 else "X" if ans == 2 else str(ans)
            print(f"   - {answer_text}: {count}ê°œ")
    
    return ox_questions, regular_questions

def print_evaluation_summary(acc_df: pd.DataFrame, pred_long_df: pd.DataFrame):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìƒì„¸ ìš”ì•½")
    print("="*80)
    
    # ê¸°ë³¸ í†µê³„
    total_predictions = len(pred_long_df)
    valid_predictions = len(pred_long_df.dropna(subset=['answer']))
    invalid_predictions = total_predictions - valid_predictions
    
    print(f"ğŸ“ˆ ì „ì²´ ì˜ˆì¸¡ ìˆ˜: {total_predictions:,}")
    print(f"âœ… ìœ íš¨ ì˜ˆì¸¡: {valid_predictions:,} ({valid_predictions/total_predictions*100:.1f}%)")
    print(f"âŒ ë¬´íš¨ ì˜ˆì¸¡: {invalid_predictions:,} ({invalid_predictions/total_predictions*100:.1f}%)")
    
    # ëª¨ë¸ë³„ ì •í™•ë„
    print(f"\nğŸ† ëª¨ë¸ë³„ ì •í™•ë„ ìˆœìœ„:")
    for i, (_, row) in enumerate(acc_df.iterrows(), 1):
        accuracy = row['accuracy']
        if pd.isna(accuracy):
            print(f"  {i}. {row['model_name']}: N/A (ë°ì´í„° ì—†ìŒ)")
        else:
            print(f"  {i}. {row['model_name']}: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    print("="*80)

def save_invalid_responses(invalid_responses: List[Dict], filename_prefix: str = "evaluation"):
    """ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)"""
    if not invalid_responses:
        logger.info("ë¬´íš¨ ì˜ˆì¸¡ì´ ì—†ì–´ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M%S")
    invalid_filename = f"evaluation/result/{filename_prefix}_invalid_responses_{timestamp}.json"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(invalid_filename), exist_ok=True)
    
    try:
        with open(invalid_filename, 'w', encoding='utf-8') as f:
            json.dump(invalid_responses, f, ensure_ascii=False, indent=2)
        logger.info(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥: {invalid_filename}")
        logger.info(f"ì´ {len(invalid_responses)}ê°œì˜ ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½ ì •ë³´ë„ ì¶œë ¥
        model_counts = {}
        for resp in invalid_responses:
            model = resp.get('model_name', 'unknown')
            model_counts[model] = model_counts.get(model, 0) + 1
        
        logger.info("ëª¨ë¸ë³„ ë¬´íš¨ ì˜ˆì¸¡ ìˆ˜:")
        for model, count in model_counts.items():
            logger.info(f"  {model}: {count}ê°œ")
            
    except Exception as e:
        logger.error(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def save_detailed_logs(pred_long_df: pd.DataFrame, filename_prefix: str = "evaluation"):
    """ìƒì„¸í•œ ë¡œê·¸ë¥¼ CSVë¡œ ì €ì¥"""
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
    
    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
    pred_log_filename = f"evaluation/result/log/{filename_prefix}_predictions_{timestamp}.csv"
    os.makedirs(os.path.dirname(pred_log_filename), exist_ok=True)
    pred_long_df.to_csv(pred_log_filename, index=False, encoding='utf-8-sig')
    logger.info(f"ìƒì„¸ ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥: {pred_log_filename}")
    
    # ëª¨ë¸ë³„ í†µê³„
    model_stats = pred_long_df.groupby('model_name').agg({
        'answer': [lambda x: x.count(), lambda x: x.notna().sum(), lambda x: x.isna().sum()]
    }).round(3)
    model_stats.columns = ['ì´_ì˜ˆì¸¡ìˆ˜', 'ìœ íš¨_ì˜ˆì¸¡ìˆ˜', 'ë¬´íš¨_ì˜ˆì¸¡ìˆ˜']
    model_stats['ìœ íš¨ìœ¨'] = (model_stats['ìœ íš¨_ì˜ˆì¸¡ìˆ˜'] / model_stats['ì´_ì˜ˆì¸¡ìˆ˜'] * 100).round(1)
    
    stats_filename = f"evaluation/result/log/{filename_prefix}_model_stats_{timestamp}.csv"
    os.makedirs(os.path.dirname(stats_filename), exist_ok=True)
    model_stats.to_csv(stats_filename, encoding='utf-8-sig')
    logger.info(f"ëª¨ë¸ í†µê³„ ì €ì¥: {stats_filename}")

def check_data_quality(df_all: pd.DataFrame, df_sample: pd.DataFrame):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")
    
    issues = []
    
    # 1. ë¹ˆ ë¬¸ì œ ê²€ì‚¬
    empty_questions = df_all[df_all['question'].str.strip() == '']
    if len(empty_questions) > 0:
        issues.append(f"ë¹ˆ ë¬¸ì œ: {len(empty_questions)}ê°œ")
    
    # 2. ë¹ˆ ì„ ì§€ ê²€ì‚¬
    empty_options = df_all[(df_all['opt1'].str.strip() == '') & 
                          (df_all['opt2'].str.strip() == '') & 
                          (df_all['opt3'].str.strip() == '') & 
                          (df_all['opt4'].str.strip() == '') & 
                          (df_all['opt5'].str.strip() == '')]
    if len(empty_options) > 0:
        issues.append(f"ë¹ˆ ì„ ì§€ ë¬¸ì œ: {len(empty_options)}ê°œ")
    
    # 3. ì •ë‹µ ì—†ëŠ” ë¬¸ì œ ê²€ì‚¬
    no_answer = df_all[df_all['answer_set'].apply(len) == 0]
    if len(no_answer) > 0:
        issues.append(f"ì •ë‹µ ì—†ëŠ” ë¬¸ì œ: {len(no_answer)}ê°œ")
    
    # 4. ì¤‘ë³µ ë¬¸ì œ ê²€ì‚¬
    duplicates = df_all[df_all.duplicated(subset=['question'], keep=False)]
    if len(duplicates) > 0:
        issues.append(f"ì¤‘ë³µ ë¬¸ì œ: {len(duplicates)}ê°œ")
    
    if issues:
        logger.warning("ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œê²¬:")
        for issue in issues:
            logger.warning(f"  - {issue}")
    else:
        logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ âœ…")
    
    return issues

def calculate_domain_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Domainë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ domain ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'domain', 'subdomain']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Domainë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    domain_acc = (
        merged.groupby(["domain", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index="domain", columns="model_name", values="correct")
        .reset_index()
    )
    
    return domain_acc

def calculate_subdomain_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Subdomainë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ subdomain ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'domain', 'subdomain']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚°
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Subdomainë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    subdomain_acc = (
        merged.groupby(["domain", "subdomain", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index=["domain", "subdomain"], columns="model_name", values="correct")
        .reset_index()
    )
    
    return subdomain_acc

def calculate_subject_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Subjectë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ subject ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'subject']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚°
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Subjectë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    subject_acc = (
        merged.groupby(["subject", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index="subject", columns="model_name", values="correct")
        .reset_index()
    )
    
    return subject_acc

def save_results_to_excel(df_all: pd.DataFrame, pred_wide: pd.DataFrame, acc: pd.DataFrame, pred_long: pd.DataFrame = None, filename: str = None, mock_mode: bool = False):
    """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥ (domain, subdomain ë¶„ì„ í¬í•¨)"""
    
    # ê¸°ë³¸ ì €ì¥ ê²½ë¡œ ì„¤ì • (í˜„ì¬ ì‚¬ìš©ì ê¸°ì¤€)
    # current_user = os.path.expanduser("~").split("/")[-1]  # í˜„ì¬ ì‚¬ìš©ìëª… ì¶”ì¶œ
    current_user = os.path.dirname(__file__)
    default_base_path = f"{home_dir}/result/"
    
    if filename is None:
        timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
        if mock_mode:
            filename = f"{default_base_path}evaluation_results_test_{timestamp}.xlsx"
        else:
            filename = f"{default_base_path}evaluation_results_{timestamp}.xlsx"
    elif not filename.startswith(('/', './', 'evaluation/')):
        # íŒŒì¼ëª…ë§Œ ì£¼ì–´ì§„ ê²½ìš° (í™•ì¥ì í¬í•¨ ì—¬ë¶€ í™•ì¸)
        timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
        if filename.endswith('.xlsx'):
            # í™•ì¥ìê°€ ìˆëŠ” ê²½ìš°
            name = filename[:-5]  # .xlsx ì œê±°
            if mock_mode and 'test' not in name:
                filename = f"{default_base_path}{name}_test_{timestamp}.xlsx"
            else:
                filename = f"{default_base_path}{name}_{timestamp}.xlsx"
        else:
            # í™•ì¥ìê°€ ì—†ëŠ” ê²½ìš°
            if mock_mode and 'test' not in filename:
                filename = f"{default_base_path}{filename}_test_{timestamp}.xlsx"
            else:
                filename = f"{default_base_path}{filename}_{timestamp}.xlsx"
    elif filename.startswith('evaluation/'):
        # evaluation/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        if mock_mode and 'test' not in filename:
            name, ext = os.path.splitext(filename)
            filename = f"{default_base_path}{name}_test{ext}"
        else:
            filename = f"{default_base_path}{filename}"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    logger.info(f"ê²°ê³¼ë¥¼ {filename}ì— ì €ì¥ ì¤‘...")
    
    # ë¶„ì„ ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
    domain_acc = None
    
    try:
        with pd.ExcelWriter(filename, engine="openpyxl") as w:
            # ê¸°ë³¸ ì‹œíŠ¸ë“¤
            df_all.to_excel(w, index=False, sheet_name="ì „ì²´ë°ì´í„°")
            pred_wide.to_excel(w, index=False, sheet_name="ëª¨ë¸ë³„ì˜ˆì¸¡")
            acc.to_excel(w, index=False, sheet_name="ì •í™•ë„")
            
            # Subject, Domain, Subdomain ë¶„ì„ ì¶”ê°€ (pred_longì´ ì œê³µëœ ê²½ìš°)
            if pred_long is not None and 'domain' in df_all.columns:
                # Subjectë³„ ì •í™•ë„ ê³„ì‚° (subject ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                if 'subject' in df_all.columns:
                    logger.info("Subjectë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                    subject_acc = calculate_subject_accuracy(pred_long, df_all)
                    subject_acc.to_excel(w, index=False, sheet_name="Subjectë³„ì •í™•ë„")
                    
                    # Subjectë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                    subject_stats = df_all.groupby('subject').size().reset_index(name='question_count')
                    subject_stats.to_excel(w, index=False, sheet_name="Subjectë³„ë¬¸ì œìˆ˜")
                
                logger.info("Domainë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                domain_acc = calculate_domain_accuracy(pred_long, df_all)
                domain_acc.to_excel(w, index=False, sheet_name="Domainë³„ì •í™•ë„")
                
                # Domainë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                domain_stats = df_all.groupby('domain').size().reset_index(name='question_count')
                domain_stats.to_excel(w, index=False, sheet_name="Domainë³„ë¬¸ì œìˆ˜")
                
                logger.info("Subdomainë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                subdomain_acc = calculate_subdomain_accuracy(pred_long, df_all)
                subdomain_acc.to_excel(w, index=False, sheet_name="Subdomainë³„ì •í™•ë„")
                
                # Subdomainë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                subdomain_stats = df_all.groupby(['domain', 'subdomain']).size().reset_index(name='question_count')
                subdomain_stats.to_excel(w, index=False, sheet_name="Subdomainë³„ë¬¸ì œìˆ˜")
            
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        
        # Domain ë¶„ì„ ìš”ì•½ ì¶œë ¥
        if pred_long is not None and 'domain' in df_all.columns and domain_acc is not None:
            print_domain_analysis_summary(df_all, domain_acc)
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def print_domain_analysis_summary(df_all: pd.DataFrame, domain_acc: pd.DataFrame):
    """Domain ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ë¶„ì•¼ë³„ ë¶„ì„ ìš”ì•½")
    print("="*80)
    
    # Domainë³„ ë¬¸ì œ ìˆ˜
    domain_counts = df_all['domain'].value_counts()
    print("ğŸ“ˆ Domainë³„ ë¬¸ì œ ìˆ˜:")
    for domain, count in domain_counts.items():
        print(f"  - {domain}: {count}ê°œ")
    
    # Subjectë³„ ë¬¸ì œ ìˆ˜ (subject ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
    if 'subject' in df_all.columns:
        subject_counts = df_all['subject'].value_counts()
        print("\nğŸ“š Subjectë³„ ë¬¸ì œ ìˆ˜:")
        for subject, count in subject_counts.items():
            if subject:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶œë ¥
                print(f"  - {subject}: {count}ê°œ")
    
    # Domainë³„ ì •í™•ë„ (ëª¨ë¸ë³„)
    print(f"\nğŸ† Domainë³„ ì •í™•ë„ (ìƒìœ„ ëª¨ë¸ ê¸°ì¤€):")
    for domain in domain_counts.index:
        domain_data = domain_acc[domain_acc['domain'] == domain]
        if len(domain_data) > 0:
            # ëª¨ë¸ë³„ ì»¬ëŸ¼ì—ì„œ ìµœê³  ì •í™•ë„ ì°¾ê¸°
            model_columns = [col for col in domain_data.columns if col != 'domain']
            if model_columns:
                best_accuracy = 0
                best_model = ""
                for model in model_columns:
                    acc = domain_data[model].iloc[0]
                    if not pd.isna(acc) and acc > best_accuracy:
                        best_accuracy = acc
                        best_model = model
                if best_model:
                    print(f"  - {domain}: {best_model} ({best_accuracy:.3f})")
    
    print("="*80)

# -----------------------------
# íƒœê·¸ ëŒ€ì¹˜ í•¨ìˆ˜ë“¤
# -----------------------------

def replace_tags_in_text(text: str, additional_tag_data: list) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ {f_0000_0000}ì´ë‚˜ {tb_0000_0000} ê°™ì€ íƒœê·¸ë¥¼ additional_tag_dataì—ì„œ ì°¾ì•„ì„œ ëŒ€ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        text: ëŒ€ì¹˜í•  í…ìŠ¤íŠ¸
        additional_tag_data: íƒœê·¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        íƒœê·¸ê°€ ëŒ€ì¹˜ëœ í…ìŠ¤íŠ¸
    """
    if not text or not additional_tag_data:
        return text
    
    # íƒœê·¸ íŒ¨í„´ ë§¤ì¹­: {f_0000_0000}, {tb_0000_0000}, {img_0000_0000}, {etc_0000_0000}, {note_0000_0000}
    tag_pattern = r'\{(f_\d{4}_\d{4}|tb_\d{4}_\d{4}|note_\d{4}_\d{4})\}'
    
    def replace_tag(match):
        tag_with_braces = match.group(0)  # {f_0000_0000}
        tag_without_braces = match.group(1)  # f_0000_0000
        
        # additional_tag_dataì—ì„œ í•´ë‹¹ íƒœê·¸ ì°¾ê¸°
        for tag_data in additional_tag_data:
            if tag_data.get('tag') == tag_with_braces:
                # data í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                if 'data' in tag_data:
                    data = tag_data.get('data', {})
                    if isinstance(data, dict):
                        # dataì—ì„œ ì ì ˆí•œ í•„ë“œ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: content, text, description, caption)
                        for field in ['content', 'text', 'description', 'caption']:
                            if field in data and data[field]:
                                return str(data[field])
                        
                        # file_pathê°€ ìˆìœ¼ë©´ íŒŒì¼ëª… í‘œì‹œ
                        if 'file_path' in data and data['file_path']:
                            return f"[{os.path.basename(data['file_path'])}]"
                    
                    # dataê°€ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    elif isinstance(data, str) and data:
                        return data
                    
                    # dataê°€ ë¦¬ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                    elif isinstance(data, list) and data:
                        return str(data[0])
                
                # data í•„ë“œê°€ ì—†ëŠ” ê²½ìš°, ì§ì ‘ í•„ë“œì—ì„œ ì°¾ê¸°
                else:
                    # ì§ì ‘ í•„ë“œì—ì„œ ì ì ˆí•œ ë‚´ìš© ì°¾ê¸° (ìš°ì„ ìˆœìœ„: content, text, description, caption)
                    for field in ['content', 'text', 'description', 'caption']:
                        if field in tag_data and tag_data[field]:
                            return str(tag_data[field])
                    
                    # file_pathê°€ ìˆìœ¼ë©´ íŒŒì¼ëª… í‘œì‹œ
                    if 'file_path' in tag_data and tag_data['file_path']:
                        return f"[{os.path.basename(tag_data['file_path'])}]"
        
        # íƒœê·¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì›ë³¸ íƒœê·¸ ìœ ì§€
        return tag_with_braces
    
    return re.sub(tag_pattern, replace_tag, text)

def replace_tags_in_qna_data(qna_data: dict, additional_tag_data: list) -> dict:
    """
    Q&A ë°ì´í„°ì˜ questionê³¼ optionsì—ì„œ íƒœê·¸ë¥¼ ëŒ€ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        qna_data: Q&A ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (ì „ì²´ qna ê°ì²´ ë˜ëŠ” qna_data ë¶€ë¶„)
        additional_tag_data: ì¶”ê°€ íƒœê·¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        íƒœê·¸ê°€ ëŒ€ì¹˜ëœ Q&A ë°ì´í„°
    """
    if not qna_data:
        return qna_data
    
    if not additional_tag_data:
        return qna_data
    
    # qna_dataê°€ ì „ì²´ qna ê°ì²´ì¸ ê²½ìš° qna_data ë¶€ë¶„ì„ ì¶”ì¶œ
    if 'qna_data' in qna_data:
        qna_info = qna_data['qna_data']
    else:
        # ì´ë¯¸ qna_data ë¶€ë¶„ë§Œ ì „ë‹¬ëœ ê²½ìš°
        qna_info = qna_data
    
    if 'description' in qna_info:
        desc = qna_info['description']
        
        # question í•„ë“œ ì²˜ë¦¬
        if 'question' in desc and desc['question']:
            desc['question'] = replace_tags_in_text(desc['question'], additional_tag_data)
        
        # options í•„ë“œ ì²˜ë¦¬ (ë¦¬ìŠ¤íŠ¸)
        if 'options' in desc and desc['options']:
            if isinstance(desc['options'], list):
                desc['options'] = [replace_tags_in_text(option, additional_tag_data) for option in desc['options']]
            else:
                desc['options'] = replace_tags_in_text(desc['options'], additional_tag_data)
        
        # answer í•„ë“œ ì²˜ë¦¬
        if 'answer' in desc and desc['answer']:
            desc['answer'] = replace_tags_in_text(desc['answer'], additional_tag_data)
        
        # explanation í•„ë“œ ì²˜ë¦¬
        if 'explanation' in desc and desc['explanation']:
            desc['explanation'] = replace_tags_in_text(desc['explanation'], additional_tag_data)
    
    return qna_data

# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -----------------------------

def extract_subject_from_filename(filename: str) -> str:
    """íŒŒì¼ëª…ì—ì„œ subject ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        filename: íŒŒì¼ëª… (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1_mock_exam_set1.json")
    
    Returns:
        str: ì¶”ì¶œëœ subject (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1")
    """
    if '_mock_exam' in filename:
        # mock_exam íŒŒì¼ì¸ ê²½ìš° íŒŒì¼ëª…ì—ì„œ subject ì¶”ì¶œ
        subject = filename.split("_")[0]
        return subject
    else:
        # ì¼ë°˜ íŒŒì¼ì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""

# -----------------------------
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# -----------------------------

def load_data_from_directory(data_path: str, apply_tag_replacement: bool = True) -> Tuple[List[dict], bool]:
    """ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Returns:
        Tuple[List[dict], bool]: (ë°ì´í„° ë¦¬ìŠ¤íŠ¸, mock_exam íŒŒì¼ í¬í•¨ ì—¬ë¶€)
    """
    json_files = []
    is_mock_exam = False
    
    for root, _, files in os.walk(data_path):
        for f in files:
            if f.endswith(".json") and ('merged' not in f):
                json_files.append(os.path.join(root, f))
                # mock_exam íŒŒì¼ì¸ì§€ í™•ì¸
                if '_mock_exam' in f:
                    is_mock_exam = True
    
    logger.info(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")
    if is_mock_exam:
        logger.info("Mock exam íŒŒì¼ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ê´€ì‹ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    all_data = []
    for file_path in json_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # íŒŒì¼ëª…ì—ì„œ subject ì¶”ì¶œ (mock_exam íŒŒì¼ì¸ ê²½ìš°)
                filename = os.path.basename(file_path)
                subject = extract_subject_from_filename(filename)
                
                if isinstance(data, list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì— subject ì¶”ê°€
                    for item in data:
                        item['subject'] = subject
                    all_data.extend(data)
                else:
                    # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° subject ì¶”ê°€
                    data['subject'] = subject
                    all_data.append(data)
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file_path} - {str(e)}")
    
    logger.info(f"ë¡œë“œëœ ì´ ë°ì´í„° ìˆ˜: {len(all_data)}")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì ìš©
    if apply_tag_replacement:
        logger.info("íƒœê·¸ ëŒ€ì¹˜ ì ìš© ì¤‘...")
        processed_count = 0
        for item in all_data:
            if 'additional_tags_found' in item and item['additional_tags_found']:
                if 'additional_tag_data' in item:
                    item['qna_data'] = replace_tags_in_qna_data(
                        item['qna_data'], 
                        item['additional_tag_data']
                    )
                    processed_count += 1
        logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì™„ë£Œ: {processed_count}ê°œ í•­ëª© ì²˜ë¦¬")
    
    return all_data, is_mock_exam

def filter_multiple_choice_questions(data: List[dict]) -> List[dict]:
    """ê°ê´€ì‹ ë¬¸ì œë§Œ í•„í„°ë§"""
    multiple_choice = []
    for item in data:
        if item.get('qna_type') == "multiple-choice":
            multiple_choice.append(item)
    
    logger.info(f"ê°ê´€ì‹ ë¬¸ì œ ìˆ˜: {len(multiple_choice)}")
    return multiple_choice

# -----------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -----------------------------

def main():
    parser = argparse.ArgumentParser(description='LLM í‰ê°€ ì‹œìŠ¤í…œ')
    parser.add_argument('--data_path', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--sample_size', type=int, default=1000, help='ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ê°’: 1000)')
    parser.add_argument('--batch_size', type=int, default=10, help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--models', nargs='+', default=['anthropic/claude-sonnet-4.5', 'google/gemini-2.5-flash', 'openai/gpt-5', 'google/gemini-2.5-pro', 'google/gemma-3-27b-it:free'], help='í‰ê°€í•  ëª¨ë¸ ëª©ë¡')
    parser.add_argument('--mock_mode', action='store_true', help='Mock ëª¨ë“œë¡œ ì‹¤í–‰ (ì‹¤ì œ API í˜¸ì¶œ ì—†ìŒ)')
    parser.add_argument('--use_ox_support', action='store_true', help='O, X ë¬¸ì œ ì§€ì› í™œì„±í™”')
    parser.add_argument('--apply_tag_replacement', action='store_true', help='íƒœê·¸ ëŒ€ì¹˜ ì ìš© (ê¸°ë³¸ê°’: True)')
    parser.add_argument('--no_tag_replacement', action='store_true', help='íƒœê·¸ ëŒ€ì¹˜ ë¹„í™œì„±í™”')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    parser.add_argument('--output_filename', type=str, help='ê²°ê³¼ Excel íŒŒì¼ëª… (ê¸°ë³¸ê°’: ìë™ ìƒì„±)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”')
    parser.add_argument('--reasoning', action='store_true', default=False, help='ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€')
    
    # API ëª¨ë“œ ì˜µì…˜ ì¶”ê°€
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--api', action='store_true', help='OpenRouter API ëª¨ë“œë¡œ ì‹¤í–‰ (ê¸°ë³¸ê°’)')
    mode_group.add_argument('--server', action='store_true', help='vLLM ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    
    logger.info("=" * 60)
    logger.info("LLM í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    logger.info(f"ìƒ˜í”Œ í¬ê¸°: {args.sample_size}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"ëª¨ë¸: {args.models}")
    logger.info(f"Mock ëª¨ë“œ: {args.mock_mode}")
    logger.info(f"O, X ë¬¸ì œ ì§€ì›: {args.use_ox_support}")
    logger.info(f"ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€: {args.reasoning}")
    logger.info(f"ì¶œë ¥ íŒŒì¼ëª…: {args.output_filename or 'ìë™ ìƒì„±'}")
    
    # API ëª¨ë“œ í™•ì¸
    use_server_mode = args.server
    if use_server_mode:
        logger.info("ëª¨ë“œ: vLLM ì„œë²„ ëª¨ë“œ")
    else:
        logger.info("ëª¨ë“œ: OpenRouter API ëª¨ë“œ (ê¸°ë³¸ê°’)")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì˜µì…˜ ì²˜ë¦¬
    apply_tag_replacement = not args.no_tag_replacement
    logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì ìš©: {apply_tag_replacement}")
    
    try:
        # ë°ì´í„° ë¡œë”©
        logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
        all_data, is_mock_exam = load_data_from_directory(args.data_path, apply_tag_replacement)
        
        # mock_exam íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê°ê´€ì‹ í•„í„°ë§ ì ìš©
        if is_mock_exam:
            multiple_choice_data = all_data
            logger.info("Mock exam íŒŒì¼ì´ë¯€ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            multiple_choice_data = filter_multiple_choice_questions(all_data)
            logger.info(f"ê°ê´€ì‹ ë¬¸ì œ í•„í„°ë§ ì™„ë£Œ: {len(multiple_choice_data)}ê°œ")
        
        if len(multiple_choice_data) == 0:
            logger.error("ì²˜ë¦¬í•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìƒ˜í”Œë§
        if len(multiple_choice_data) > args.sample_size:
            random.seed(args.seed)
            sample_data = random.sample(multiple_choice_data, args.sample_size)
        else:
            sample_data = multiple_choice_data
            logger.info(f"ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(sample_data)}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        if args.use_ox_support:
            df_temp = json_to_df_all_improved(sample_data, use_ox_support=True)
        else:
            df_temp = json_to_df_all(sample_data)
        
        quality_issues = check_data_quality(df_temp, df_temp.sample(n=min(50, len(df_temp)), random_state=args.seed))
        
        # O, X ë¬¸ì œ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support:
            ox_questions, regular_questions = analyze_ox_questions(df_temp)
        
        # í‰ê°€ ì‹¤í–‰
        logger.info("í‰ê°€ ì‹¤í–‰ ì¤‘...")
        if args.use_ox_support:
            df_all, pred_long, pred_wide, acc = run_eval_pipeline_improved(
                sample_data, args.models, args.sample_size, args.batch_size, args.seed, args.mock_mode, use_server_mode, args.reasoning
            )
        else:
            df_all, pred_long, pred_wide, acc = run_eval_pipeline(
                sample_data, args.models, args.sample_size, args.batch_size, args.seed, args.mock_mode, use_server_mode, args.reasoning
            )
        
        # ê²°ê³¼ ì¶œë ¥
        print_evaluation_summary(acc, pred_long)
        
        # O, X ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support and 'is_ox_question' in df_all.columns:
            sample_with_type = df_all[df_all['id'].isin(pred_long['id'])][['id', 'is_ox_question']].copy()
            merged_with_type = pred_long.merge(sample_with_type, on='id', how='left')
            
            if len(merged_with_type[merged_with_type['is_ox_question'] == True]) > 0:
                print("\n" + "="*60)
                print("ğŸ“Š O, X ë¬¸ì œ vs ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„ ë¹„êµ")
                print("="*60)
                
                # O, X ë¬¸ì œ ì •í™•ë„
                ox_correct = merged_with_type[merged_with_type['is_ox_question'] == True]['answer'].notna().sum()
                ox_total = len(merged_with_type[merged_with_type['is_ox_question'] == True])
                ox_acc = ox_correct / ox_total if ox_total > 0 else 0
                
                # ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„
                regular_correct = merged_with_type[merged_with_type['is_ox_question'] == False]['answer'].notna().sum()
                regular_total = len(merged_with_type[merged_with_type['is_ox_question'] == False])
                regular_acc = regular_correct / regular_total if regular_total > 0 else 0
                
                print(f"O, X ë¬¸ì œ: {ox_correct}/{ox_total} ({ox_acc:.1%})")
                print(f"ì¼ë°˜ ê°ê´€ì‹: {regular_correct}/{regular_total} ({regular_acc:.1%})")
        
        # ìƒì„¸ ë¡œê·¸ ì €ì¥
        # save_detailed_logs(pred_long, "evaluation")
        
        # Excel íŒŒì¼ ì €ì¥
        save_results_to_excel(df_all, pred_wide, acc, pred_long, args.output_filename, args.mock_mode)
        
        logger.info("=" * 60)
        logger.info("í‰ê°€ ì™„ë£Œ")
        logger.info("=" * 60)
        
        # í‰ê°€ ì™„ë£Œ í›„ ìºì‹œ ì •ë¦¬ (ì„ íƒì )
        if not use_server_mode:  # API ëª¨ë“œì—ì„œëŠ” ìºì‹œ ì •ë¦¬í•˜ì§€ ì•ŠìŒ (ì¬ì‚¬ìš© ê°€ëŠ¥)
            logger.info("[CACHE] API ëª¨ë“œì´ë¯€ë¡œ ìºì‹œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
        else:
            logger.info("[CACHE] vLLM ì„œë²„ ëª¨ë“œì´ë¯€ë¡œ ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
            clear_model_cache()
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìºì‹œ ì •ë¦¬
        clear_model_cache()
        raise

if __name__ == "__main__":
    main()
