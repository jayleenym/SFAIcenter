#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM í‰ê°€ ì‹œìŠ¤í…œ - í†µí•© ë²„ì „
O, X ë¬¸ì œë¥¼ í¬í•¨í•œ ê°ê´€ì‹ ë¬¸ì œ í‰ê°€ ì‹œìŠ¤í…œ

ì‚¬ìš©ë²•:
    # OpenRouter API ëª¨ë“œ (ê¸°ë³¸ê°’)
    python multiple_eval_by_model.py --data_path /path/to/data --sample_size 1000 --api
    
    # vLLM ì„œë²„ ëª¨ë“œ
    python multiple_eval_by_model.py --data_path /path/to/data --sample_size 1000 --server
"""

import os
import pandas as pd
import numpy as np
import re
import time
import logging
import random
import json
import datetime as dt
from typing import List, Dict, Tuple, Iterable, Set, Any
from dataclasses import dataclass
from tqdm import tqdm
import argparse

# -----------------------------
# ë¡œê¹… ì„¤ì •
# -----------------------------
# pipeline/configì—ì„œ PROJECT_ROOT_PATH, ONEDRIVE_PATH, SFAICENTER_PATH import ì‹œë„
try:
    import sys
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root_dir = os.path.dirname(current_dir)  # tools
    sys.path.insert(0, project_root_dir)
    from pipeline.config import PROJECT_ROOT_PATH, ONEDRIVE_PATH, SFAICENTER_PATH
    project_root = PROJECT_ROOT_PATH
    onedrive_path = ONEDRIVE_PATH
    sfaicenter_path = SFAICENTER_PATH
except ImportError:
    # fallback: pipelineì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    import platform
    system = platform.system()
    home_dir = os.path.expanduser("~")
    if system == "Windows":
        onedrive_path = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar")
    else:
        onedrive_path = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar")
    sfaicenter_path = project_root  # fallback

# ì¤‘ì•™í™”ëœ ë¡œê¹… ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
from core.logger import setup_logger
logger = setup_logger(
    name=__name__,
    log_file='multiple_eval_by_model.log',
    use_console=True,
    use_file=True
)

# -----------------------------
# ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ê·œí™”
# -----------------------------
CIRCLED_MAP = {"â‘ ":"1","â‘¡":"2","â‘¢":"3","â‘£":"4","â‘¤":"5"}

# normalize_option_textëŠ” core.utils.TextProcessor.normalize_option_textë¥¼ ì‚¬ìš©
# ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì œê±°í•˜ê³  core.utilsì—ì„œ import
from core.utils import TextProcessor
normalize_option_text = TextProcessor.normalize_option_text

# -----------------------------
# O, X ë¬¸ì œ ì²˜ë¦¬ ê°œì„ 
# -----------------------------

def is_ox_question(question: str, options: list) -> bool:
    """O, X ë¬¸ì œì¸ì§€ íŒë‹¨"""
    if not options or len(options) == 0:
        return False
    # optionsê°€ ë¹„ì–´ìˆê±°ë‚˜ 2ê°œ ì´í•˜ì´ê³ , O/X í˜•íƒœì¸ì§€ í™•ì¸
    if len(options) <= 2:
        option_text = " ".join(options).upper()
        return "O" in option_text or "X" in option_text
    return False

def parse_answer_set(ans, question: str = "", options: list = None) -> Set[int]:
    """ì •ë‹µ íŒŒì‹± í•¨ìˆ˜ - O, X ë¬¸ì œë„ ì²˜ë¦¬, ë¦¬ìŠ¤íŠ¸ ë‹µì•ˆë„ ì²˜ë¦¬ (ë³€í˜• ì‹œí—˜ì§€ìš©)"""
    if not ans:
        return set()
    
    # ë¦¬ìŠ¤íŠ¸ ë‹µì•ˆ ì²˜ë¦¬ (ë³€í˜• ì‹œí—˜ì§€ì˜ ê²½ìš°: ["â‘ ", "â‘¢"] í˜•ì‹)
    if isinstance(ans, list):
        result_set = set()
        for item in ans:
            if not item:
                continue
            s = str(item).strip()
            # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
            for k, v in CIRCLED_MAP.items():
                s = s.replace(k, v)
            # 1~5 ìˆ«ì ì¶”ì¶œ
            nums = re.findall(r"[1-5]", s)
            result_set.update(int(n) for n in nums)
        return result_set
    
    # ë¬¸ìì—´ ë‹µì•ˆ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
    s = str(ans).strip()
    
    # O, X ë¬¸ì œ ì²˜ë¦¬
    if s.upper() in ['O', 'X']:
        # O, X ë¬¸ì œëŠ” 1ë²ˆ(O), 2ë²ˆ(X)ìœ¼ë¡œ ë³€í™˜
        return {1} if s.upper() == 'O' else {2}
    
    # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
    for k, v in CIRCLED_MAP.items():
        s = s.replace(k, v)
    # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ ëª¨ë‘ í—ˆìš©í•˜ì—¬ 1~5 ì¶”ì¶œ
    nums = re.findall(r"[1-5]", s)
    return set(int(n) for n in nums)

# -----------------------------
# JSON â†’ df_all ë³€í™˜
# -----------------------------

def json_to_df_all(json_list: List[dict], use_ox_support: bool = False, transformed: bool = False) -> pd.DataFrame:
    """
    JSON â†’ df_all ë³€í™˜ í•¨ìˆ˜
    ì»¬ëŸ¼: subject, domain, subdomain, book_id, tag, id, question, opt1..opt5, answer_set [, is_ox_question]
    
    Args:
        json_list: JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        use_ox_support: O, X ë¬¸ì œ ì§€ì› ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        transformed: ë³€í˜• ì‹œí—˜ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False, Trueë©´ answerê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ)
    
    Note:
        ì¤‘ë³µ ì œê±°ëŠ” í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. IDë§Œ ê³ ìœ í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    """
    rows = []
    for item in json_list:
        book_id = str(item.get("file_id", ""))
        
        # ìµœìƒìœ„ êµ¬ì¡° (exam íŒŒì¼ êµ¬ì¡°)
        tag = item.get("tag", "")
        q = (item.get("question") or "").strip()
        opts = item.get("options", [])
        answer = item.get("answer", "")
        domain = item.get("domain", "")
        subdomain = item.get("subdomain", "")
        
        # ë³€í˜• ì‹œí—˜ì§€ì˜ ê²½ìš° answerê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ (parse_answer_setì—ì„œ ì²˜ë¦¬)
        ans_set = parse_answer_set(answer, q, opts)
        
        # O, X ë¬¸ì œì¸ì§€ íŒë‹¨ (ox ëª¨ë“œê°€ ì¼œì§„ ê²½ìš°ì—ë§Œ)
        is_ox = False
        if use_ox_support:
            is_ox = is_ox_question(q, opts)
            
            if is_ox:
                # O, X ë¬¸ì œëŠ” 2ê°œ ì„ ì§€ë¡œ ê³ ì •
                opts = ["O", "X"] + [""] * 3
            else:
                # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
                opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        else:
            # 5ì§€ì„ ë‹¤ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆì¹¸ ë³´ì •
            opts = list(opts)[:5] + [""] * max(0, 5 - len(opts))
        
        opts = [normalize_option_text(x) for x in opts]
        
        # subject ì •ë³´ ì¶”ì¶œ
        subject = item.get("subject", "")

        # ê¸°ë³¸ ì»¬ëŸ¼ êµ¬ì„±
        row_data = {
            "subject": subject,
            "domain": domain,
            "subdomain": subdomain,
            "book_id": book_id,
            "tag": tag,
            "id": f"{book_id}_{tag}",
            "question": q,
            "opt1": opts[0], "opt2": opts[1], "opt3": opts[2], "opt4": opts[3], "opt5": opts[4],
            "answer_set": ans_set
        }
        
        # ox ëª¨ë“œê°€ ì¼œì§„ ê²½ìš°ì—ë§Œ is_ox_question ì¶”ê°€
        if use_ox_support:
            row_data["is_ox_question"] = is_ox
        
        rows.append(row_data)
    df = pd.DataFrame(rows)
    
    # ë°ì´í„° ìˆ˜ ë¡œê¹…
    original_count = len(df)
    logger.info(f"JSON ë³€í™˜ ì™„ë£Œ: {original_count}ê°œ ë¬¸ì œ")
    
    # ì¤‘ë³µ ID í™•ì¸ (ì •ë³´ë§Œ ì¶œë ¥)
    duplicate_ids = df[df.duplicated(subset=["id"], keep=False)]
    unique_duplicate_ids = set()  # ì´ˆê¸°í™”
    
    if len(duplicate_ids) > 0:
        duplicate_count = len(duplicate_ids)
        unique_duplicate_ids = set(duplicate_ids["id"].unique())
        logger.info(f"ì¤‘ë³µëœ ID ë°œê²¬: {duplicate_count}ê°œ í–‰, {len(unique_duplicate_ids)}ê°œ ê³ ìœ  ID")
        
        # ì¤‘ë³µ ìƒì„¸ ì •ë³´ (ê° IDë³„ ì¤‘ë³µ íšŸìˆ˜)
        id_counts = df["id"].value_counts()
        duplicated_id_counts = id_counts[id_counts > 1]
        if len(duplicated_id_counts) > 0:
            logger.info(f"IDë³„ ì¤‘ë³µ íšŸìˆ˜ (ìµœëŒ€ 10ê°œ):")
            for dup_id, count in duplicated_id_counts.head(10).items():
                logger.info(f"  - {dup_id}: {count}íšŒ")
    
    # ID ì¤‘ë³µì´ ìˆìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ìœ í•˜ê²Œ ë§Œë“¤ê¸° (pivot ì‹œ ë¬¸ì œ ë°©ì§€)
    # ì¤‘ë³µ ì œê±°ëŠ” í•˜ì§€ ì•Šê³ , IDë§Œ ê³ ìœ í•˜ê²Œ ë§Œë“¦
    if len(unique_duplicate_ids) > 0:
        logger.info(f"ID ì¤‘ë³µ ë°œê²¬ ({len(unique_duplicate_ids)}ê°œ ê³ ìœ  ID) - ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ìœ  ID ìƒì„± ì¤‘...")
        df = df.reset_index(drop=True)
        # ì¤‘ë³µëœ IDì— ëŒ€í•´ì„œë§Œ ì¸ë±ìŠ¤ ì¶”ê°€
        df['id'] = df.apply(
            lambda row: f"{row['id']}_{row.name}" if row['id'] in unique_duplicate_ids else row['id'],
            axis=1
        )
        logger.info("ê³ ìœ  ID ìƒì„± ì™„ë£Œ")
    else:
        logger.info(f"ID ì¤‘ë³µ ì—†ìŒ: ëª¨ë“  {original_count}ê°œ ë¬¸ì œì˜ IDê°€ ê³ ìœ í•©ë‹ˆë‹¤")
    
    return df

# -----------------------------
# ë°°ì¹˜ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
# -----------------------------

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸ˆìœµì „ë¬¸ê°€ì´ì ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•´, ê° ë¬¸ì œì˜ ì •ë‹µ "ë²ˆí˜¸ë§Œ" í•˜ë‚˜ ì„ íƒí•©ë‹ˆë‹¤.

ê·œì¹™
- ê° ë¬¸ì œëŠ” ê³ ìœ  IDì™€ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë‹¹ "ID<TAB>ë²ˆí˜¸" í˜•ì‹ìœ¼ë¡œë§Œ í•©ë‹ˆë‹¤. (ì˜ˆ: SS0000_q_0377_0001<TAB>3)
- ë‹¤ë¥¸ ê¸€ì, ë§ˆí¬ë‹¤ìš´, ì´ìœ , ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì œëŠ” ë³´ê¸°(1~5) ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¦…ë‹ˆë‹¤.
- ì¶œë ¥ ì¤„ ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
"""

SYSTEM_PROMPT_TRANSFORMED = """ë‹¹ì‹ ì€ ê¸ˆìœµì „ë¬¸ê°€ì´ì ê°ê´€ì‹ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•´, ê° ë¬¸ì œëŠ” "ëª¨ë‘ ê³ ë¥´ì‹œì˜¤" ìœ í˜•ì…ë‹ˆë‹¤. ì •ë‹µì´ ë˜ëŠ” ëª¨ë“  ë²ˆí˜¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

ê·œì¹™
- ê° ë¬¸ì œëŠ” ê³ ìœ  IDì™€ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë‹¹ "ID<TAB>ë²ˆí˜¸1,ë²ˆí˜¸2,..." í˜•ì‹ìœ¼ë¡œë§Œ í•©ë‹ˆë‹¤. (ì˜ˆ: SS0000_q_0377_0001<TAB>1,3 ë˜ëŠ” SS0000_q_0377_0001<TAB>1 3)
- ì—¬ëŸ¬ ì •ë‹µì´ ìˆëŠ” ê²½ìš° ì‰¼í‘œ(,) ë˜ëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ëª¨ë‘ ì„ íƒí•©ë‹ˆë‹¤. (ì˜ˆ: 1,3 ë˜ëŠ” 1 3)
- ì •ë‹µì´ í•˜ë‚˜ì¸ ê²½ìš°ì—ë„ ë™ì¼í•œ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: 3 ë˜ëŠ” 3)
- ë‹¤ë¥¸ ê¸€ì, ë§ˆí¬ë‹¤ìš´, ì´ìœ , ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì œëŠ” ë³´ê¸°(1~5) ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì„ íƒí•©ë‹ˆë‹¤.
- ì¶œë ¥ ì¤„ ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
"""

def build_user_prompt(batch_df: pd.DataFrame, transformed: bool = False) -> str:
    lines = []
    if transformed:
        lines.append("ë‹¤ìŒì€ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì œëŠ” 'ëª¨ë‘ ê³ ë¥´ì‹œì˜¤' ìœ í˜•ì…ë‹ˆë‹¤. ì •ë‹µì´ ë˜ëŠ” ëª¨ë“  ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n")
    else:
        lines.append("ë‹¤ìŒì€ ê¸ˆìœµ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´ ì •ë‹µ ë²ˆí˜¸ë§Œ ê³ ë¥´ì„¸ìš”.\n")
    lines.append("ë¬¸ì œë“¤")
    for _, r in batch_df.iterrows():
        lines.append(f"ID: {r['id']}")
        lines.append(f"Q: {r['question']}")
        lines.append(f"1) {r['opt1']}")
        lines.append(f"2) {r['opt2']}")
        lines.append(f"3) {r['opt3']}")
        lines.append(f"4) {r['opt4']}")
        lines.append(f"5) {r['opt5']}\n")
    lines.append("ì¶œë ¥ í˜•ì‹(ì¤‘ìš”)")
    for _, r in batch_df.iterrows():
        if transformed:
            lines.append(f"{r['id']}\\t{{ë²ˆí˜¸1,ë²ˆí˜¸2,...}}  (ì˜ˆ: 1,3 ë˜ëŠ” 1 3)")
        else:
            lines.append(f"{r['id']}\\t{{ë²ˆí˜¸}}")
    return "\n".join(lines)

# -----------------------------
# LLM í˜¸ì¶œ ì¶”ìƒí™”
# -----------------------------

# ëª¨ë¸ ìºì‹œë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_model_cache = {}
_config_cache = None
_query_models_module = None

def _find_config_file():
    """Config íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤ (LLMQuery._find_config_fileê³¼ ë™ì¼í•œ ë¡œì§)"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ llm_config.ini ì°¾ê¸°
    default_path = os.path.join(project_root, 'llm_config.ini')
    if os.path.exists(default_path):
        return default_path
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° find ëª…ë ¹ì–´ë¡œ ê²€ìƒ‰ (fallback)
    config_path = os.popen(f"find {project_root} -type f -name 'llm_config.ini' 2>/dev/null").read().strip()
    if config_path and os.path.exists(config_path):
        return config_path
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
    return default_path

def _load_config():
    """Config íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œ"""
    global _config_cache
    if _config_cache is None:
        import configparser
        # LLMQueryì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ config íŒŒì¼ ì°¾ê¸°
        config_path = _find_config_file()
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        
        _config_cache = configparser.ConfigParser()
        _config_cache.read(config_path, encoding='utf-8')
        logger.info(f"[CACHE] Config íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
    
    return _config_cache

def _load_query_models(api_key: str = None):
    """LLMQuery ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ìºì‹œ
    
    Args:
        api_key: API í‚¤ (Noneì´ë©´ ê¸°ë³¸ key ì‚¬ìš©, key_evaluate ë“± ë‹¤ë¥¸ í‚¤ ì‚¬ìš© ê°€ëŠ¥)
    """
    global _query_models_module
    # api_keyê°€ ì œê³µë˜ë©´ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if _query_models_module is None or api_key is not None:
        import sys
        import os
        # tools/core/llm_query.pyì—ì„œ LLMQuery í´ë˜ìŠ¤ import
        current_dir = os.path.dirname(os.path.abspath(__file__))
        tools_dir = os.path.dirname(current_dir)  # evaluation -> tools
        sys.path.insert(0, tools_dir)
        
        try:
            from core.llm_query import LLMQuery
            _query_models_module = LLMQuery(api_key=api_key)
            if api_key:
                logger.info(f"[CACHE] LLMQuery ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (ì»¤ìŠ¤í…€ API í‚¤ ì‚¬ìš©): {tools_dir}")
            else:
                logger.info(f"[CACHE] LLMQuery ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ: {tools_dir}")
        except Exception as e:
            logger.error(f"[CACHE] LLMQuery ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    return _query_models_module

def _load_model_cached(model_name: str):
    """ëª¨ë¸ì„ ìºì‹œì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ"""
    global _model_cache
    
    if model_name not in _model_cache:
        logger.info(f"[CACHE] ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        llm_query = _load_query_models()
        
        # LLMQuery.load_vllm_modelì€ model_pathë¥¼ ë°›ìœ¼ë¯€ë¡œ, model_nameì„ model_pathë¡œ ì‚¬ìš©
        # vLLM ëª¨ë¸ ê²½ë¡œëŠ” model_nameê³¼ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
        llm_query.load_vllm_model(model_name)
        
        # LLMQuery ì¸ìŠ¤í„´ìŠ¤ì—ì„œ llm, tokenizer, sampling_params ê°€ì ¸ì˜¤ê¸°
        llm = llm_query.llm
        tokenizer = llm_query.tokenizer
        sampling_params = llm_query.sampling_params
        
        _model_cache[model_name] = (llm, tokenizer, sampling_params)
        logger.info(f"[CACHE] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    else:
        logger.debug(f"[CACHE] ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    
    return _model_cache[model_name]

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œë¥¼ ì •ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ"""
    global _model_cache
    if _model_cache:
        logger.info(f"[CACHE] {len(_model_cache)}ê°œ ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì¤‘...")
        _model_cache.clear()
        logger.info("[CACHE] ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

def get_cache_info():
    """í˜„ì¬ ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    global _model_cache, _config_cache, _query_models_module
    return {
        "cached_models": list(_model_cache.keys()),
        "config_loaded": _config_cache is not None,
        "query_models_loaded": _query_models_module is not None
    }

def call_llm(model_name: str, system_prompt: str, user_prompt: str, use_server_mode: bool=False, max_retries: int=3, api_key: str = None) -> Tuple[str, float]:
    """
    - use_server_mode=Trueë©´ vLLM ì„œë²„ ëª¨ë“œë¡œ í˜¸ì¶œ
    - use_server_mode=Falseë©´ OpenRouter APIë¡œ í˜¸ì¶œ
    - api_key: API í‚¤ (Noneì´ë©´ ê¸°ë³¸ key ì‚¬ìš©, key_evaluate ë“± ë‹¤ë¥¸ í‚¤ ì‚¬ìš© ê°€ëŠ¥)
    - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨
    
    Returns:
        Tuple[str, float]: (ì‘ë‹µ ë¬¸ìì—´, ì†Œìš” ì‹œê°„(ì´ˆ))
    """
    for attempt in range(max_retries):
            try:
                if use_server_mode:
                    # vLLM ì„œë²„ ëª¨ë“œ - ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©
                    logger.debug(f"[VLLM] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})")
                    start_time = time.time()
                    
                    # ìºì‹œëœ ëª¨ë¸ ë¡œë“œ
                    llm, tokenizer, sampling_params = _load_model_cached(model_name)
                    llm_query = _load_query_models(api_key=api_key)
                    
                    # LLMQuery.query_vllmì€ ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œì´ë¯€ë¡œ ì§ì ‘ í˜¸ì¶œ
                    # í•˜ì§€ë§Œ ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•¨
                    ans = llm_query.query_vllm(system_prompt, user_prompt)
                    
                    elapsed_time = time.time() - start_time
                    logger.debug(f"[VLLM] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    
                    return ans, elapsed_time
                else:
                    # OpenRouter API ëª¨ë“œ
                    logger.debug(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries})")
                    start_time = time.time()
                    
                    # LLMQuery ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                    llm_query = _load_query_models(api_key=api_key)
                    
                    # LLMQuery.query_openrouter ì‹œê·¸ë‹ˆì²˜: (system_prompt, user_prompt, model_name)
                    ans = llm_query.query_openrouter(system_prompt, user_prompt, model_name)
                    
                    elapsed_time = time.time() - start_time
                    logger.debug(f"[API] ëª¨ë¸ {model_name} í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    time.sleep(1.5)
                    
                    return ans, elapsed_time
                
            except Exception as e:
                mode_str = "[VLLM]" if use_server_mode else "[API]"
                logger.warning(f"{mode_str} ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    logger.error(f"{mode_str} ëª¨ë¸ {model_name} ìµœì¢… ì‹¤íŒ¨ - ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")
                    raise e
                else:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    logger.info(f"{mode_str} {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)

# -----------------------------
# ëª¨ë¸ ì¶œë ¥ íŒŒì‹± 
# -----------------------------

def parse_model_output(raw: str, expected_ids: List[str], transformed: bool = False) -> Dict[str, Any]:
    """
    ëª¨ë¸ ì›ì‹œ ì¶œë ¥(raw)ì„ íŒŒì‹±.
    - ê¸°ë³¸ ëª¨ë“œ: {id: answer(1~5)} - ë‹¨ì¼ ë‹µì•ˆ
    - ë³€í˜• ëª¨ë“œ: {id: Set[int]} - ì—¬ëŸ¬ ë‹µì•ˆ (ëª¨ë‘ ê³ ë¥´ì‹œì˜¤)
    - 'ID\\të²ˆí˜¸' í¬ë§· ê¸°ì¤€
    - ì˜ëª»ëœ ì¤„/ëˆ„ë½ ì¤„ì€ NaN ì²˜ë¦¬
    """
    id_set = set(expected_ids)
    if transformed:
        # ë³€í˜• ëª¨ë“œ: Set[int] ë°˜í™˜
        out: Dict[str, Set[int]] = {k: set() for k in expected_ids}
    else:
        # ê¸°ë³¸ ëª¨ë“œ: float ë°˜í™˜ (ë‹¨ì¼ ë‹µì•ˆ)
        out: Dict[str, float] = {k: np.nan for k in expected_ids}
    
    if not raw or not raw.strip():
        logger.warning("ëª¨ë¸ ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return out

    # íƒ­ êµ¬ë¶„ í¬ë§· ì²˜ë¦¬
    lines = raw.splitlines()
    logger.debug(f"íŒŒì‹±í•  ì¤„ ìˆ˜: {len(lines)}")
    
    for i, ln in enumerate(lines):
        ln = ln.strip()
        logger.debug(f"ì¤„ {i+1}: '{ln}'")
        
        if not ln:
            logger.debug(f"ì¤„ {i+1}: ë¹ˆ ì¤„, ìŠ¤í‚µ")
            continue
        
        # íƒ­ ì •ê·œí™”: <TAB>, ì‹¤ì œ íƒ­, \\t ëª¨ë‘ \të¡œ ë³€í™˜
        # 1. <TAB> (literal string) -> \t
        ln = ln.replace("<TAB>", "\t")
        # 2. \\t (backslash-t) -> \t
        ln = ln.replace("\\t", "\t")
        # 3. ì‹¤ì œ íƒ­ ë¬¸ìëŠ” ì´ë¯¸ \tì´ë¯€ë¡œ ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”
        logger.debug(f"ì¤„ {i+1} (íƒ­ ì •ê·œí™” í›„): '{ln}'")
            
        if "\t" not in ln:
            logger.debug(f"ì¤„ {i+1}: íƒ­ì´ ì—†ìŒ, ìŠ¤í‚µ")
            continue
            
        left, right = ln.split("\t", 1)
        _id = left.strip()
        logger.debug(f"ì¤„ {i+1}: ID='{_id}', ë‹µë³€='{right}'")
        
        if _id not in id_set:
            logger.debug(f"ì¤„ {i+1}: ID '{_id}'ê°€ ì˜ˆìƒ ëª©ë¡ì— ì—†ìŒ, ìŠ¤í‚µ")
            continue
        
        if transformed:
            # ë³€í˜• ëª¨ë“œ: ì—¬ëŸ¬ ë‹µì•ˆ íŒŒì‹± (ì˜ˆ: "1,3" ë˜ëŠ” "1 3" ë˜ëŠ” "1, 3")
            # ì‰¼í‘œ ë˜ëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ëª¨ë“  ìˆ«ì ì¶”ì¶œ
            # â‘ ~â‘¤ ë¥¼ 1~5ë¡œ ì¹˜í™˜
            answer_str = right
            for k, v in CIRCLED_MAP.items():
                answer_str = answer_str.replace(k, v)
            # ì‰¼í‘œ, ê³µë°±, ìŠ¬ë˜ì‹œ ë“±ìœ¼ë¡œ êµ¬ë¶„ëœ ëª¨ë“  1~5 ìˆ«ì ì¶”ì¶œ
            nums = re.findall(r"[1-5]", answer_str)
            if nums:
                answer_set = set(int(n) for n in nums)
                out[_id] = answer_set
                logger.debug(f"ì¤„ {i+1}: ID '{_id}' -> ë‹µë³€ {answer_set}")
            else:
                logger.debug(f"ì¤„ {i+1}: ID '{_id}'ì˜ ë‹µë³€ì—ì„œ 1~5 ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        else:
            # ê¸°ë³¸ ëª¨ë“œ: ì²« ë²ˆì§¸ 1~5 ì¶”ì¶œ (ì¤‘ê´„í˜¸ í¬í•¨: {4}, {5} ë“±ë„ ì¸ì‹)
            # ì¤‘ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ìˆ«ì ë˜ëŠ” ì¼ë°˜ ìˆ«ì ëª¨ë‘ ì¸ì‹
            m = re.search(r"\{?([1-5])\}?", right)
            if m:
                answer = float(m.group(1))
                out[_id] = answer
                logger.debug(f"ì¤„ {i+1}: ID '{_id}' -> ë‹µë³€ {answer}")
            else:
                logger.debug(f"ì¤„ {i+1}: ID '{_id}'ì˜ ë‹µë³€ì—ì„œ 1~5 ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    return out

# -----------------------------
# í‰ê°€ íŒŒì´í”„ë¼ì¸
# -----------------------------

def run_eval_pipeline(
    json_list: List[dict],
    models: List[str],
    sample_size: int = 300,
    batch_size: int = 50,
    seed: int = 42,
    use_server_mode: bool = False,
    use_ox_support: bool = True,
    api_key: str = None,
    output_base_dir: str = None,
    transformed: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    í‰ê°€ íŒŒì´í”„ë¼ì¸
    
    Args:
        json_list: í‰ê°€í•  JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        models: í‰ê°€í•  ëª¨ë¸ ëª©ë¡
        sample_size: ìƒ˜í”Œ í¬ê¸°
        batch_size: ë°°ì¹˜ í¬ê¸°
        seed: ëœë¤ ì‹œë“œ
        use_server_mode: vLLM ì„œë²„ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        use_ox_support: O, X ë¬¸ì œ ì§€ì› ì—¬ë¶€
        api_key: API í‚¤ (Noneì´ë©´ ê¸°ë³¸ key ì‚¬ìš©, key_evaluate ë“± ë‹¤ë¥¸ í‚¤ ì‚¬ìš© ê°€ëŠ¥)
        output_base_dir: ì¶œë ¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: 6_exam_evaluation)
        transformed: ë³€í˜• ì‹œí—˜ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False, Trueë©´ answerê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ)
    
    ë°˜í™˜:
      df_all      : ì „ì²´ ì›ì¥ (ì •ê·œí™” ì„ ì§€ + answer_set + is_ox_question)
      pred_long   : (id, model_name, answer) ë¡± í¬ë§·
      pred_wide   : id ê¸°ì¤€ ëª¨ë¸ë³„ ì˜ˆì¸¡ ì™€ì´ë“œ
      acc_by_model: ëª¨ë¸ë³„ ì •í™•ë„ (ë³µìˆ˜ì •ë‹µ ì§€ì›: ì˜ˆì¸¡ âˆˆ answer_set ì´ë©´ ì •ë‹µ)
    """
    logger.info(f"í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ìƒ˜í”Œìˆ˜: {sample_size}, ë°°ì¹˜í¬ê¸°: {batch_size}, ëª¨ë¸ìˆ˜: {len(models)}, O/X ì§€ì›: {use_ox_support}, ë³€í˜• ëª¨ë“œ: {transformed}")
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶”ì  ì‹œì‘
    overall_start_time = time.time()
    overall_start_datetime = dt.datetime.now()
    
    # (1) JSON â†’ df_all
    logger.info("1ë‹¨ê³„: JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    # ì¤‘ë³µ ì œê±°ëŠ” í•˜ì§€ ì•ŠìŒ (ëª¨ë“  ë¬¸ì œ ìœ ì§€)
    df_all = json_to_df_all(json_list, use_ox_support=use_ox_support, transformed=transformed)
    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)
    logger.info(f"ì „ì²´ ë°ì´í„°: {len(df_all)}ê°œ ë¬¸ì œ")
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ (ì¤‘ë³µ ë¬¸ì œ í™•ì¸ í¬í•¨)
    quality_report = check_data_quality(json_list, df_all)

    # O, X ë¬¸ì œ ë¶„ì„ (use_ox_supportê°€ Trueì¼ ë•Œë§Œ)
    if use_ox_support:
        ox_questions, regular_questions = analyze_ox_questions(df_all)

    # (2) ìƒ˜í”Œë§
    logger.info(f"2ë‹¨ê³„: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
    # ìƒ˜í”Œ í¬ê¸°ê°€ ì „ì²´ ë°ì´í„°ë³´ë‹¤ í° ê²½ìš°, ì „ì²´ ë°ì´í„° í¬ê¸°ë¡œ ì¡°ì •
    actual_sample_size = min(sample_size, len(df_all))
    if actual_sample_size < sample_size:
        logger.warning(f"ìš”ì²­í•œ ìƒ˜í”Œ í¬ê¸°({sample_size})ê°€ ì „ì²´ ë°ì´í„°({len(df_all)})ë³´ë‹¤ í¼. {actual_sample_size}ê°œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    df_sample = df_all.sample(n=actual_sample_size, random_state=seed).reset_index(drop=True)
    logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ ë¬¸ì œ")

    # ìƒ˜í”Œì—ì„œ O, X ë¬¸ì œ ë¹„ìœ¨ í™•ì¸ (use_ox_supportê°€ Trueì¼ ë•Œë§Œ)
    if use_ox_support:
        sample_ox = df_sample[df_sample['is_ox_question'] == True]
        sample_regular = df_sample[df_sample['is_ox_question'] == False]
        logger.info(f"ìƒ˜í”Œ ë‚´ O, X ë¬¸ì œ: {len(sample_ox)}ê°œ, ì¼ë°˜ ê°ê´€ì‹: {len(sample_regular)}ê°œ")

    # (3) ë°°ì¹˜ ë¶„í• 
    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]
    logger.info(f"3ë‹¨ê³„: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ")

    # (4) ëª¨ë¸ í˜¸ì¶œ/íŒŒì‹± ëˆ„ì 
    logger.info("4ë‹¨ê³„: ëª¨ë¸ í˜¸ì¶œ ë° ì˜ˆì¸¡ ì‹œì‘...")
    rows = []
    invalid_responses = []  # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ìš©
    total_calls = len(batches) * len(models)
    
    # ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ì¶”ì 
    model_response_times = {model: [] for model in models}

    # SYSTEM_PROMPTë¥¼ transformedì— ë”°ë¼ ì„ íƒ
    local_system_prompt = SYSTEM_PROMPT_TRANSFORMED if transformed else SYSTEM_PROMPT
    
    # ì „ì²´ ì§„í–‰ìƒí™© í‘œì‹œ
    with tqdm(total=total_calls, desc="ëª¨ë¸ í˜¸ì¶œ ì§„í–‰", unit="call") as pbar:
        for bidx, bdf in enumerate(batches, 1):
            user_prompt = build_user_prompt(bdf, transformed=transformed)
            ids = bdf["id"].tolist()
            
            for model in models:
                try:
                    # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© í‘œì‹œ (tqdm ì§„í–‰ë°”ì—ë§Œ í‘œì‹œ, ë¡œê·¸ëŠ” ìµœì†Œí™”)
                    pbar.set_description(f"ë°°ì¹˜ {bidx}/{len(batches)} - {model}")
                    
                    raw, response_time = call_llm(model, local_system_prompt, user_prompt, use_server_mode=use_server_mode, api_key=api_key)
                    # ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ê¸°ë¡
                    model_response_times[model].append(response_time)
                    # ëª¨ë“  ëª¨ë¸ ì‘ë‹µì„ backlogë¡œ ì €ì¥
                    if output_base_dir:
                        # output_base_dirì´ ì œê³µëœ ê²½ìš° ì‚¬ìš©
                        output_dir = os.path.join(output_base_dir, 'model_output')
                    else:
                        # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                        try:
                            from pipeline.config import ONEDRIVE_PATH
                            base_path = ONEDRIVE_PATH
                        except ImportError:
                            import platform
                            system = platform.system()
                            home_dir = os.path.expanduser("~")
                            if system == "Windows":
                                base_path = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar")
                            else:
                                base_path = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar")
                        output_dir = os.path.join(base_path, 'evaluation', 'eval_data', '6_exam_evaluation', 'model_output')
                    os.makedirs(output_dir, exist_ok=True)
                    output_file = os.path.join(output_dir, f"model_output_{model.replace('/', '_')}.txt")
                    with open(output_file, "a", encoding="utf-8") as f:
                        f.write(f"\n{'='*80}\n")
                        f.write(f"ë°°ì¹˜: {bidx}/{len(batches)}, ëª¨ë¸: {model}, ì‹œê°„: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ID ëª©ë¡: {ids}\n")
                        f.write(f"{'='*80}\n")
                        f.write(raw)
                        f.write(f"\n{'='*80}\n\n")
                    parsed = parse_model_output(raw, ids, transformed=transformed)
                    
                    # íŒŒì‹± ê²°ê³¼ ê²€ì¦
                    if transformed:
                        # ë³€í˜• ëª¨ë“œ: Set[int] ë°˜í™˜, ë¹ˆ ì§‘í•©ì´ë©´ ë¬´íš¨
                        valid_predictions = sum(1 for v in parsed.values() if isinstance(v, set) and len(v) > 0)
                    else:
                        # ê¸°ë³¸ ëª¨ë“œ: float ë°˜í™˜, NaNì´ ì•„ë‹ˆë©´ ìœ íš¨
                        valid_predictions = sum(1 for v in parsed.values() if not np.isnan(v))
                    
                    # ë¬´íš¨ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¡œê·¸ ì¶œë ¥
                    if valid_predictions < len(ids):
                        logger.warning(f"ë°°ì¹˜ {bidx} - {model}: {valid_predictions}/{len(ids)}ê°œ ìœ íš¨ ì˜ˆì¸¡ (ë¬´íš¨ ì˜ˆì¸¡ ê°ì§€)")
                        logger.warning(f"ì˜ˆìƒ ID: {ids}")
                        logger.warning(f"ëª¨ë¸ ì›ì‹œ ì¶œë ¥:\n{raw}")
                        logger.warning(f"íŒŒì‹±ëœ ê²°ê³¼: {parsed}")
                        
                        # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨)
                        for _id in ids:
                            is_invalid = False
                            if transformed:
                                # ë³€í˜• ëª¨ë“œ: Set[int]ê°€ ë¹ˆ ì§‘í•©ì´ë©´ ë¬´íš¨
                                is_invalid = not (isinstance(parsed[_id], set) and len(parsed[_id]) > 0)
                            else:
                                # ê¸°ë³¸ ëª¨ë“œ: NaNì´ë©´ ë¬´íš¨
                                is_invalid = np.isnan(parsed[_id])
                            
                            if is_invalid:
                                # ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                question_info = bdf[bdf['id'] == _id].iloc[0] if len(bdf[bdf['id'] == _id]) > 0 else None
                                
                                invalid_response = {
                                    "model_name": model,
                                    "batch_id": bidx,
                                    "question_id": _id,
                                    "question": question_info['question'] if question_info is not None else "ì •ë³´ ì—†ìŒ",
                                    "options": {
                                        "opt1": question_info['opt1'] if question_info is not None else "",
                                        "opt2": question_info['opt2'] if question_info is not None else "",
                                        "opt3": question_info['opt3'] if question_info is not None else "",
                                        "opt4": question_info['opt4'] if question_info is not None else "",
                                        "opt5": question_info['opt5'] if question_info is not None else ""
                                    },
                                    "correct_answer": list(question_info['answer_set']) if question_info is not None else [],
                                    "model_raw_output": raw,
                                    "parsed_result": list(parsed[_id]) if isinstance(parsed[_id], set) else parsed[_id],
                                    "timestamp": dt.datetime.now().isoformat()
                                }
                                invalid_responses.append(invalid_response)
                    
                    for _id in ids:
                        rows.append({"id": _id, "model_name": model, "answer": parsed[_id]})
                    
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {bidx} - {model} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
                    for _id in ids:
                        if transformed:
                            rows.append({"id": _id, "model_name": model, "answer": set()})
                        else:
                            rows.append({"id": _id, "model_name": model, "answer": np.nan})
                    pbar.update(1)

    logger.info("5ë‹¨ê³„: ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    pred_long = pd.DataFrame(rows)
    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)
    
    # (5) ì™€ì´ë“œ í¬ë§·
    pred_wide = pred_long.pivot(index="id", columns="model_name", values="answer").reset_index()
    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)

    # (6) ì •í™•ë„ ê³„ì‚°
    logger.info("6ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì¤‘...")
    key = df_sample[["id", "answer_set"]].copy()
    
    def _is_correct(pred, s: Set[int], is_transformed: bool = False) -> float:
        """ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
        - ê¸°ë³¸ ëª¨ë“œ: predê°€ floatì´ê³  sì— í¬í•¨ë˜ë©´ ì •ë‹µ
        - ë³€í˜• ëª¨ë“œ: predê°€ Set[int]ì´ê³  ëª¨ë“  ë‹µì•ˆì´ sì— í¬í•¨ë˜ê³ , predì™€ sê°€ ë™ì¼í•˜ë©´ ì •ë‹µ
        """
        if not s:
            return np.nan
        
        if is_transformed:
            # ë³€í˜• ëª¨ë“œ: predëŠ” Set[int]
            if not isinstance(pred, set) or len(pred) == 0:
                return np.nan
            # ëª¨ë¸ì´ ì„ íƒí•œ ëª¨ë“  ë‹µì•ˆì´ ì •ë‹µ ì§‘í•©ì— í¬í•¨ë˜ê³ , ê°œìˆ˜ë„ ì¼ì¹˜í•´ì•¼ ì •ë‹µ
            return float(pred == s)
        else:
            # ê¸°ë³¸ ëª¨ë“œ: predëŠ” float
            if np.isnan(pred):
                return np.nan
            return float(int(pred) in s)

    merged = pred_long.merge(key, on="id", how="left")
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"], is_transformed=transformed), axis=1)

    acc_by_model = (
        merged.groupby("model_name", dropna=False)["correct"]
        .mean()
        .reset_index()
        .rename(columns={"correct": "accuracy"})
        .sort_values("accuracy", ascending=False)
    )
    
    # O, X ë¬¸ì œì™€ ì¼ë°˜ ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„
    sample_with_type = df_sample[["id", "is_ox_question"]].copy()
    merged_with_type = merged.merge(sample_with_type, on="id", how="left")
    
    # O, X ë¬¸ì œ ì •í™•ë„
    ox_accuracy = merged_with_type[merged_with_type['is_ox_question'] == True].groupby("model_name")["correct"].mean()
    regular_accuracy = merged_with_type[merged_with_type['is_ox_question'] == False].groupby("model_name")["correct"].mean()
    
    logger.info("í‰ê°€ ì™„ë£Œ!")
    logger.info("ëª¨ë¸ë³„ ì „ì²´ ì •í™•ë„:")
    for _, row in acc_by_model.iterrows():
        logger.info(f"  {row['model_name']}: {row['accuracy']:.3f}")
    
    if len(ox_accuracy) > 0:
        logger.info("O, X ë¬¸ì œ ì •í™•ë„:")
        for model, acc in ox_accuracy.items():
            logger.info(f"  {model}: {acc:.3f}")
    
    # ìºì‹œ ì •ë³´ ë¡œê¹…
    cache_info = get_cache_info()
    logger.info(f"[CACHE] í‰ê°€ ì™„ë£Œ í›„ ìºì‹œ ìƒíƒœ: {len(cache_info['cached_models'])}ê°œ ëª¨ë¸ ìºì‹œë¨")
    
    # ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥
    if 'invalid_responses' in locals() and invalid_responses:
        save_invalid_responses(invalid_responses, "evaluation", output_base_dir=output_base_dir)
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶”ì  ì¢…ë£Œ
    overall_end_time = time.time()
    overall_end_datetime = dt.datetime.now()
    overall_elapsed_time = overall_end_time - overall_start_time
    
    # ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚° ë° ì¶œë ¥
    logger.info("=" * 80)
    logger.info("â±ï¸  ì‹¤í–‰ ì‹œê°„ í†µê³„")
    logger.info("=" * 80)
    logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œì‘ ì‹œê°„: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ì „ì²´ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„: {overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ì „ì²´ ì‹¤í–‰ ì†Œìš” ì‹œê°„: {overall_elapsed_time:.2f}ì´ˆ ({overall_elapsed_time/60:.2f}ë¶„)")
    logger.info("")
    logger.info("ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„:")
    for model in models:
        if model_response_times[model]:
            avg_time = np.mean(model_response_times[model])
            total_time = np.sum(model_response_times[model])
            call_count = len(model_response_times[model])
            logger.info(f"  {model}:")
            logger.info(f"    - í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            logger.info(f"    - ì´ ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
            logger.info(f"    - í˜¸ì¶œ íšŸìˆ˜: {call_count}íšŒ")
        else:
            logger.info(f"  {model}: í˜¸ì¶œ ê¸°ë¡ ì—†ìŒ")
    logger.info("=" * 80)
    
    # ì½˜ì†”ì—ë„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("â±ï¸  ì‹¤í–‰ ì‹œê°„ í†µê³„")
    print("=" * 80)
    print(f"ì „ì²´ ì‹¤í–‰ ì‹œì‘ ì‹œê°„: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì „ì²´ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„: {overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì „ì²´ ì‹¤í–‰ ì†Œìš” ì‹œê°„: {overall_elapsed_time:.2f}ì´ˆ ({overall_elapsed_time/60:.2f}ë¶„)")
    print("")
    print("ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„:")
    for model in models:
        if model_response_times[model]:
            avg_time = np.mean(model_response_times[model])
            total_time = np.sum(model_response_times[model])
            call_count = len(model_response_times[model])
            print(f"  {model}:")
            print(f"    - í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            print(f"    - ì´ ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
            print(f"    - í˜¸ì¶œ íšŸìˆ˜: {call_count}íšŒ")
        else:
            print(f"  {model}: í˜¸ì¶œ ê¸°ë¡ ì—†ìŒ")
    print("=" * 80 + "\n")
    
    # ì‹¤í–‰ ì‹œê°„ í†µê³„ë¥¼ ë³„ë„ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë¸ë³„ë¡œ íŒŒì¼ ìƒì„±)
    try:
        saved_files = save_timing_statistics(
            overall_start_datetime,
            overall_end_datetime,
            overall_elapsed_time,
            model_response_times,
            models,
            "evaluation",
            output_base_dir=output_base_dir
        )
        logger.info(f"ì‹¤í–‰ ì‹œê°„ í†µê³„ íŒŒì¼ ì €ì¥ ì™„ë£Œ: ì´ {len(saved_files)}ê°œ íŒŒì¼")
        for file_path in saved_files:
            logger.info(f"  - {file_path}")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹œê°„ í†µê³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    return df_all, pred_long, pred_wide, acc_by_model

# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -----------------------------

def analyze_ox_questions(df: pd.DataFrame):
    """O, X ë¬¸ì œ ë¶„ì„"""
    ox_questions = df[df['is_ox_question'] == True]
    regular_questions = df[df['is_ox_question'] == False]
    
    print(f"ğŸ“Š ë¬¸ì œ ìœ í˜• ë¶„ì„")
    print(f"   - O, X ë¬¸ì œ: {len(ox_questions)}ê°œ")
    print(f"   - ì¼ë°˜ ê°ê´€ì‹: {len(regular_questions)}ê°œ")
    print(f"   - ì „ì²´: {len(df)}ê°œ")
    
    if len(ox_questions) > 0:
        print(f"\nğŸ” O, X ë¬¸ì œ ì •ë‹µ ë¶„í¬:")
        ox_answers = ox_questions['answer_set'].apply(lambda x: list(x) if x else [])
        answer_counts = {}
        for answers in ox_answers:
            for ans in answers:
                answer_counts[ans] = answer_counts.get(ans, 0) + 1
        
        for ans, count in sorted(answer_counts.items()):
            answer_text = "O" if ans == 1 else "X" if ans == 2 else str(ans)
            print(f"   - {answer_text}: {count}ê°œ")
    
    return ox_questions, regular_questions

def print_evaluation_summary(acc_df: pd.DataFrame, pred_long_df: pd.DataFrame):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìƒì„¸ ìš”ì•½")
    print("="*80)
    
    # ê¸°ë³¸ í†µê³„
    total_predictions = len(pred_long_df)
    valid_predictions = len(pred_long_df.dropna(subset=['answer']))
    invalid_predictions = total_predictions - valid_predictions
    
    print(f"ğŸ“ˆ ì „ì²´ ì˜ˆì¸¡ ìˆ˜: {total_predictions:,}")
    print(f"âœ… ìœ íš¨ ì˜ˆì¸¡: {valid_predictions:,} ({valid_predictions/total_predictions*100:.1f}%)")
    print(f"âŒ ë¬´íš¨ ì˜ˆì¸¡: {invalid_predictions:,} ({invalid_predictions/total_predictions*100:.1f}%)")
    
    # ëª¨ë¸ë³„ ì •í™•ë„
    print(f"\nğŸ† ëª¨ë¸ë³„ ì •í™•ë„ ìˆœìœ„:")
    for i, (_, row) in enumerate(acc_df.iterrows(), 1):
        accuracy = row['accuracy']
        if pd.isna(accuracy):
            print(f"  {i}. {row['model_name']}: N/A (ë°ì´í„° ì—†ìŒ)")
        else:
            print(f"  {i}. {row['model_name']}: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    print("="*80)

def save_timing_statistics(
    overall_start_datetime: dt.datetime,
    overall_end_datetime: dt.datetime,
    overall_elapsed_time: float,
    model_response_times: Dict[str, List[float]],
    models: List[str],
    filename_prefix: str = "evaluation",
    output_base_dir: str = None
):
    """ì‹¤í–‰ ì‹œê°„ í†µê³„ë¥¼ ë³„ë„ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥ (JSON ë° í…ìŠ¤íŠ¸ í˜•ì‹) - ëª¨ë¸ë³„ë¡œ íŒŒì¼ ìƒì„±"""
    if output_base_dir:
        # output_base_dirì´ ì œê³µëœ ê²½ìš° ì‚¬ìš©
        log_dir = os.path.join(output_base_dir, 'timing_stats')
    else:
        # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        try:
            from pipeline.config import ONEDRIVE_PATH
            base_path = ONEDRIVE_PATH
        except ImportError:
            import platform
            system = platform.system()
            home_dir = os.path.expanduser("~")
            if system == "Windows":
                base_path = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar")
            else:
                base_path = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar")
        log_dir = os.path.join(base_path, 'evaluation',  'eval_data', '6_exam_evaluation', 'timing_stats')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = overall_end_datetime.strftime("%Y-%m-%d_%H%M%S")
    
    saved_files = []
    
    # ê° ëª¨ë¸ë³„ë¡œ íŒŒì¼ ìƒì„±
    for model in models:
        model_name_safe = model.replace('/', '_').replace(':', '_')
        
        if model_response_times[model]:
            times = model_response_times[model]
            model_stat = {
                "average_response_time_seconds": float(np.mean(times)),
                "total_response_time_seconds": float(np.sum(times)),
                "total_response_time_minutes": float(np.sum(times) / 60),
                "call_count": len(times),
                "min_response_time_seconds": float(np.min(times)),
                "max_response_time_seconds": float(np.max(times)),
                "std_response_time_seconds": float(np.std(times))
            }
        else:
            model_stat = {
                "average_response_time_seconds": None,
                "total_response_time_seconds": None,
                "total_response_time_minutes": None,
                "call_count": 0,
                "min_response_time_seconds": None,
                "max_response_time_seconds": None,
                "std_response_time_seconds": None
            }
        
        # ëª¨ë¸ë³„ JSON íŒŒì¼ ì €ì¥
        json_data = {
            "evaluation_info": {
                "start_time": overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S'),
                "end_time": overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S'),
                "start_time_iso": overall_start_datetime.isoformat(),
                "end_time_iso": overall_end_datetime.isoformat(),
                "elapsed_time_seconds": float(overall_elapsed_time),
                "elapsed_time_minutes": float(overall_elapsed_time / 60),
                "elapsed_time_hours": float(overall_elapsed_time / 3600),
                "model_name": model
            },
            "model_statistics": model_stat
        }
        
        # ëª¨ë¸ë³„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        md_filename = os.path.join(log_dir, f"STATS_{filename_prefix}_timing_{model_name_safe}_{timestamp}.md")
        with open(md_filename, 'w', encoding='utf-8') as f:
            f.write(f"# â±ï¸ ì‹¤í–‰ ì‹œê°„ í†µê³„ - {model}\n\n")
            f.write("---\n\n")
            f.write(f"**ì „ì²´ ì‹¤í–‰ ì‹œì‘ ì‹œê°„**: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ì „ì²´ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„**: {overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ì „ì²´ ì‹¤í–‰ ì†Œìš” ì‹œê°„**: {overall_elapsed_time:.2f}ì´ˆ ({overall_elapsed_time/60:.2f}ë¶„, {overall_elapsed_time/3600:.2f}ì‹œê°„)\n\n")
            f.write(f"**ëª¨ë¸**: {model}\n\n")
            f.write("---\n\n")
            
            if model_response_times[model]:
                avg_time = np.mean(model_response_times[model])
                total_time = np.sum(model_response_times[model])
                call_count = len(model_response_times[model])
                min_time = np.min(model_response_times[model])
                max_time = np.max(model_response_times[model])
                std_time = np.std(model_response_times[model])
                
                f.write("## ì‘ë‹µ ì‹œê°„ í†µê³„\n\n")
                f.write("| í•­ëª© | ê°’ |\n")
                f.write("|------|-----|\n")
                f.write(f"| í‰ê·  ì‘ë‹µ ì‹œê°„ | {avg_time:.2f}ì´ˆ |\n")
                f.write(f"| ì´ ì‘ë‹µ ì‹œê°„ | {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„) |\n")
                f.write(f"| í˜¸ì¶œ íšŸìˆ˜ | {call_count:,}íšŒ |\n")
                f.write(f"| ìµœì†Œ ì‘ë‹µ ì‹œê°„ | {min_time:.2f}ì´ˆ |\n")
                f.write(f"| ìµœëŒ€ ì‘ë‹µ ì‹œê°„ | {max_time:.2f}ì´ˆ |\n")
                f.write(f"| í‘œì¤€ í¸ì°¨ | {std_time:.2f}ì´ˆ |\n")
            else:
                f.write("## ì‘ë‹µ ì‹œê°„ í†µê³„\n\n")
                f.write("í˜¸ì¶œ ê¸°ë¡ ì—†ìŒ\n")
        logger.info(f"ì‹¤í–‰ ì‹œê°„ í†µê³„ (ë§ˆí¬ë‹¤ìš´) ì €ì¥ [{model}]: {md_filename}")
        saved_files.append(md_filename)
    
    return saved_files

def save_invalid_responses(invalid_responses: List[Dict], filename_prefix: str = "evaluation", output_base_dir: str = None):
    """ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë¸ëª…, ë¬¸ì œ, ë‹µë³€ í¬í•¨) - ëª¨ë¸ë³„ë¡œ íŒŒì¼ ìƒì„±"""
    if not invalid_responses:
        logger.info("ë¬´íš¨ ì˜ˆì¸¡ì´ ì—†ì–´ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if output_base_dir:
        # output_base_dirì´ ì œê³µëœ ê²½ìš° ì‚¬ìš©
        invalid_dir = os.path.join(output_base_dir, 'invalid_responses')
    else:
        # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        try:
            from pipeline.config import ONEDRIVE_PATH
            base_path = ONEDRIVE_PATH
        except ImportError:
            import platform
            system = platform.system()
            home_dir = os.path.expanduser("~")
            if system == "Windows":
                base_path = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar")
            else:
                base_path = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar")
        invalid_dir = os.path.join(base_path, 'evaluation', 'eval_data', '6_exam_evaluation', 'invalid_responses')
    os.makedirs(invalid_dir, exist_ok=True)
    
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M%S")
    
    # ëª¨ë¸ë³„ë¡œ ë¬´íš¨ ì˜ˆì¸¡ ë¶„ë¥˜
    model_invalid_responses = {}
    for resp in invalid_responses:
        model = resp.get('model_name', 'unknown')
        if model not in model_invalid_responses:
            model_invalid_responses[model] = []
        model_invalid_responses[model].append(resp)
    
    saved_files = []
    
    # ê° ëª¨ë¸ë³„ë¡œ íŒŒì¼ ìƒì„±
    for model, model_responses in model_invalid_responses.items():
        model_name_safe = model.replace('/', '_').replace(':', '_')
        invalid_filename = os.path.join(invalid_dir, f"{filename_prefix}_invalid_responses_{model_name_safe}_{timestamp}.json")
        
        try:
            with open(invalid_filename, 'w', encoding='utf-8') as f:
                json.dump(model_responses, f, ensure_ascii=False, indent=2)
            logger.info(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ [{model}]: {invalid_filename}")
            logger.info(f"  - ì´ {len(model_responses)}ê°œì˜ ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ")
            saved_files.append(invalid_filename)
        except Exception as e:
            logger.error(f"ë¬´íš¨ ì˜ˆì¸¡ ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨ [{model}]: {str(e)}")
    
    # ì „ì²´ ìš”ì•½ ì •ë³´ ì¶œë ¥
    logger.info("ëª¨ë¸ë³„ ë¬´íš¨ ì˜ˆì¸¡ ìˆ˜:")
    for model, model_responses in model_invalid_responses.items():
        logger.info(f"  {model}: {len(model_responses)}ê°œ")
    
    return saved_files

def save_detailed_logs(pred_long_df: pd.DataFrame, filename_prefix: str = "evaluation"):
    """ìƒì„¸í•œ ë¡œê·¸ë¥¼ CSVë¡œ ì €ì¥"""
    # SFAICENTER_PATH ê¸°ë°˜ ê²½ë¡œ ì‚¬ìš©
    try:
        from pipeline.config import SFAICENTER_PATH
        log_base_path = SFAICENTER_PATH
    except ImportError:
        # fallback: pipelineì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        script_dir = os.path.dirname(os.path.abspath(__file__))
        log_base_path = os.path.dirname(script_dir)
    
    timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
    log_dir = os.path.join(log_base_path, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
    pred_log_filename = os.path.join(log_dir, f"{filename_prefix}_predictions_{timestamp}.csv")
    pred_long_df.to_csv(pred_log_filename, index=False, encoding='utf-8-sig')
    logger.info(f"ìƒì„¸ ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥: {pred_log_filename}")
    
    # ëª¨ë¸ë³„ í†µê³„
    model_stats = pred_long_df.groupby('model_name').agg({
        'answer': [lambda x: x.count(), lambda x: x.notna().sum(), lambda x: x.isna().sum()]
    }).round(3)
    model_stats.columns = ['ì´_ì˜ˆì¸¡ìˆ˜', 'ìœ íš¨_ì˜ˆì¸¡ìˆ˜', 'ë¬´íš¨_ì˜ˆì¸¡ìˆ˜']
    model_stats['ìœ íš¨ìœ¨'] = (model_stats['ìœ íš¨_ì˜ˆì¸¡ìˆ˜'] / model_stats['ì´_ì˜ˆì¸¡ìˆ˜'] * 100).round(1)
    
    stats_filename = os.path.join(log_dir, f"{filename_prefix}_model_stats_{timestamp}.csv")
    model_stats.to_csv(stats_filename, encoding='utf-8-sig')
    logger.info(f"ëª¨ë¸ í†µê³„ ì €ì¥: {stats_filename}")

def check_real_duplicates_in_data(json_list: List[dict]) -> Dict[str, Any]:
    """
    ë¡œë“œëœ JSON ë°ì´í„°ì—ì„œ ì§„ì§œ ì¤‘ë³µ ë¬¸ì œë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    (ë¬¸ì œ/ì •ë‹µ/ì„ íƒì§€ê°€ ëª¨ë‘ ë™ì¼í•œ ê²½ìš°ë¥¼ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨)
    
    ìµœìƒìœ„ question/options/answer êµ¬ì¡°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    
    Args:
        json_list: ê²€ì‚¬í•  JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        Dict: ì¤‘ë³µ ê²€ì‚¬ ê²°ê³¼ (ì¤‘ë³µ ê·¸ë£¹ ì •ë³´ í¬í•¨)
    """
    from collections import defaultdict
    
    # ë¬¸ì œ/ì •ë‹µ/ì„ íƒì§€ë¥¼ ì¡°í•©í•œ í‚¤ë¡œ ì¤‘ë³µ í™•ì¸
    content_keys = defaultdict(list)
    
    for i, item in enumerate(json_list):
        # ìµœìƒìœ„ êµ¬ì¡° (exam íŒŒì¼ êµ¬ì¡°)
        question = (item.get("question") or "").strip()
        answer_raw = item.get("answer") or ""
        options = item.get("options", [])
        tag = item.get("tag", "")
        
        # answerê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (ë³€í˜•ëœ ì‹œí—˜ì§€ì˜ ê²½ìš°)
        if isinstance(answer_raw, list):
            # ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•´)
            answer = '|'.join(sorted([str(a).strip() for a in answer_raw if a]))
        else:
            answer = str(answer_raw).strip()
        
        # ë¹ˆ ë¬¸ì œëŠ” ìŠ¤í‚µ (ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ)
        if not question:
            continue
        
        # optionsë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ì •ë ¬í•˜ì§€ ì•ŠìŒ)
        options_str = '|'.join([str(opt).strip() for opt in options]) if options else ''
        
        # ë¬¸ì œ/ì •ë‹µ/ì„ íƒì§€ë¥¼ ì¡°í•©í•œ í‚¤ ìƒì„±
        content_key = f"{question}|{answer}|{options_str}"
        content_keys[content_key].append({
            'index': i,
            'id': f"{item.get('file_id', '')}_{tag}",
            'question': question,
            'answer': answer,
            'options': options,
            'file_id': item.get('file_id', ''),
            'tag': tag
        })
    
    # ì§„ì§œ ì¤‘ë³µ ì°¾ê¸° (ë¬¸ì œ/ì •ë‹µ/ì„ íƒì§€ê°€ ëª¨ë‘ ë™ì¼í•œ ê²½ìš°)
    real_duplicates = {key: items for key, items in content_keys.items() if len(items) > 1}
    
    return {
        'total_count': len(json_list),
        'unique_count': len(content_keys),
        'duplicate_groups': len(real_duplicates),
        'duplicates': real_duplicates
    }

def check_data_quality(json_list: List[dict], df_all: pd.DataFrame = None):
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ (ì¤‘ë³µ ë¬¸ì œ í™•ì¸ í¬í•¨)
    
    Args:
        json_list: ê²€ì‚¬í•  JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        df_all: DataFrame í˜•íƒœì˜ ë°ì´í„° (ì„ íƒì )
    
    Returns:
        Dict: í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼
    """
    logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")
    
    issues = []
    quality_report = {
        'total_questions': len(json_list),
        'issues': [],
        'duplicate_info': None
    }
    
    # DataFrameì´ ì œê³µëœ ê²½ìš° DataFrame ê¸°ë°˜ ê²€ì‚¬
    if df_all is not None:
        # 1. ë¹ˆ ë¬¸ì œ ê²€ì‚¬
        empty_questions = df_all[df_all['question'].str.strip() == '']
        if len(empty_questions) > 0:
            issue_msg = f"ë¹ˆ ë¬¸ì œ: {len(empty_questions)}ê°œ"
            issues.append(issue_msg)
            quality_report['issues'].append({
                'type': 'empty_question',
                'count': len(empty_questions),
                'message': issue_msg
            })
        
        # 2. ë¹ˆ ì„ ì§€ ê²€ì‚¬
        empty_options = df_all[(df_all['opt1'].str.strip() == '') & 
                              (df_all['opt2'].str.strip() == '') & 
                              (df_all['opt3'].str.strip() == '') & 
                              (df_all['opt4'].str.strip() == '') & 
                              (df_all['opt5'].str.strip() == '')]
        if len(empty_options) > 0:
            issue_msg = f"ë¹ˆ ì„ ì§€ ë¬¸ì œ: {len(empty_options)}ê°œ"
            issues.append(issue_msg)
            quality_report['issues'].append({
                'type': 'empty_options',
                'count': len(empty_options),
                'message': issue_msg
            })
        
        # 3. ì •ë‹µ ì—†ëŠ” ë¬¸ì œ ê²€ì‚¬
        no_answer = df_all[df_all['answer_set'].apply(len) == 0]
        if len(no_answer) > 0:
            issue_msg = f"ì •ë‹µ ì—†ëŠ” ë¬¸ì œ: {len(no_answer)}ê°œ"
            issues.append(issue_msg)
            quality_report['issues'].append({
                'type': 'no_answer',
                'count': len(no_answer),
                'message': issue_msg
            })
    
    # 4. ì§„ì§œ ì¤‘ë³µ ë¬¸ì œ ê²€ì‚¬ (ë¬¸ì œ/ì •ë‹µ/ì„ íƒì§€ê°€ ëª¨ë‘ ë™ì¼í•œ ê²½ìš°)
    logger.info("ì¤‘ë³µ ë¬¸ì œ ê²€ì‚¬ ì¤‘...")
    duplicate_info = check_real_duplicates_in_data(json_list)
    quality_report['duplicate_info'] = duplicate_info
    
    if duplicate_info['duplicate_groups'] > 0:
        issue_msg = f"ì¤‘ë³µ ë¬¸ì œ ê·¸ë£¹: {duplicate_info['duplicate_groups']}ê°œ (ì´ {duplicate_info['total_count']}ê°œ ì¤‘ ê³ ìœ : {duplicate_info['unique_count']}ê°œ)"
        issues.append(issue_msg)
        quality_report['issues'].append({
            'type': 'duplicates',
            'count': duplicate_info['duplicate_groups'],
            'message': issue_msg
        })
        
        # ì¤‘ë³µ ìƒì„¸ ì •ë³´ ë¡œê¹… (ìµœëŒ€ 10ê°œ ê·¸ë£¹ë§Œ)
        logger.warning(f"ì¤‘ë³µ ë¬¸ì œ ë°œê²¬: {duplicate_info['duplicate_groups']}ê°œ ê·¸ë£¹")
        for i, (content_key, items) in enumerate(list(duplicate_info['duplicates'].items())[:10], 1):
            logger.warning(f"  ì¤‘ë³µ ê·¸ë£¹ {i}: {len(items)}ê°œ í•­ëª©")
            for item in items[:3]:  # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                logger.warning(f"    - ID: {item['id']}, ë¬¸ì œ: {item['question'][:50]}...")
            if len(items) > 3:
                logger.warning(f"    ... ì™¸ {len(items) - 3}ê°œ í•­ëª©")
    else:
        logger.info(f"ì¤‘ë³µ ë¬¸ì œ ì—†ìŒ: ëª¨ë“  {duplicate_info['total_count']}ê°œ ë¬¸ì œê°€ ê³ ìœ í•©ë‹ˆë‹¤")
    
    # ê²°ê³¼ ìš”ì•½
    if issues:
        logger.warning("ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œê²¬:")
        for issue in issues:
            logger.warning(f"  - {issue}")
    else:
        logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ âœ…")
    
    return quality_report

def calculate_domain_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Domainë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ domain ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'domain', 'subdomain']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Domainë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    domain_acc = (
        merged.groupby(["domain", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index="domain", columns="model_name", values="correct")
        .reset_index()
    )
    
    return domain_acc

def calculate_subdomain_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Subdomainë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ subdomain ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'domain', 'subdomain']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚°
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Subdomainë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    subdomain_acc = (
        merged.groupby(["domain", "subdomain", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index=["domain", "subdomain"], columns="model_name", values="correct")
        .reset_index()
    )
    
    return subdomain_acc

def calculate_subject_accuracy(pred_long: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Subjectë³„ ì •í™•ë„ ê³„ì‚° - ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœë¡œ ë°˜í™˜"""
    # pred_longê³¼ df_allì„ ë³‘í•©í•˜ì—¬ subject ì •ë³´ ì¶”ê°€
    merged = pred_long.merge(df_all[['id', 'subject']], on='id', how='left')
    
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚°
    def _is_correct(pred: float, s: Set[int]) -> float:
        if np.isnan(pred) or not s:
            return np.nan
        return float(int(pred) in s)
    
    # answer_set ì •ë³´ ì¶”ê°€
    merged = merged.merge(df_all[['id', 'answer_set']], on='id', how='left')
    merged["correct"] = merged.apply(lambda r: _is_correct(r["answer"], r["answer_set"]), axis=1)
    
    # Subjectë³„ ì •í™•ë„ ê³„ì‚° (ëª¨ë¸ë³„ ì»¬ëŸ¼ í˜•íƒœ)
    subject_acc = (
        merged.groupby(["subject", "model_name"], dropna=False)["correct"]
        .mean()
        .reset_index()
        .pivot(index="subject", columns="model_name", values="correct")
        .reset_index()
    )
    
    return subject_acc

def save_results_to_excel(df_all: pd.DataFrame, pred_wide: pd.DataFrame, acc: pd.DataFrame, pred_long: pd.DataFrame = None, filename: str = None):
    """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥ (domain, subdomain ë¶„ì„ í¬í•¨)"""
    
    # ONEDRIVE_PATH ê¸°ë°˜ ê²½ë¡œ ì‚¬ìš©
    try:
        from pipeline.config import ONEDRIVE_PATH
        default_base_path = os.path.join(ONEDRIVE_PATH, 'evaluation', 'result')
    except ImportError:
        import platform
        system = platform.system()
        home_dir = os.path.expanduser("~")
        if system == "Windows":
            default_base_path = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar", "evaluation", "result")
        else:
            default_base_path = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar", "evaluation", "result")
    
    if filename is None:
        timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
        filename = f"{default_base_path}evaluation_results_{timestamp}.xlsx"
    elif not filename.startswith(('/', './', 'evaluation/')):
        # íŒŒì¼ëª…ë§Œ ì£¼ì–´ì§„ ê²½ìš° (í™•ì¥ì í¬í•¨ ì—¬ë¶€ í™•ì¸)
        timestamp = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
        if filename.endswith('.xlsx'):
            # í™•ì¥ìê°€ ìˆëŠ” ê²½ìš°
            name = filename[:-5]  # .xlsx ì œê±°
            filename = f"{default_base_path}{name}_{timestamp}.xlsx"
        else:
            # í™•ì¥ìê°€ ì—†ëŠ” ê²½ìš°
            filename = f"{default_base_path}{filename}_{timestamp}.xlsx"
    elif filename.startswith('evaluation/'):
        # evaluation/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        filename = f"{default_base_path}{filename}"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    logger.info(f"ê²°ê³¼ë¥¼ {filename}ì— ì €ì¥ ì¤‘...")
    
    # ë¶„ì„ ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
    domain_acc = None
    
    try:
        with pd.ExcelWriter(filename, engine="openpyxl") as w:
            # ê¸°ë³¸ ì‹œíŠ¸ë“¤
            df_all.to_excel(w, index=False, sheet_name="ì „ì²´ë°ì´í„°")
            pred_wide.to_excel(w, index=False, sheet_name="ëª¨ë¸ë³„ì˜ˆì¸¡")
            acc.to_excel(w, index=False, sheet_name="ì •í™•ë„")
            
            # Subject, Domain, Subdomain ë¶„ì„ ì¶”ê°€ (pred_longì´ ì œê³µëœ ê²½ìš°)
            if pred_long is not None and 'domain' in df_all.columns:
                # Subjectë³„ ì •í™•ë„ ê³„ì‚° (subject ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                if 'subject' in df_all.columns:
                    logger.info("Subjectë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                    subject_acc = calculate_subject_accuracy(pred_long, df_all)
                    subject_acc.to_excel(w, index=False, sheet_name="Subjectë³„ì •í™•ë„")
                    
                    # Subjectë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                    subject_stats = df_all.groupby('subject').size().reset_index(name='question_count')
                    subject_stats.to_excel(w, index=False, sheet_name="Subjectë³„ë¬¸ì œìˆ˜")
                
                logger.info("Domainë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                domain_acc = calculate_domain_accuracy(pred_long, df_all)
                domain_acc.to_excel(w, index=False, sheet_name="Domainë³„ì •í™•ë„")
                
                # Domainë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                domain_stats = df_all.groupby('domain').size().reset_index(name='question_count')
                domain_stats.to_excel(w, index=False, sheet_name="Domainë³„ë¬¸ì œìˆ˜")
                
                logger.info("Subdomainë³„ ì •í™•ë„ ê³„ì‚° ì¤‘...")
                subdomain_acc = calculate_subdomain_accuracy(pred_long, df_all)
                subdomain_acc.to_excel(w, index=False, sheet_name="Subdomainë³„ì •í™•ë„")
                
                # Subdomainë³„ ë¬¸ì œ ìˆ˜ í†µê³„
                subdomain_stats = df_all.groupby(['domain', 'subdomain']).size().reset_index(name='question_count')
                subdomain_stats.to_excel(w, index=False, sheet_name="Subdomainë³„ë¬¸ì œìˆ˜")
            
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        
        # Domain ë¶„ì„ ìš”ì•½ ì¶œë ¥
        if pred_long is not None and 'domain' in df_all.columns and domain_acc is not None:
            print_domain_analysis_summary(df_all, domain_acc)
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def print_domain_analysis_summary(df_all: pd.DataFrame, domain_acc: pd.DataFrame):
    """Domain ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ë¶„ì•¼ë³„ ë¶„ì„ ìš”ì•½")
    print("="*80)
    
    # Domainë³„ ë¬¸ì œ ìˆ˜
    domain_counts = df_all['domain'].value_counts()
    print("ğŸ“ˆ Domainë³„ ë¬¸ì œ ìˆ˜:")
    for domain, count in domain_counts.items():
        print(f"  - {domain}: {count}ê°œ")
    
    # Subjectë³„ ë¬¸ì œ ìˆ˜ (subject ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
    if 'subject' in df_all.columns:
        subject_counts = df_all['subject'].value_counts()
        print("\nğŸ“š Subjectë³„ ë¬¸ì œ ìˆ˜:")
        for subject, count in subject_counts.items():
            if subject:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶œë ¥
                print(f"  - {subject}: {count}ê°œ")
    
    # Domainë³„ ì •í™•ë„ (ëª¨ë¸ë³„)
    print(f"\nğŸ† Domainë³„ ì •í™•ë„ (ìƒìœ„ ëª¨ë¸ ê¸°ì¤€):")
    for domain in domain_counts.index:
        domain_data = domain_acc[domain_acc['domain'] == domain]
        if len(domain_data) > 0:
            # ëª¨ë¸ë³„ ì»¬ëŸ¼ì—ì„œ ìµœê³  ì •í™•ë„ ì°¾ê¸°
            model_columns = [col for col in domain_data.columns if col != 'domain']
            if model_columns:
                best_accuracy = 0
                best_model = ""
                for model in model_columns:
                    acc = domain_data[model].iloc[0]
                    if not pd.isna(acc) and acc > best_accuracy:
                        best_accuracy = acc
                        best_model = model
                if best_model:
                    print(f"  - {domain}: {best_model} ({best_accuracy:.3f})")
    
    print("="*80)

# -----------------------------
# íƒœê·¸ ëŒ€ì¹˜ í•¨ìˆ˜ë“¤
# -----------------------------

# TagProcessorë¥¼ ì§ì ‘ ì‚¬ìš©
from qna.qna_processor import TagProcessor

# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -----------------------------

def extract_subject_from_filename(filename: str) -> str:
    """íŒŒì¼ëª…ì—ì„œ subject ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        filename: íŒŒì¼ëª… (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1_exam.json" ë˜ëŠ” "ê¸ˆìœµì‹¤ë¬´1_exam_transformed.json")
    
    Returns:
        str: ì¶”ì¶œëœ subject (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1")
    """
    if '_exam_transformed.json' in filename:
        # ë³€í˜•ëœ exam íŒŒì¼ì¸ ê²½ìš° íŒŒì¼ëª…ì—ì„œ subject ì¶”ì¶œ
        # íŒŒì¼ëª… í˜•ì‹: "{exam_name}_exam_transformed.json" (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1_exam_transformed.json")
        subject = filename.split("_exam_transformed.json")[0]
        return subject
    elif '_exam.json' in filename:
        # exam íŒŒì¼ì¸ ê²½ìš° íŒŒì¼ëª…ì—ì„œ subject ì¶”ì¶œ
        # íŒŒì¼ëª… í˜•ì‹: "{exam_name}_exam.json" (ì˜ˆ: "ê¸ˆìœµì‹¤ë¬´1_exam.json")
        subject = filename.split("_exam.json")[0]
        return subject
    else:
        # ì¼ë°˜ íŒŒì¼ì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""

# -----------------------------
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# -----------------------------

def load_data_from_directory(data_path: str, apply_tag_replacement: bool = False) -> List[dict]:
    """ë””ë ‰í† ë¦¬ ë˜ëŠ” íŒŒì¼ ê²½ë¡œì—ì„œ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Args:
        data_path: ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ
        apply_tag_replacement: íƒœê·¸ ëŒ€ì¹˜ ì ìš© ì—¬ë¶€
    
    Returns:
        List[dict]: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    json_files = []
    
    # íŒŒì¼ ê²½ë¡œì¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ì§€ í™•ì¸
    # ë¨¼ì € ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    logger.info(f"ê²½ë¡œ í™•ì¸ ì¤‘: {data_path}")
    logger.info(f"  - ê²½ë¡œ ì¡´ì¬: {os.path.exists(data_path)}")
    logger.info(f"  - íŒŒì¼ì¸ì§€: {os.path.isfile(data_path)}")
    logger.info(f"  - ë””ë ‰í† ë¦¬ì¸ì§€: {os.path.isdir(data_path)}")
    
    if not os.path.exists(data_path):
        logger.warning(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return []
    
    if os.path.isfile(data_path):
        # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
        logger.info(f"íŒŒì¼ ê²½ë¡œë¡œ ì¸ì‹: {data_path}")
        if data_path.endswith(".json") and ('merged' not in os.path.basename(data_path)):
            json_files.append(data_path)
            logger.info(f"JSON íŒŒì¼ ì¶”ê°€: {data_path}")
        else:
            logger.warning(f"JSON íŒŒì¼ì´ ì•„ë‹ˆê±°ë‚˜ merged íŒŒì¼ì…ë‹ˆë‹¤: {data_path}")
            logger.warning(f"  - íŒŒì¼ëª…: {os.path.basename(data_path)}")
            logger.warning(f"  - .jsonìœ¼ë¡œ ëë‚˜ëŠ”ì§€: {data_path.endswith('.json')}")
            logger.warning(f"  - merged í¬í•¨: {'merged' in os.path.basename(data_path)}")
    elif os.path.isdir(data_path):
        # ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ ê²½ìš°
        logger.debug(f"ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ì¸ì‹: {data_path}")
        for root, _, files in os.walk(data_path):
            for f in files:
                if f.endswith(".json") and ('merged' not in f):
                    json_files.append(os.path.join(root, f))
    else:
        logger.warning(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        logger.warning(f"  - íŒŒì¼ ì¡´ì¬: {os.path.exists(data_path)}")
        logger.warning(f"  - íŒŒì¼ì¸ì§€: {os.path.isfile(data_path)}")
        logger.warning(f"  - ë””ë ‰í† ë¦¬ì¸ì§€: {os.path.isdir(data_path)}")
        return []
    
    logger.info(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")
    
    all_data = []
    for file_path in json_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # íŒŒì¼ëª…ì—ì„œ subject ì¶”ì¶œ (fallbackìš©)
                filename = os.path.basename(file_path)
                subject_from_filename = extract_subject_from_filename(filename)
                
                if isinstance(data, list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì— subject ì¶”ê°€
                    for item in data:
                        # JSON ë‚´ë¶€ì— ì´ë¯¸ subjectê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
                        if 'subject' not in item or not item.get('subject'):
                            if subject_from_filename:
                                item['subject'] = subject_from_filename
                    all_data.extend(data)
                else:
                    # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° subject ì¶”ê°€
                    # JSON ë‚´ë¶€ì— ì´ë¯¸ subjectê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
                    if 'subject' not in data or not data.get('subject'):
                        if subject_from_filename:
                            data['subject'] = subject_from_filename
                    all_data.append(data)
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file_path} - {str(e)}")
    
    logger.info(f"ë¡œë“œëœ ì´ ë°ì´í„° ìˆ˜: {len(all_data)}")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì ìš©
    if apply_tag_replacement:
        logger.info("íƒœê·¸ ëŒ€ì¹˜ ì ìš© ì¤‘...")
        processed_count = 0
        for item in all_data:
            if 'additional_tags_found' in item and item['additional_tags_found']:
                if 'additional_tag_data' in item:
                    item['qna_data'] = TagProcessor.replace_tags_in_qna_data(
                        item['qna_data'], 
                        item['additional_tag_data']
                    )
                    processed_count += 1
        logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì™„ë£Œ: {processed_count}ê°œ í•­ëª© ì²˜ë¦¬")
    
    return all_data

def filter_multiple_choice_questions(data: List[dict]) -> List[dict]:
    """ê°ê´€ì‹ ë¬¸ì œë§Œ í•„í„°ë§"""
    multiple_choice = []
    for item in data:
        if item.get('qna_type') == "multiple-choice":
            multiple_choice.append(item)
    
    logger.info(f"ê°ê´€ì‹ ë¬¸ì œ ìˆ˜: {len(multiple_choice)}")
    return multiple_choice

# -----------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -----------------------------

def main():
    parser = argparse.ArgumentParser(description='LLM í‰ê°€ ì‹œìŠ¤í…œ')
    parser.add_argument('--data_path', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--sample_size', type=int, default=1000, help='ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ê°’: 1000)')
    parser.add_argument('--batch_size', type=int, default=10, help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--models', nargs='+', default=['anthropic/claude-sonnet-4.5', 'google/gemini-2.5-flash', 'openai/gpt-5', 'google/gemini-2.5-pro', 'google/gemma-3-27b-it:free'], help='í‰ê°€í•  ëª¨ë¸ ëª©ë¡')
    parser.add_argument('--use_ox_support', action='store_true', help='O, X ë¬¸ì œ ì§€ì› í™œì„±í™”')
    parser.add_argument('--apply_tag_replacement', action='store_true', default=False, help='íƒœê·¸ ëŒ€ì¹˜ ì ìš© (ê¸°ë³¸ê°’: False)')
    parser.add_argument('--no_tag_replacement', action='store_true', help='íƒœê·¸ ëŒ€ì¹˜ ë¹„í™œì„±í™” (deprecated: ê¸°ë³¸ê°’ì´ Falseì´ë¯€ë¡œ ë” ì´ìƒ í•„ìš” ì—†ìŒ)')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    parser.add_argument('--output_filename', type=str, help='ê²°ê³¼ Excel íŒŒì¼ëª… (ê¸°ë³¸ê°’: ìë™ ìƒì„±)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”')
    
    # API ëª¨ë“œ ì˜µì…˜ ì¶”ê°€
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--api', action='store_true', help='OpenRouter API ëª¨ë“œë¡œ ì‹¤í–‰ (ê¸°ë³¸ê°’)')
    mode_group.add_argument('--server', action='store_true', help='vLLM ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    
    logger.info("=" * 60)
    logger.info("LLM í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    logger.info(f"ìƒ˜í”Œ í¬ê¸°: {args.sample_size}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"ëª¨ë¸: {args.models}")
    logger.info(f"O, X ë¬¸ì œ ì§€ì›: {args.use_ox_support}")
    logger.info(f"ì¶œë ¥ íŒŒì¼ëª…: {args.output_filename or 'ìë™ ìƒì„±'}")
    
    # API ëª¨ë“œ í™•ì¸
    use_server_mode = args.server
    if use_server_mode:
        logger.info("ëª¨ë“œ: vLLM ì„œë²„ ëª¨ë“œ")
    else:
        logger.info("ëª¨ë“œ: OpenRouter API ëª¨ë“œ (ê¸°ë³¸ê°’)")
    
    # íƒœê·¸ ëŒ€ì¹˜ ì˜µì…˜ ì²˜ë¦¬
    apply_tag_replacement = args.apply_tag_replacement
    if args.no_tag_replacement:
        apply_tag_replacement = False
        logger.warning("--no_tag_replacement ì˜µì…˜ì€ deprecatedì…ë‹ˆë‹¤. --apply_tag_replacementë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ì´ Falseì…ë‹ˆë‹¤.")
    logger.info(f"íƒœê·¸ ëŒ€ì¹˜ ì ìš©: {apply_tag_replacement}")
    
    try:
        # ë°ì´í„° ë¡œë”©
        logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
        all_data = load_data_from_directory(args.data_path, apply_tag_replacement)
        
        # ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        multiple_choice_data = all_data
        
        # ìƒ˜í”Œë§
        if len(multiple_choice_data) > args.sample_size:
            random.seed(args.seed)
            sample_data = random.sample(multiple_choice_data, args.sample_size)
        else:
            sample_data = multiple_choice_data
            logger.info(f"ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(sample_data)}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        df_temp = json_to_df_all(sample_data, use_ox_support=args.use_ox_support)
        
        quality_report = check_data_quality(sample_data, df_temp)
        
        # O, X ë¬¸ì œ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support:
            ox_questions, regular_questions = analyze_ox_questions(df_temp)
        
        # í‰ê°€ ì‹¤í–‰
        logger.info("í‰ê°€ ì‹¤í–‰ ì¤‘...")
        df_all, pred_long, pred_wide, acc = run_eval_pipeline(
            sample_data, args.models, args.sample_size, args.batch_size, args.seed, use_server_mode, args.use_ox_support
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print_evaluation_summary(acc, pred_long)
        
        # O, X ë¬¸ì œë³„ ì •í™•ë„ ë¶„ì„ (ì§€ì›í•˜ëŠ” ê²½ìš°)
        if args.use_ox_support and 'is_ox_question' in df_all.columns:
            sample_with_type = df_all[df_all['id'].isin(pred_long['id'])][['id', 'is_ox_question']].copy()
            merged_with_type = pred_long.merge(sample_with_type, on='id', how='left')
            
            if len(merged_with_type[merged_with_type['is_ox_question'] == True]) > 0:
                print("\n" + "="*60)
                print("ğŸ“Š O, X ë¬¸ì œ vs ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„ ë¹„êµ")
                print("="*60)
                
                # O, X ë¬¸ì œ ì •í™•ë„
                ox_correct = merged_with_type[merged_with_type['is_ox_question'] == True]['answer'].notna().sum()
                ox_total = len(merged_with_type[merged_with_type['is_ox_question'] == True])
                ox_acc = ox_correct / ox_total if ox_total > 0 else 0
                
                # ì¼ë°˜ ê°ê´€ì‹ ì •í™•ë„
                regular_correct = merged_with_type[merged_with_type['is_ox_question'] == False]['answer'].notna().sum()
                regular_total = len(merged_with_type[merged_with_type['is_ox_question'] == False])
                regular_acc = regular_correct / regular_total if regular_total > 0 else 0
                
                print(f"O, X ë¬¸ì œ: {ox_correct}/{ox_total} ({ox_acc:.1%})")
                print(f"ì¼ë°˜ ê°ê´€ì‹: {regular_correct}/{regular_total} ({regular_acc:.1%})")
        
        # ìƒì„¸ ë¡œê·¸ ì €ì¥
        # save_detailed_logs(pred_long, "evaluation")
        
        # Excel íŒŒì¼ ì €ì¥
        save_results_to_excel(df_all, pred_wide, acc, pred_long, args.output_filename)
        
        logger.info("=" * 60)
        logger.info("í‰ê°€ ì™„ë£Œ")
        logger.info("=" * 60)
        
        # í‰ê°€ ì™„ë£Œ í›„ ìºì‹œ ì •ë¦¬ (ì„ íƒì )
        if not use_server_mode:  # API ëª¨ë“œì—ì„œëŠ” ìºì‹œ ì •ë¦¬í•˜ì§€ ì•ŠìŒ (ì¬ì‚¬ìš© ê°€ëŠ¥)
            logger.info("[CACHE] API ëª¨ë“œì´ë¯€ë¡œ ìºì‹œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
        else:
            logger.info("[CACHE] vLLM ì„œë²„ ëª¨ë“œì´ë¯€ë¡œ ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
            clear_model_cache()
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìºì‹œ ì •ë¦¬
        clear_model_cache()
        raise

if __name__ == "__main__":
    main()
