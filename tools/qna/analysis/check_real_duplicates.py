#!/usr/bin/env python3
import json
import os
from collections import defaultdict
import sys
import glob
from datetime import datetime
import shutil

def check_real_duplicates_single_file(file_path, return_details=False):
    """
    ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¬¸ì œ/ì •ë‹µ/í•´ì„¤/ì„ íƒì§€ê°€ ëª¨ë‘ ë™ì¼í•œ ì§„ì§œ ì¤‘ë³µì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
        return 0, 0, {} if return_details else (0, 0)
    
    print(f"ğŸ“ íŒŒì¼: {os.path.basename(file_path)}")
    print(f"   ì´ Q&A ê°œìˆ˜: {len(data)}")
    
    # ë¬¸ì œ/ì •ë‹µ/í•´ì„¤/ì„ íƒì§€ë¥¼ ì¡°í•©í•œ í‚¤ë¡œ ì¤‘ë³µ í™•ì¸
    content_keys = defaultdict(list)
    
    for i, item in enumerate(data):
        qna_data = item.get('qna_data', {})
        description = qna_data.get('description', {})
        
        question = description.get('question', '').strip()
        answer = description.get('answer', '').strip()
        explanation = description.get('explanation', '').strip()
        options = description.get('options', [])
        
        # optionsë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ì •ë ¬í•˜ì§€ ì•ŠìŒ)
        options_str = '|'.join([opt.strip() for opt in options]) if options else ''
        
        # ë¬¸ì œ/ì •ë‹µ/í•´ì„¤/ì„ íƒì§€ë¥¼ ì¡°í•©í•œ í‚¤ ìƒì„±
        content_key = f"{question}|{answer}|{explanation}|{options_str}"
        content_keys[content_key].append({
            'index': i,
            'page': item.get('page', ''),
            'question': question,
            'answer': answer,
            'explanation': explanation,
            'options': options
        })
    
    # ì§„ì§œ ì¤‘ë³µ ì°¾ê¸° (ë¬¸ì œ/ì •ë‹µ/í•´ì„¤/ì„ íƒì§€ê°€ ëª¨ë‘ ë™ì¼í•œ ê²½ìš°)
    real_duplicates = {key: items for key, items in content_keys.items() if len(items) > 1}
    
    print(f"   ê³ ìœ í•œ Q&A ì¡°í•©: {len(content_keys)}ê°œ")
    print(f"   ì¤‘ë³µëœ Q&A ì¡°í•©: {len(real_duplicates)}ê°œ")
    
    if real_duplicates:
        print(f"   âš ï¸  ì§„ì§œ ì¤‘ë³µ ë°œê²¬:")
        for i, (content_key, items) in enumerate(real_duplicates.items()):
            print(f"     ì¤‘ë³µ ê·¸ë£¹ {i+1}:")
            for item in items:
                print(f"       - ì¸ë±ìŠ¤ {item['index']}, í˜ì´ì§€ {item['page']}: {item['question'][:20]}...")
    else:
        print(f"   âœ… ì§„ì§œ ì¤‘ë³µ ì—†ìŒ")
    
    if return_details:
        return len(data), len(real_duplicates), real_duplicates
    else:
        return len(data), len(real_duplicates)

def save_duplicates_report(duplicates_data, output_dir):
    """
    ì¤‘ë³µ ê²€ì‚¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    # # JSON í˜•íƒœë¡œ ìƒì„¸ ì •ë³´ ì €ì¥
    # json_file = os.path.join(output_dir, f"duplicates_report_{timestamp}.json")
    # with open(json_file, 'w', encoding='utf-8') as f:
    #     json.dump(duplicates_data, f, ensure_ascii=False, indent=2)
    
    # í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸ ì €ì¥
    txt_file = os.path.join(os.path.dirname(output_dir), f"duplicates_report_{timestamp}.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ì¤‘ë³µ ê²€ì‚¬ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
        f.write("=" * 80 + "\n")
        f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ê²€ì‚¬ëœ íŒŒì¼ ìˆ˜: {duplicates_data['summary']['total_files']}ê°œ\n")
        f.write(f"ì´ Q&A ê°œìˆ˜: {duplicates_data['summary']['total_qna']}ê°œ\n")
        f.write(f"ì´ ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {duplicates_data['summary']['total_duplicates']}ê°œ\n")
        f.write(f"ì¤‘ë³µì´ ìˆëŠ” íŒŒì¼ ìˆ˜: {duplicates_data['summary']['files_with_duplicates']}ê°œ\n")
        f.write("\n")
        
        if duplicates_data['summary']['files_with_duplicates'] > 0:
            f.write("ì¤‘ë³µì´ ë°œê²¬ëœ íŒŒì¼ë“¤:\n")
            f.write("-" * 40 + "\n")
            
            for file_info in duplicates_data['files_with_duplicates']:
                f.write(f"\nğŸ“ íŒŒì¼: {file_info['filename']}\n")
                f.write(f"   ê²½ë¡œ: {file_info['filepath']}\n")
                f.write(f"   ì´ Q&A ê°œìˆ˜: {file_info['total_qna']}ê°œ\n")
                f.write(f"   ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {file_info['duplicate_groups']}ê°œ\n")
                f.write("\n")
                
                for group_idx, (content_key, items) in enumerate(file_info['duplicates'].items(), 1):
                    f.write(f"   ì¤‘ë³µ ê·¸ë£¹ {group_idx}:\n")
                    for item in items:
                        f.write(f"     - ì¸ë±ìŠ¤ {item['index']}, í˜ì´ì§€ {item['page']}\n")
                        f.write(f"       ë¬¸ì œ: {item['question'][:100]}{'...' if len(item['question']) > 100 else ''}\n")
                        f.write(f"       ì •ë‹µ: {item['answer'][:100]}{'...' if len(item['answer']) > 100 else ''}\n")
                        f.write(f"       í•´ì„¤: {item['explanation'][:100]}{'...' if len(item['explanation']) > 100 else ''}\n")
                        if item.get('options'):
                            f.write(f"       ì„ íƒì§€:\n")
                            for opt_idx, option in enumerate(item['options'], 1):
                                f.write(f"         {opt_idx}. {option[:80]}{'...' if len(option) > 80 else ''}\n")
                        f.write("\n")
        else:
            f.write("âœ… ëª¨ë“  íŒŒì¼ì—ì„œ ì¤‘ë³µ ì—†ìŒ - ëª¨ë“  Q&Aê°€ ê³ ìœ í•©ë‹ˆë‹¤!\n")
    
    return txt_file

def remove_duplicates_from_file(file_path, duplicates_data, create_backup=True):
    """
    íŒŒì¼ì—ì„œ ì¤‘ë³µëœ ë¬¸ì œë“¤ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜ (ì¸ë±ìŠ¤ê°€ í° ê²ƒë“¤ ì‚­ì œ)
    """
    try:
        # ì›ë³¸ íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°±ì—… ìƒì„±
        if create_backup:
            backup_path = file_path + f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.copy2(file_path, backup_path)
            print(f"ğŸ“ ë°±ì—… íŒŒì¼ ìƒì„±: {backup_path}")
        
        # ì‚­ì œí•  ì¸ë±ìŠ¤ë“¤ ìˆ˜ì§‘ (ê° ì¤‘ë³µ ê·¸ë£¹ì—ì„œ ì¸ë±ìŠ¤ê°€ ê°€ì¥ í° ê²ƒë“¤ ì œì™¸)
        indices_to_remove = set()
        
        for content_key, items in duplicates_data.items():
            if len(items) > 1:
                # ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_items = sorted(items, key=lambda x: x['index'])
                # ì²« ë²ˆì§¸(ì¸ë±ìŠ¤ê°€ ê°€ì¥ ì‘ì€) ê²ƒë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
                for item in sorted_items[1:]:
                    indices_to_remove.add(item['index'])
        
        # ì‚­ì œí•  ì¸ë±ìŠ¤ë“¤ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ (ë’¤ì—ì„œë¶€í„° ì‚­ì œ)
        sorted_indices = sorted(indices_to_remove, reverse=True)
        
        # ì¤‘ë³µ ë¬¸ì œë“¤ ì‚­ì œ
        removed_count = 0
        for index in sorted_indices:
            if 0 <= index < len(data):
                del data[index]
                removed_count += 1
        
        # ìˆ˜ì •ëœ ë°ì´í„° ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {removed_count}ê°œì˜ ì¤‘ë³µ ë¬¸ì œ ì‚­ì œ ì™„ë£Œ")
        return removed_count, len(data)
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
        return 0, 0

def find_extracted_qna_files(directory_path):
    """
    ë””ë ‰í† ë¦¬ í•˜ìœ„ì˜ ëª¨ë“  extracted_qna.json íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    pattern = os.path.join(directory_path, "**", "*extracted_qna.json")
    files = glob.glob(pattern, recursive=True)
    return sorted(files)

def check_real_duplicates(directory_path, remove_duplicates=False):
    """
    ë””ë ‰í† ë¦¬ í•˜ìœ„ì˜ ëª¨ë“  extracted_qna.json íŒŒì¼ì„ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"ğŸ” ê²€ì‚¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {directory_path}")
    
    # extracted_qna.json íŒŒì¼ë“¤ ì°¾ê¸°
    files = find_extracted_qna_files(directory_path)
    
    if not files:
        print(f"âŒ extracted_qna.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    print("=" * 80)
    
    total_qna = 0
    total_duplicates = 0
    files_with_duplicates = 0
    files_with_duplicates_data = []
    
    # ê° íŒŒì¼ë³„ë¡œ ê²€ì‚¬
    for i, file_path in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}]")
        qna_count, duplicate_count, duplicates = check_real_duplicates_single_file(file_path, return_details=True)
        
        total_qna += qna_count
        total_duplicates += duplicate_count
        if duplicate_count > 0:
            files_with_duplicates += 1
            files_with_duplicates_data.append({
                'filename': os.path.basename(file_path),
                'filepath': file_path,
                'total_qna': qna_count,
                'duplicate_groups': duplicate_count,
                'duplicates': duplicates
            })
        
        print("-" * 40)
    
    # ì „ì²´ ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ì „ì²´ ê²€ì‚¬ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}")
    print(f"ê²€ì‚¬ëœ íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    print(f"ì´ Q&A ê°œìˆ˜: {total_qna}ê°œ")
    print(f"ì´ ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {total_duplicates}ê°œ")
    print(f"ì¤‘ë³µì´ ìˆëŠ” íŒŒì¼ ìˆ˜: {files_with_duplicates}ê°œ")
    
    if total_duplicates == 0:
        print(f"âœ… ëª¨ë“  íŒŒì¼ì—ì„œ ì¤‘ë³µ ì—†ìŒ - ëª¨ë“  Q&Aê°€ ê³ ìœ í•©ë‹ˆë‹¤!")
    else:
        print(f"âš ï¸  {files_with_duplicates}ê°œ íŒŒì¼ì—ì„œ ì¤‘ë³µ ë°œê²¬ - ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¤‘ë³µì´ ìˆëŠ” íŒŒì¼ ìˆ˜ê°€ 1ê°œ ì´ìƒì´ë©´ ë¦¬í¬íŠ¸ ì €ì¥
        if files_with_duplicates >= 1:
            print(f"\nğŸ’¾ ì¤‘ë³µ ê²€ì‚¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            duplicates_data = {
                'summary': {
                    'total_files': len(files),
                    'total_qna': total_qna,
                    'total_duplicates': total_duplicates,
                    'files_with_duplicates': files_with_duplicates
                },
                'files_with_duplicates': files_with_duplicates_data
            }
            
            # ë¦¬í¬íŠ¸ ì €ì¥ (í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥)
            try:
                txt_file = save_duplicates_report(duplicates_data, directory_path)
                # print(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_file}")
                print(f"âœ… í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥: {txt_file}")
            except Exception as e:
                print(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ì¤‘ë³µ ì‚­ì œ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
            if remove_duplicates:
                print(f"\nğŸ—‘ï¸  ì¤‘ë³µ ë¬¸ì œ ì‚­ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                total_removed = 0
                files_processed = 0
                
                for file_info in files_with_duplicates_data:
                    file_path = file_info['filepath']
                    print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)}")
                    
                    removed_count, remaining_count = remove_duplicates_from_file(
                        file_path, 
                        file_info['duplicates']
                    )
                    
                    if removed_count > 0:
                        total_removed += removed_count
                        files_processed += 1
                        print(f"   ì‚­ì œëœ ë¬¸ì œ: {removed_count}ê°œ")
                        print(f"   ë‚¨ì€ ë¬¸ì œ: {remaining_count}ê°œ")
                
                print(f"\nğŸ“Š ì¤‘ë³µ ì‚­ì œ ì™„ë£Œ:")
                print(f"   ì²˜ë¦¬ëœ íŒŒì¼: {files_processed}ê°œ")
                print(f"   ì´ ì‚­ì œëœ ë¬¸ì œ: {total_removed}ê°œ")
                
                # ì‚­ì œ í›„ ì¬ê²€ì‚¬
                print(f"\nğŸ” ì‚­ì œ í›„ ì¬ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                check_real_duplicates(directory_path, remove_duplicates=False)
    
    return total_qna, total_duplicates

if __name__ == "__main__":
    if len(sys.argv) < 3 or len(sys.argv) > 4:
        print("ì‚¬ìš©ë²•: python check_real_duplicates.py <cycle> [--remove]")
        print("ì˜ˆì‹œ: python check_real_duplicates.py 1")
        print("ì˜ˆì‹œ: python check_real_duplicates.py 1 --remove")
        sys.exit(1)
    
    cycle = sys.argv[1]
    remove_duplicates = len(sys.argv) == 4 and sys.argv[3] == "--remove"

# pipeline/configì—ì„œ ONEDRIVE_PATH import ì‹œë„
try:
    import sys
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
    sys.path.insert(0, project_root)
    from tools import ONEDRIVE_PATH
except ImportError:
    # fallback: pipelineì´ ì—†ëŠ” ê²½ìš° í”Œë«í¼ë³„ ê¸°ë³¸ê°’ ì‚¬ìš©
    import platform
    system = platform.system()
    home_dir = os.path.expanduser("~")
    if system == "Windows":
        ONEDRIVE_PATH = os.path.join(home_dir, "OneDrive", "ë°ì´í„°L", "selectstar")
    else:
        ONEDRIVE_PATH = os.path.join(home_dir, "Library", "CloudStorage", "OneDrive-ê°œì¸", "ë°ì´í„°L", "selectstar")

    directory_path = os.path.join(ONEDRIVE_PATH, 'evaluation', 'workbook_data', f'{cycle}C', 'Lv5')
    
    if not os.path.exists(directory_path):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory_path}")
        sys.exit(1)
    
    if not os.path.isdir(directory_path):
        print(f"âŒ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {directory_path}")
        sys.exit(1)
    
    check_real_duplicates(directory_path, remove_duplicates)
