{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "import tools.ProcessFiles as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yejin/Desktop/Desktop_AICenter✨/SFAIcenter/data_yejin/FIN_workbook/1C'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel = pf.get_excel_data(1, pf.BASE_PATH.replace('jinym', 'yejin'))\n",
    "json_files = pf.get_filelist(1, pf.FINAL_DATA_PATH.replace('jinym', 'yejin'))\n",
    "data_dir = '/Users/yejin/Desktop/Desktop_AICenter✨/SFAIcenter/data_yejin/FIN_workbook/1C'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel[excel['분류'] != 'Lv5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple, short, essay = [],[],[]\n",
    "origin = json.load(open(data_dir+'/merged_extracted_qna_domain.json', 'r', encoding='utf-8'))\n",
    "for qna in origin:\n",
    "    if len(qna['qna_data']['description']['answer']) > 0:\n",
    "        if qna.get('qna_type') == \"multiple-choice\":\n",
    "            multiple.append(qna)\n",
    "        elif qna.get('qna_type') == \"short-answer\":\n",
    "            short.append(qna)\n",
    "        elif qna.get('qna_type') == \"essay\":\n",
    "            essay.append(qna)\n",
    "# data_dir = os.path.join(base_dir,'Lv2', 're')\n",
    "# origin = json.load(open(os.path.join(base_dir, 'merged_qna_set.json'), 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2296, 8, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multiple), len(short), len(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Iterable, Set\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# -----------------------------\n",
    "# 0) 유틸: 텍스트 정규화\n",
    "# -----------------------------\n",
    "CIRCLED_MAP = {\"①\":\"1\",\"②\":\"2\",\"③\":\"3\",\"④\":\"4\",\"⑤\":\"5\"}\n",
    "\n",
    "def normalize_option_text(s: str) -> str:\n",
    "    \"\"\"선지 앞에 붙은 ①~⑤, 1), (1), 1. 등 번호 표기를 제거하고 본문만 남김.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    # ①~⑤ 제거\n",
    "    s = re.sub(r\"^\\s*[①-⑤]\\s*\", \"\", s)\n",
    "    # 1), (1), 1. 등 제거\n",
    "    s = re.sub(r\"^\\s*(?:\\(?([1-5])\\)?[.)])\\s*\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_answer_set(ans: str) -> Set[int]:\n",
    "    \"\"\"'①, ⑤' 같은 복수정답도 {1,5}로 파싱. 빈/이상값은 빈 set.\"\"\"\n",
    "    if not ans:\n",
    "        return set()\n",
    "    s = str(ans)\n",
    "    # ①~⑤ 를 1~5로 치환\n",
    "    for k, v in CIRCLED_MAP.items():\n",
    "        s = s.replace(k, v)\n",
    "    # 쉼표/슬래시/공백 구분 모두 허용하여 1~5 추출\n",
    "    nums = re.findall(r\"[1-5]\", s)\n",
    "    return set(int(n) for n in nums)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) JSON → df_all 변환\n",
    "# -----------------------------\n",
    "def json_to_df_all(json_list: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    입력 JSON(list[dict])을 파싱해 df_all 생성.\n",
    "    컬럼: book_id, tag, id, question, opt1..opt5, answer_set\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for item in json_list:\n",
    "        book_id = str(item.get(\"file_id\", \"\"))\n",
    "        qna = item.get(\"qna_data\", {}) or {}\n",
    "        tag  = qna.get(\"tag\", \"\")\n",
    "        desc = qna.get(\"description\", {}) or {}\n",
    "        q    = (desc.get(\"question\") or \"\").strip()\n",
    "        opts = desc.get(\"options\") or []\n",
    "        # 5지선다 기준으로 빈칸 보정\n",
    "        opts = list(opts)[:5] + [\"\"] * max(0, 5 - len(opts))\n",
    "        opts = [normalize_option_text(x) for x in opts]\n",
    "        ans_set = parse_answer_set(desc.get(\"answer\", \"\"))\n",
    "\n",
    "        rows.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"tag\": tag,\n",
    "            \"id\": f\"{book_id}_{tag}\",\n",
    "            \"question\": q,\n",
    "            \"opt1\": opts[0], \"opt2\": opts[1], \"opt3\": opts[2], \"opt4\": opts[3], \"opt5\": opts[4],\n",
    "            \"answer_set\": ans_set\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # 혹시 id 중복이 있으면 마지막 것 유지(필요시 정책 변경)\n",
    "    df = df.drop_duplicates(\"id\", keep=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 배치 사용자 프롬프트 생성 (50문제)\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = \"\"\"당신은 금융전문가이자 객관식 문제 풀이 전문가입니다.\n",
    "여러 금융 객관식 문제에 대해, 각 문제의 정답 \"번호만\" 하나 선택합니다.\n",
    "\n",
    "규칙\n",
    "- 각 문제는 고유 ID와 함께 제시됩니다.\n",
    "- 출력은 반드시 한 줄당 \"ID<TAB>번호\" 형식으로만 합니다. (예: 9791166791123_q_0377_0001<TAB>3)\n",
    "- 다른 글자, 마크다운, 이유, 기호는 절대 출력하지 않습니다.\n",
    "- 모든 문제는 보기(1~5) 중 하나만 고릅니다.\n",
    "- 출력 줄 수는 입력 문제 개수와 동일해야 합니다.\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(batch_df: pd.DataFrame) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"다음은 금융 객관식 문제들입니다. 각 문제에 대해 정답 번호만 고르세요.\\n\")\n",
    "    lines.append(\"문제들\")\n",
    "    for _, r in batch_df.iterrows():\n",
    "        lines.append(f\"ID: {r['id']}\")\n",
    "        lines.append(f\"Q: {r['question']}\")\n",
    "        lines.append(f\"1) {r['opt1']}\")\n",
    "        lines.append(f\"2) {r['opt2']}\")\n",
    "        lines.append(f\"3) {r['opt3']}\")\n",
    "        lines.append(f\"4) {r['opt4']}\")\n",
    "        lines.append(f\"5) {r['opt5']}\\n\")\n",
    "    lines.append(\"출력 형식(중요)\")\n",
    "    for _, r in batch_df.iterrows():\n",
    "        lines.append(f\"{r['id']}\\\\t{{번호}}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LLM 호출 추상화 (모의/실제)\n",
    "# -----------------------------\n",
    "def call_llm(model_name: str, system_prompt: str, user_prompt: str, mock_mode: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    - mock_mode=True면 임의 번호(1~5)를 생성해 파이프라인 검증용 출력 반환.\n",
    "    - 실제 환경에서는 이 함수를 OpenAI/사내엔진 호출로 교체.\n",
    "    \"\"\"\n",
    "    if mock_mode:\n",
    "        # 입력 user_prompt에서 ID 목록 회수\n",
    "        ids = [ln.split(\"\\t\")[0] for ln in user_prompt.splitlines() if \"\\t{번호}\" in ln]\n",
    "        # 무작위 예측(1~5)\n",
    "        rng = np.random.default_rng(42)\n",
    "        preds = rng.integers(1, 6, size=len(ids))\n",
    "        return \"\\n\".join(f\"{_id}\\t{int(a)}\" for _id, a in zip(ids, preds))\n",
    "        raise NotImplementedError(\"call_llm를 실제 API로 연결하세요 (mock_mode=True로 파이프라인 검증 가능).\")\n",
    "\n",
    "    # ---- 실제 호출 예시 (의사코드)\n",
    "    # from openai import OpenAI\n",
    "    # client = OpenAI()\n",
    "    # resp = client.chat.completions.create(\n",
    "    #     model=model_name,\n",
    "    #     temperature=0,\n",
    "    #     top_p=1,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": system_prompt},\n",
    "    #         {\"role\": \"user\", \"content\": user_prompt},\n",
    "    #     ],\n",
    "    # )\n",
    "    # return resp.choices[0].message.content\n",
    "    else:\n",
    "        import tools.Openrouter as Openrouter\n",
    "        ans = Openrouter.query_model_openrouter(system_prompt, user_prompt, model_name)\n",
    "        return ans\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 모델 출력 파싱 (ID<TAB>번호)\n",
    "# -----------------------------\n",
    "def parse_model_output(raw: str, expected_ids: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    모델 원시 출력(raw)을 {id: answer(1~5)}로 변환.\n",
    "    - 'ID\\\\t번호' 포맷 기준\n",
    "    - 잘못된 줄/누락 줄은 NaN 처리\n",
    "    \"\"\"\n",
    "    id_set = set(expected_ids)\n",
    "    out: Dict[str, float] = {k: np.nan for k in expected_ids}\n",
    "\n",
    "    for ln in (raw or \"\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or \"\\t\" not in ln:\n",
    "            continue\n",
    "        left, right = ln.split(\"\\t\", 1)\n",
    "        _id = left.strip()\n",
    "        if _id not in id_set:\n",
    "            continue\n",
    "        # 첫 번째 1~5 추출\n",
    "        m = re.search(r\"[1-5]\", right)\n",
    "        if m:\n",
    "            out[_id] = float(m.group(0))\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 파이프라인: 300 샘플 → 50개씩 × 모델 호출 → DF 정리/정확도\n",
    "# -----------------------------\n",
    "def run_eval_pipeline(\n",
    "    json_list: List[dict],\n",
    "    models: List[str],\n",
    "    sample_size: int = 300,\n",
    "    batch_size: int = 50,\n",
    "    seed: int = 42,\n",
    "    mock_mode: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      df_all      : 전체 원장 (정규화 선지 + answer_set)\n",
    "      pred_long   : (id, model_name, answer) 롱 포맷\n",
    "      pred_wide   : id 기준 모델별 예측 와이드\n",
    "      acc_by_model: 모델별 정확도 (복수정답 지원: 예측 ∈ answer_set 이면 정답)\n",
    "    \"\"\"\n",
    "    # (1) JSON → df_all\n",
    "    df_all = json_to_df_all(json_list)\n",
    "    df_all = df_all.sort_values(by=['book_id', 'tag'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # (2) 300개 샘플 (재현성 유지)\n",
    "    df_sample = df_all.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # (3) 50문제 배치 분할\n",
    "    batches = [df_sample.iloc[i:i+batch_size] for i in range(0, len(df_sample), batch_size)]\n",
    "\n",
    "    # (4) 모델 호출/파싱 누적\n",
    "    rows = []\n",
    "    for bidx, bdf in enumerate(batches, 1):\n",
    "        user_prompt = build_user_prompt(bdf)\n",
    "        ids = bdf[\"id\"].tolist()\n",
    "        for model in models:\n",
    "            raw = call_llm(model, SYSTEM_PROMPT, user_prompt, mock_mode=mock_mode)\n",
    "            parsed = parse_model_output(raw, ids)\n",
    "            for _id in ids:\n",
    "                rows.append({\"id\": _id, \"model_name\": model, \"answer\": parsed[_id]})\n",
    "\n",
    "    pred_long = pd.DataFrame(rows)\n",
    "    pred_long = pred_long.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "    # (5) 와이드 포맷\n",
    "    pred_wide = pred_long.pivot(index=\"id\", columns=\"model_name\", values=\"answer\").reset_index()\n",
    "    pred_wide = pred_wide.sort_values(by=['id'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # (6) 정확도 (복수정답 집합 매칭)\n",
    "    key = df_sample[[\"id\", \"answer_set\"]].copy()\n",
    "    # 정답 집합이 비어있으면 채점 제외 (NaN)\n",
    "    def _is_correct(pred: float, s: Set[int]) -> float:\n",
    "        if np.isnan(pred) or not s:\n",
    "            return np.nan\n",
    "        return float(int(pred) in s)\n",
    "\n",
    "    merged = pred_long.merge(key, on=\"id\", how=\"left\")\n",
    "    merged[\"correct\"] = merged.apply(lambda r: _is_correct(r[\"answer\"], r[\"answer_set\"]), axis=1)\n",
    "\n",
    "    acc_by_model = (\n",
    "        merged.groupby(\"model_name\", dropna=False)[\"correct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"correct\": \"accuracy\"})\n",
    "        .sort_values(\"accuracy\", ascending=False)\n",
    "    )\n",
    "    acc_by_model = acc_by_model.sort_values(\"accuracy\", ascending=False)\n",
    "\n",
    "    return df_all, pred_long, pred_wide, acc_by_model\n",
    "\n",
    "# -----------------------------\n",
    "# 6) 사용 예시\n",
    "# -----------------------------\n",
    "# data = [...]  # 네가 제공한 JSON 리스트\n",
    "data = multiple\n",
    "models = ['anthropic/claude-sonnet-4', 'google/gemini-2.5-flash', 'x-ai/grok-4-fast', 'deepseek/deepseek-chat-v3-0324', 'openai/gpt-4.1', 'openai/gpt-oss-120b', 'openai/gpt-5', 'meta-llama/llama-3.3-70b-instruct']  # 실제 모델명\n",
    "# models = ['x-ai/grok-4-fast:free']\n",
    "df_all, pred_long, pred_wide, acc = run_eval_pipeline(\n",
    "    data, models, sample_size=300, batch_size=50, seed=42, mock_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model_name  accuracy\n",
      "2            google/gemini-2.5-flash  0.753333\n",
      "0          anthropic/claude-sonnet-4  0.750000\n",
      "4                     openai/gpt-4.1  0.683333\n",
      "1     deepseek/deepseek-chat-v3-0324  0.673575\n",
      "7                   x-ai/grok-4-fast  0.670000\n",
      "3  meta-llama/llama-3.3-70b-instruct  0.533333\n",
      "5                       openai/gpt-5       NaN\n",
      "6                openai/gpt-oss-120b       NaN\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(\"evaluation_results.xlsx\", engine=\"openpyxl\") as w:\n",
    "    df_all.to_excel(w, index=False, sheet_name=\"final_report\")   # 통합 뷰\n",
    "    pred_wide.to_excel(w, index=False, sheet_name=\"pred_wide\")         # 모델별 예측(가로)\n",
    "    acc.to_excel(w, index=False, sheet_name=\"accuracy\")       # 모델별 정확도 요약\n",
    "\n",
    "print(acc)\n",
    "# final_report 시트만 보면 거의 끝납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
