#!/usr/bin/env python3
"""
ë°±ì—… JSON íŒŒì¼ì˜ page_contentsë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
ì œnníšŒ, ë²ˆí˜¸, ì§ˆë¬¸, ì˜µì…˜, í•´ì„¤ì„ ë¶„ë¦¬í•˜ì—¬ add_info í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import json
import re
import os
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime


class PageContentParser:
    def __init__(self):
        """í˜ì´ì§€ ë‚´ìš© íŒŒì„œ ì´ˆê¸°í™”"""
        # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ë“¤
        self.session_pattern = r'ì œ(\d+)íšŒ'
        self.question_number_pattern = r'^(\d{2})\s+(.+?)(?=\nâ‘ |\ní•´ì„¤|$)'
        self.option_pattern = r'[â‘ â‘¡â‘¢â‘£â‘¤]\s+(.+?)(?=\n[â‘ â‘¡â‘¢â‘£â‘¤]|\ní•´ì„¤|$)'
        self.explanation_pattern = r'í•´ì„¤\s*([â‘ â‘¡â‘¢â‘£â‘¤]?)\s*(.+?)(?=\nì œ\d+íšŒ|\n\d{2}\s|$)'
        
    def extract_session_info(self, text: str) -> List[str]:
        """ì œnníšŒ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        sessions = re.findall(self.session_pattern, text)
        return sessions
    
    def split_by_sessions(self, text: str) -> List[Dict[str, str]]:
        """í…ìŠ¤íŠ¸ë¥¼ íšŒì°¨ë³„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
        # ì œnníšŒ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
        parts = re.split(r'(ì œ\d+íšŒ)', text)
        
        sessions = []
        current_session = None
        
        for i, part in enumerate(parts):
            if re.match(r'ì œ\d+íšŒ', part):
                if current_session:
                    sessions.append(current_session)
                current_session = {
                    'session': part,
                    'content': ''
                }
            elif current_session:
                current_session['content'] += part
        
        if current_session:
            sessions.append(current_session)
            
        return sessions
    
    def parse_question_content(self, content: str) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        questions = []
        
        # ì§ˆë¬¸ ë²ˆí˜¸ì™€ ë‚´ìš© ì¶”ì¶œ (ë” ìœ ì—°í•œ íŒ¨í„´ ì‚¬ìš©)
        question_matches = re.finditer(r'^(\d{1,2})\s+(.+?)(?=\nâ‘ |\ní•´ì„¤|$)', content, re.MULTILINE | re.DOTALL)
        
        for match in question_matches:
            question_num = match.group(1)
            question_text = match.group(2).strip()
            
            # í•´ë‹¹ ì§ˆë¬¸ ì´í›„ì˜ ë‚´ìš© ì¶”ì¶œ
            start_pos = match.end()
            next_question_match = re.search(r'^\d{1,2}\s+', content[start_pos:], re.MULTILINE)
            end_pos = start_pos + next_question_match.start() if next_question_match else len(content)
            
            question_content = content[start_pos:end_pos].strip()
            
            # í•´ì„¤ ì¶”ì¶œ (ë” ì •í™•í•œ íŒ¨í„´ ì‚¬ìš©)
            explanation_match = re.search(r'í•´ì„¤\s*([â‘ â‘¡â‘¢â‘£â‘¤]?)\s*(.+?)(?=\nì œ\d+íšŒ|\n\d{1,2}\s|$)', question_content, re.DOTALL)
            
            answer = ""
            explanation = ""
            if explanation_match:
                # answer = explanation_match.group(1).strip()
                explanation = explanation_match.group(1).strip() + " " + explanation_match.group(2).strip()
            
            # ì˜µì…˜ì—ì„œ í•´ì„¤ ë¶€ë¶„ ì œê±°
            options_text = re.sub(r'\ní•´ì„¤.*$', '', question_content, flags=re.DOTALL)
            options = self.extract_options(options_text)
            
            # ì§ˆë¬¸ì´ ìˆê³  ì˜µì…˜ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            if question_text and options:
                questions.append({
                    'number': question_num,
                    'question': question_text,
                    'options': options,
                    'answer': answer,
                    'explanation': explanation
                })
        
        return questions
    
    def extract_options(self, text: str) -> List[str]:
        """ì˜µì…˜ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        options = []
        option_matches = re.finditer(r'([â‘ â‘¡â‘¢â‘£â‘¤]\s+.+?)(?=\n[â‘ â‘¡â‘¢â‘£â‘¤]|\ní•´ì„¤|$)', text, re.DOTALL)
        
        for match in option_matches:
            option_text = match.group(1).strip()
            # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
            option_text = re.sub(r'\s+', ' ', option_text)
            options.append(option_text)
        
        return options
    
    def parse_page_content(self, page_content: str, page_num: str) -> Tuple[List[Dict[str, Any]], str]:
        """í˜ì´ì§€ ë‚´ìš©ì„ ì „ì²´ì ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
        add_info_items = []
        new_page_contents = ""
        
        # ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ íŠ¹ìˆ˜í•œ ê²½ìš° ì²˜ë¦¬ (15íšŒ ë³€)
        if "15íšŒ ë³€" in page_content and "CHAPTER" in page_content:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±
            lines = page_content.split('\n')
            question_text = ""
            options = []
            answer = ""
            explanation = ""
            
            for i, line in enumerate(lines):
                if line.strip().startswith('01 '):
                    question_text = line.strip()[3:]  # "01 " ì œê±°
                    # ë‹¤ìŒ ì¤„ë“¤ì—ì„œ ì˜µì…˜ ì¶”ì¶œ
                    for j in range(i+1, len(lines)):
                        option_line = lines[j].strip()
                        if option_line.startswith('â‘ '):
                            options.append(option_line)  # ë²ˆí˜¸ í¬í•¨
                        elif option_line.startswith('â‘¡'):
                            options.append(option_line)  # ë²ˆí˜¸ í¬í•¨
                        elif option_line.startswith('â‘¢'):
                            options.append(option_line)  # ë²ˆí˜¸ í¬í•¨
                        elif option_line.startswith('â‘£'):
                            options.append(option_line)  # ë²ˆí˜¸ í¬í•¨
                        elif option_line.startswith('â‘¤'):
                            options.append(option_line)  # ë²ˆí˜¸ í¬í•¨
                        elif option_line.startswith('í•´ì„¤'):
                            explanation_text = option_line[2:]  # "í•´ì„¤ " ì œê±°
                            # ë‹µ ì¶”ì¶œ
                            # answer_match = re.search(r'([â‘ â‘¡â‘¢â‘£â‘¤])', explanation_text)
                            # if answer_match:
                            #     answer = answer_match.group(1)
                            explanation = explanation_text
                            break
            
            if question_text and options:
                tag = f"q_{page_num}_0001"
                add_info_item = {
                    "tag": tag,
                    "type": "question",
                    "description": {
                        "number": "01",
                        "question": question_text,
                        "options": options,
                        "answer": answer,
                        "explanation": explanation
                    },
                    "caption": ["15íšŒ ë³€"],
                    "file_path": None,
                    "bbox": None
                }
                add_info_items.append(add_info_item)
                
                # page_contentsë¥¼ íƒœê·¸ í˜•íƒœë¡œ ë³€í™˜
                new_page_contents = f"CHAPTER. 01. ì´ì„¤\n{{{tag}}}"
        
        # íšŒì°¨ë³„ë¡œ ë¶„ë¦¬
        sessions = self.split_by_sessions(page_content)
        
        if not new_page_contents:  # ì²« ë²ˆì§¸ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°
            tag_parts = []
        
        for session in sessions:
            session_name = session['session']
            content = session['content']
            
            # ì§ˆë¬¸ë“¤ íŒŒì‹±
            questions = self.parse_question_content(content)
            
            for i, question in enumerate(questions):
                tag = f"q_{page_num}_{len(add_info_items)+i+1:04d}"
                
                add_info_item = {
                    "tag": tag,
                    "type": "question",
                    "description": {
                        "number": question['number'],
                        "question": question['question'],
                        "options": question['options'],
                        "answer": question['answer'],
                        "explanation": question['explanation']
                    },
                    "caption": [session_name],
                    "file_path": None,
                    "bbox": None
                }
                
                add_info_items.append(add_info_item)
                
                # page_contentsì— íƒœê·¸ ì¶”ê°€
                if not new_page_contents:  # ì²« ë²ˆì§¸ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°
                    tag_parts.append(f"{{{tag}}}")
        
        # page_contents ìƒì„±
        if not new_page_contents and tag_parts:
            new_page_contents = "\n".join(tag_parts)
        
        return add_info_items, new_page_contents


class JSONRefinementProcessor:
    def __init__(self, backup_file_path: str):
        """
        JSON ì •ì œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            backup_file_path: ë°±ì—… JSON íŒŒì¼ ê²½ë¡œ
        """
        self.backup_file_path = backup_file_path
        self.parser = PageContentParser()
        self.data = None
        
    def load_backup_file(self) -> Dict[str, Any]:
        """ë°±ì—… íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(self.backup_file_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"âœ“ ë°±ì—… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {os.path.basename(self.backup_file_path)}")
            return self.data
        except Exception as e:
            print(f"âœ— ë°±ì—… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def process_all_pages(self) -> Dict[str, Any]:
        """ëª¨ë“  í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not self.data:
            return None
        
        processed_data = self.data.copy()
        contents = processed_data.get('contents', [])
        
        # ë©”íƒ€ë°ì´í„° ì •ë¦¬
        processed_data['file_id'] = 'SS0267'  # ì›ë³¸ íŒŒì¼ IDë¡œ ë³€ê²½
        processed_data['ISBN'] = '9791198804068'  # ISBN ì¶”ê°€
        processed_data['cat3_specific'] = 'ê°ì •í‰ê°€ì‚¬'  # ë” êµ¬ì²´ì ì¸ ë¶„ë¥˜ë¡œ ë³€ê²½
        
        total_questions = 0
        processed_pages = 0
        
        print(f"\nğŸ” ì´ {len(contents)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘...")
        
        for i, page in enumerate(contents):
            page_content = page.get('page_contents', '')
            page_num = page.get('page', f'{i:04d}')
            
            if page_content.strip():
                try:
                    # í˜ì´ì§€ ë‚´ìš© íŒŒì‹±
                    add_info_items, new_page_contents = self.parser.parse_page_content(page_content, page_num)
                    
                    if add_info_items:
                        page['add_info'] = add_info_items
                        page['page_contents'] = new_page_contents  # ìƒˆë¡œìš´ page_contentsë¡œ êµì²´
                        total_questions += len(add_info_items)
                        processed_pages += 1
                        
                        if processed_pages <= 5:  # ì²˜ìŒ 5ê°œ í˜ì´ì§€ë§Œ ë¡œê·¸ ì¶œë ¥
                            print(f"  í˜ì´ì§€ {page_num}: {len(add_info_items)}ê°œ ì§ˆë¬¸ ì¶”ì¶œ")
                    
                except Exception as e:
                    print(f"  âœ— í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  - ì²˜ë¦¬ëœ í˜ì´ì§€: {processed_pages}ê°œ")
        print(f"  - ì¶”ì¶œëœ ì§ˆë¬¸: {total_questions}ê°œ")
        
        return processed_data
    
    def save_refined_data(self, refined_data: Dict[str, Any], output_path: str = None) -> str:
        """ì •ì œëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        if not refined_data:
            print("âœ— ì •ì œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"SS0267_refined_from_backup_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(refined_data, f, ensure_ascii=False, indent=4)
            print(f"âœ“ ì •ì œëœ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            return output_path
        except Exception as e:
            print(f"âœ— íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def generate_processing_report(self, refined_data: Dict[str, Any], output_path: str = None) -> str:
        """ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"processing_report_{timestamp}.txt"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("ë°±ì—… íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œ\n")
                f.write("=" * 50 + "\n\n")
                
                contents = refined_data.get('contents', [])
                total_questions = 0
                pages_with_questions = 0
                
                f.write("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n")
                f.write("-" * 20 + "\n")
                
                for page in contents:
                    add_info = page.get('add_info', [])
                    if add_info:
                        pages_with_questions += 1
                        total_questions += len(add_info)
                
                f.write(f"ì´ í˜ì´ì§€ ìˆ˜: {len(contents)}\n")
                f.write(f"ì§ˆë¬¸ì´ ìˆëŠ” í˜ì´ì§€ ìˆ˜: {pages_with_questions}\n")
                f.write(f"ì´ ì¶”ì¶œëœ ì§ˆë¬¸ ìˆ˜: {total_questions}\n\n")
                
                # í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´ (ì²˜ìŒ 10ê°œë§Œ)
                f.write("ğŸ“„ í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´ (ì²˜ìŒ 10ê°œ)\n")
                f.write("-" * 30 + "\n")
                
                count = 0
                for page in contents:
                    if count >= 10:
                        break
                    
                    add_info = page.get('add_info', [])
                    if add_info:
                        f.write(f"í˜ì´ì§€ {page.get('page', 'N/A')}:\n")
                        for item in add_info:
                            desc = item.get('description', {})
                            f.write(f"  - {desc.get('number', 'N/A')}ë²ˆ: {desc.get('question', 'N/A')[:50]}...\n")
                            f.write(f"    ë‹µ: {desc.get('answer', 'N/A')}, ì˜µì…˜ ìˆ˜: {len(desc.get('options', []))}\n")
                        f.write("\n")
                        count += 1
            
            print(f"âœ“ ì²˜ë¦¬ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path
        except Exception as e:
            print(f"âœ— ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë°±ì—… íŒŒì¼ ê²½ë¡œ ì„¤ì •
    backup_file_path = "/Users/jinym/Desktop/Desktop_AICenterâœ¨/SFAIcenter/data/FINAL/2C/Lv3_4/SS0267_workbook/SS0267.json.bak"
    
    print("ë°±ì—… íŒŒì¼ í˜ì´ì§€ ë‚´ìš© íŒŒì‹± ë° ì •ì œ ë„êµ¬")
    print("=" * 50)
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = JSONRefinementProcessor(backup_file_path)
    
    # ë°±ì—… íŒŒì¼ ë¡œë“œ
    data = processor.load_backup_file()
    if not data:
        print("âœ— ë°±ì—… íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬
    print("\nğŸ”§ í˜ì´ì§€ ë‚´ìš© íŒŒì‹± ë° ì •ì œ ì¤‘...")
    refined_data = processor.process_all_pages()
    
    if not refined_data:
        print("âœ— ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ì •ì œëœ ë°ì´í„° ì €ì¥
    print("\nğŸ’¾ ì •ì œëœ ë°ì´í„° ì €ì¥ ì¤‘...")
    output_path = processor.save_refined_data(refined_data)
    
    # ì²˜ë¦¬ ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“ ì²˜ë¦¬ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report_path = processor.generate_processing_report(refined_data)
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  - {output_path}")
    print(f"  - {report_path}")


if __name__ == "__main__":
    main()
