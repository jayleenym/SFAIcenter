{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "            { \n",
    "                \"tag\": \"q_0000_0001\", \n",
    "                \"type\": \"question\", \n",
    "                \"description\": { \n",
    "                    \"number\": \"\", \n",
    "                    \"question\": \"\", \n",
    "                    \"options\": [], \n",
    "                    \"options\": null, \n",
    "                    \"answer\": \"\", \n",
    "                    \"explanation\": \"\" \n",
    "                    }, \n",
    "                \"caption\": [ \n",
    "                    \"\" \n",
    "                    ], \n",
    "                \"file_path\": null, \n",
    "                \"bbox\": null \n",
    "            }, \n",
    "{ \n",
    "    \"tag\": \"note_0000_0001\", \n",
    "    \"type\": \"footnote\", \n",
    "    \"description\": \"1 이러한 계약을 법적으로는 금전소비대차계약이라고 한다. 차용증서를 영어로는 I owe you.의 소리를 따라 IOU 라고 한다.\", \n",
    "    \"caption\": null, \n",
    "    \"file_path\": null, \n",
    "    \"bbox\": null \n",
    "}, \n",
    "{ \n",
    "    \"tag\": \"tb_0000_0001\", \n",
    "    \"type\": \"table\", \n",
    "    \"description\": \"\", \n",
    "    \"caption\": [], \n",
    "    \"file_path\": \"??????/crop/tb_0000_0001.png\", \n",
    "    \"bbox\": null \n",
    "}, \n",
    "{ \n",
    "    \"tag\": \"img_0000_0001\", \n",
    "    \"type\": \"image\", \n",
    "    \"description\": null, \n",
    "    \"caption\": [], \n",
    "    \"file_path\": \"?????/crop/img_0000_0001.png\", \n",
    "    \"bbox\": null \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도서 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.ProcessFiles as pf\n",
    "import pandas as pd\n",
    "excel = pd.concat([pf.get_excel_data(1), pf.get_excel_data(2), pf.get_excel_data(3)]) # i차 분석\n",
    "excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lv2 파일명 변경, Lv3/4/5 폴더 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "analysis = {1:'1차 분석', 2:'2차 분석', 3: '3차 분석'}\n",
    "buy = {1:'1차 구매', 2:'2차 구매', 3: '3차 구매'}\n",
    "i = 1\n",
    "\n",
    "excel_analy = pd.read_excel(os.path.join(base_path, 'book_list_ALL.xlsx'), sheet_name=analysis[i], header=3)[['관리번호_old', '관리번호', 'ISBN', '도서명', '분류']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "zip_path = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/zip/250826_1차_원천코퍼스_보완사항반영\"\n",
    "original_path = f\"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/{i}C\"\n",
    "\n",
    "error_list = []\n",
    "\n",
    "for fn, tn, isbn, lv in tqdm(zip(excel_analy['관리번호_old'], excel_analy['관리번호'], excel_analy['ISBN'], excel_analy['분류'])):\n",
    "   fn = str(fn)\n",
    "   if '/' in lv: \n",
    "      lv = lv.replace(\"/\", '_')\n",
    "   \n",
    "   if 'Lv' not in lv:\n",
    "      continue\n",
    "\n",
    "   # os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/1C/Lv2 && mv {isbn}.json {tn}.json\")\n",
    "   if lv == 'Lv2':\n",
    "      # os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/1C/Lv2 && mkdir {tn} && mv {tn}.json {tn}/\")\n",
    "      continue\n",
    "   else:\n",
    "      os.system(f\"cp {zip_path}/{fn}.zip {original_path}/{lv}/{tn}.zip && cd {original_path}/{lv} && mkdir {tn} && unzip {tn}.zip -d {tn} && find . -type f -name '*{fn}*'| sed -e 'p' -e 's/{fn}/{tn}/g' | xargs -n 2 mv\")\n",
    "\n",
    "      file_path = f\"{original_path}/{lv}/{tn}/{tn}.json\"\n",
    "      try:\n",
    "         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "               content = f.read()\n",
    "         \n",
    "         # Replace the old filename with new filename\n",
    "         updated_content = content.replace(str(fn), str(tn))\n",
    "         \n",
    "         with open(file_path, 'w', encoding='utf-8') as f:\n",
    "               f.write(updated_content)\n",
    "               \n",
    "         print(f\"Updated {fn} to {tn} in {file_path}\")\n",
    "      except Exception as e:\n",
    "         print(f\"Error processing {fn} to {tn}: {e}\")\n",
    "         error_list.append((fn, tn, lv))\n",
    "         # 이름 바꾸기\n",
    "         # os.system(f\"cd {original_path}/{lv} && mkdir {tn} && unzip {tn}.zip -d {tn} && find . -type f -name '*{fn}*'| sed -e 'p' -e 's/{fn}/{tn}/g' | xargs -n 2 mv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "zip_path = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/zip/250826_1차_원천코퍼스_보완사항반영\"\n",
    "original_path = f\"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/{i}C\"\n",
    "\n",
    "error_list = []\n",
    "\n",
    "for fn, tn, isbn, lv in tqdm(zip(excel_analy['관리번호_old'], excel_analy['관리번호'], excel_analy['ISBN'], excel_analy['분류'])):\n",
    "   fn = str(fn)\n",
    "   if '/' in lv: \n",
    "      lv = lv.replace(\"/\", '_')\n",
    "   \n",
    "   if 'Lv' not in lv:\n",
    "      continue\n",
    "\n",
    "   # os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/1C/Lv2 && mv {isbn}.json {tn}.json\")\n",
    "   if lv == 'Lv2':\n",
    "      # os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/1C/Lv2 && mkdir {tn} && mv {tn}.json {tn}/\")\n",
    "      continue\n",
    "   else:\n",
    "    #   os.system(f\"cp {zip_path}/{fn}.zip {original_path}/{lv}/{tn}.zip && cd {original_path}/{lv} && mkdir {tn} && unzip {tn}.zip -d {tn} && find . -type f -name '*{fn}*'| sed -e 'p' -e 's/{fn}/{tn}/g' | xargs -n 2 mv\")\n",
    "      os.system(f\"cd {original_path}/{lv}/{tn} && find . -type f -name '*{fn}*'| sed -e 'p' -e 's/{fn}/{tn}/g' | xargs -n 2 mv\")\n",
    "\n",
    "      file_path = f\"{original_path}/{lv}/{tn}/{tn}.json\"\n",
    "      try:\n",
    "         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "               content = f.read()\n",
    "         \n",
    "         # Replace the old filename with new filename\n",
    "         updated_content = content.replace(str(fn), str(tn))\n",
    "         \n",
    "         with open(file_path, 'w', encoding='utf-8') as f:\n",
    "               f.write(updated_content)\n",
    "               \n",
    "         print(f\"Updated {fn} to {tn} in {file_path}\")\n",
    "      except Exception as e:\n",
    "         print(f\"Error processing {fn} to {tn}: {e}\")\n",
    "         error_list.append((fn, tn, lv))\n",
    "         # 이름 바꾸기\n",
    "         # os.system(f\"cd {original_path}/{lv} && mkdir {tn} && unzip {tn}.zip -d {tn} && find . -type f -name '*{fn}*'| sed -e 'p' -e 's/{fn}/{tn}/g' | xargs -n 2 mv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from_path = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/evaluation/eval_data/mock_exam/test\" \n",
    "from_path = '/Users/jinym/Desktop/Desktop_AICenter✨/SFAIcenter/tools/QueryModels.py'\n",
    "to_path = \"yjmoon@oci004:/home/yjmoon/src\"\n",
    "\n",
    "\n",
    "# 업로드\n",
    "os.system(f\"scp -r {from_path} {to_path}\")\n",
    "# for ss in file:\n",
    "    # print(ss)\n",
    "    # os.system(f\"scp -r /Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/3C/Lv3_4/{ss} yjmoon@oci004:/home/yjmoon/FINAL/{ss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn, tn, lv in zip(excel_analy['관리번호'], excel_analy['관리번호_new'], excel_analy['분류']):\n",
    "    fn = str(fn)\n",
    "    if '/' in lv: \n",
    "       lv = lv.replace(\"/\", '_')\n",
    "#     print(fn, tn, lv)\n",
    "    \n",
    "    dir = os.path.join(pf.FINAL_DATA_PATH, pf.CYCLE_PATH[1], lv)\n",
    "    os.system(f'cd {dir} && mv {fn}_low {tn}_low && cd {tn} &&'+f' for file in *{fn}*; do mv \"$file\" \"$'+'{file//'+fn+'/'+tn+'}\"; done')\n",
    "    print(os.path.join(dir, fn))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.ProcessFiles as pf\n",
    "\n",
    "pf.move_jsons(3, pf.ORIGINAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 개별 처리\n",
    "- 불필요 페이지 제거\n",
    "- 오타\n",
    "- 페이지 머리말/꼬리말"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from typing import List, Dict, Any\n",
    "import tools.ProcessFiles as pf\n",
    "import tools.ProcessLv2 as pl2\n",
    "\n",
    "json_files = pf.get_filelist(1)\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, json, shutil\n",
    "# import tools.ProcessFiles as pf\n",
    "\n",
    "# i = 3\n",
    "# # 폴더 만들어 옮기기\n",
    "# excel = pf.get_excel_data(i)\n",
    "# FINAL_DATA_PATH = os.path.join(\"/Users/jinym/Desktop/Desktop_AICenter✨/SFAIcenter\", '3C')\n",
    "# ORIGINAL_DATA_PATH = os.path.join(\"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/\", '3C')\n",
    "\n",
    "# json_files = pf.get_filelist(i)\n",
    "\n",
    "# for j in json_files:\n",
    "#     new_path = j.replace(ORIGINAL_DATA_PATH, FINAL_DATA_PATH)\n",
    "#     try:\n",
    "#         shutil.copy(j, new_path)\n",
    "#     except FileNotFoundError:\n",
    "#         os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "#         shutil.copy(j, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tools.ProcessLv2 as pl2\n",
    "\n",
    "# for j in json_files:\n",
    "if True:\n",
    "    j = '/Users/jinym/Desktop/Desktop_AICenter✨/SFAIcenter/data/FINAL/2C/Lv2/SS0017_low/SS0017.json'\n",
    "    INPUT_PATH = j\n",
    "    BACKUP_PATH = INPUT_PATH + \".bak\"\n",
    "\n",
    "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        origin = json.load(f)\n",
    "\n",
    "    with open(BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(origin, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # id = excel[excel['ISBN'] == int(origin.get('file_id'))].index[0]\n",
    "    # break\n",
    "    # id = os.path.splitext(os.path.basename(j))[0]\n",
    "    # try:\n",
    "    #     new = {\n",
    "    #         'file_id': id,\n",
    "    #         'ISBN': str(excel.loc[id, 'ISBN']),\n",
    "    #         'title': excel.loc[id, '도서명'],\n",
    "    #         'cat1_domain': excel.loc[id, '코퍼스 1분류'],\n",
    "    #         'cat2_sub': excel.loc[id, '코퍼스 2분류'],\n",
    "    #         'cat3_specific': excel.loc[id, '비고'],\n",
    "    #         'pub_date': str(excel.loc[id, '출판일'])[:10],\n",
    "    #         # 'contents': [],\n",
    "    #         'contents': origin['contents']\n",
    "    #     }\n",
    "    # except:\n",
    "    #     new = origin\n",
    "    \n",
    "    new = pl2.fill_chapter(origin)\n",
    "    # new = pl2.merge_paragraphs(origin)\n",
    "    # new = pl2.erase_page(origin, 1)\n",
    "    # new = pl2.format_change(3,origin)\n",
    "    # new = origin\n",
    "\n",
    "    # for i in range(len(new['contents'])):\n",
    "        # contents = new['contents'][i]\n",
    "        # # c = extract_qna(contents)\n",
    "        # page_contents = pl2.remove_enter(contents['page_contents'])\n",
    "        # contents['page_contents'] = page_contents\n",
    "        # # c = contents\n",
    "\n",
    "    with open(INPUT_PATH.replace(\".json\", \".json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    # with open(INPUT_PATH.replace(\".json\", \"_new.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lv2 pdf_page 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "analysis = {1:'1차 분석', 2:'2차 분석', 3: '3차 분석'}\n",
    "buy = {1:'1차 구매', 2:'2차 구매', 3: '3차 구매'}\n",
    "i = 3\n",
    "\n",
    "excel_analy = pd.read_excel(os.path.join(base_path, 'book_list_ALL.xlsx'), sheet_name=analysis[i], header=3, engine='openpyxl')[['관리번호', 'ISBN', '도서명', '분류']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = excel_analy[excel_analy['관리번호'] == tn]['분류'].tolist()[0]\n",
    "lv\n",
    "# print(lv == 'Lv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version - using Python file operations instead of sed\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import fitz\n",
    "\n",
    "dpi = 300\n",
    "mat = fitz.Matrix(dpi / 150, dpi / 150)\n",
    "\n",
    "# for tn, isbn, lv in tqdm(zip(excel_analy['관리번호'], excel_analy['ISBN'], excel_analy['분류'])):\n",
    "for tn in tqdm(excel_analy['관리번호']):\n",
    "   # if '/' in lv: \n",
    "   #    lv = lv.replace(\"/\", '_')\n",
    "   \n",
    "   # if 'Lv' not in lv:\n",
    "   #    continue\n",
    "   lv = excel_analy[excel_analy['관리번호'] == tn]['분류'].tolist()[0]\n",
    "   # os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/1C/Lv2 && mv {isbn}.json {tn}.json\")\n",
    "   if lv == 'Lv2':\n",
    "      os.system(f\"cd /Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/{i}C/Lv2 && mkdir {tn} && mv {tn}*.json {tn}/ && cd {tn} && mkdir pdf_page\")\n",
    "      # pdf_page 만들기\n",
    "      pdf_path = f\"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/{i}C/Lv1/{tn}.pdf\"\n",
    "      doc = fitz.open(pdf_path)\n",
    "      for p, page in enumerate(doc):\n",
    "         img = page.get_pixmap(matrix=mat)\n",
    "         img.save(f\"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/ORIGINAL/{i}C/Lv2/{tn}/pdf_page/{tn}_{p+1:04d}.png\")\n",
    "      \n",
    "      # break\n",
    "\n",
    "   # SS0041 - SS0045 사이에서 MuPDF error: syntax error: invalid key in dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개별처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "with open('/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C/Lv3_4/SS0181_workbook/SS0181.json', 'r', encoding='utf-8') as f:\n",
    "    origin = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출제예상문제 # OX # 모의고사\n",
    "for page in origin['contents']:\n",
    "    page_contents = page.get('page_contents')\n",
    "    new_page_contents = page_contents\n",
    "    page_num = page.get('page')\n",
    "    new_addinfo = page.get('add_info')\n",
    "    if '오엑스 문제풀이' in page.get('page_contents'):\n",
    "    # if page.get('page') == \"0291\":\n",
    "        # if True:\n",
    "            # pattern = r'([0-9]{2})\\s+(.+?)\\s+O/X\\s*\\n해설\\s?(.+?)\\s?(?=\\d+\\s+|$)'\n",
    "            # matches = list(re.finditer(pattern, page_contents, re.DOTALL))\n",
    "\n",
    "            # for idx, match in enumerate(matches):\n",
    "            #     actual_idx = idx + 1  # 1부터 시작\n",
    "            #     num = match.group(1).zfill(2)\n",
    "            #     question = match.group(2).strip()\n",
    "            #     explanation = match.group(3).strip()\n",
    "        \n",
    "            #     # 태그 생성: {q_0074_0001}, {q_0074_0002} ...\n",
    "            #     tag = \"{\"+ f\"q_{page_num}_{str(actual_idx).zfill(4)}\" + \"}\\n\"\n",
    "            #     new_page_contents = new_page_contents.replace(page_contents[match.span()[0]:match.span()[1]], tag)\n",
    "\n",
    "            #     new_addinfo.append({\n",
    "            #         \"tag\": f\"q_{page_num}_{str(actual_idx).zfill(4)}\",\n",
    "            #         \"type\": \"question\",\n",
    "            #         \"description\": {\n",
    "            #             \"number\": num,\n",
    "            #             \"question\": question,\n",
    "            #             \"options\": [\n",
    "            #                 \"O\",\n",
    "            #                 \"X\"\n",
    "            #             ],\n",
    "            #             \"answer\": \"\",\n",
    "            #             \"explanation\": explanation\n",
    "            #         },\n",
    "            #         \"caption\": [],\n",
    "            #         \"file_path\": None,\n",
    "            #         \"bbox\": None\n",
    "            #     })\n",
    "\n",
    "            # page['page_contents'] = new_page_contents\n",
    "            # page['add_info'] = new_addinfo\n",
    "            pass\n",
    "\n",
    "    elif (\"출제예상문제\" in page_contents) or (re.match(r'\\d{2}\\s+', page_contents)):\n",
    "    # if page.get('page') == \"0385\":\n",
    "        # if True:\n",
    "            pattern = r'(\\d{2})\\s+(.+?)\\n((?:①[^\\n]+\\n)(?:②[^\\n]+\\n)(?:③[^\\n]+\\n)(?:④[^\\n]+\\n))\\n?정답\\s+([①②③④⑤])\\n해설\\s*(.+?)(?=\\n\\d{2}\\s+|$)'\n",
    "            matches = list(re.finditer(pattern, page_contents, re.DOTALL))\n",
    "\n",
    "            for idx, match in enumerate(matches):\n",
    "                actual_idx = idx + 1  # 1부터 시작\n",
    "                num = match.group(1).zfill(2)\n",
    "                question = match.group(2).strip()\n",
    "                options = match.group(3).strip().split('\\n')\n",
    "                answer = match.group(4).strip()\n",
    "                explanation = match.group(5).strip()\n",
    "\n",
    "        \n",
    "                # 태그 생성: {q_0074_0001}, {q_0074_0002} ...\n",
    "                tag = \"{\"+ f\"q_{page_num}_{str(actual_idx).zfill(4)}\" + \"}\"\n",
    "                new_page_contents = new_page_contents.replace(page_contents[match.span()[0]:match.span()[1]], tag)\n",
    "\n",
    "                new_addinfo.append({\n",
    "                    \"tag\": f\"q_{page_num}_{str(actual_idx).zfill(4)}\",\n",
    "                    \"type\": \"question\",\n",
    "                    \"description\": {\n",
    "                        \"number\": num,\n",
    "                        \"question\": question,\n",
    "                        \"options\": options,\n",
    "                        \"answer\": answer,\n",
    "                        \"explanation\": explanation\n",
    "                    },\n",
    "                    \"caption\": [],\n",
    "                    \"file_path\": None,\n",
    "                    \"bbox\": None\n",
    "                })\n",
    "\n",
    "            page['page_contents'] = new_page_contents\n",
    "            page['add_info'] = new_addinfo\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C/Lv3_4/SS0181_workbook/SS0181.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(origin, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_list = \"\"\"SS0001\n",
    "SS0004\n",
    "SS0005\n",
    "SS0006\n",
    "SS0007\n",
    "SS0008\n",
    "SS0009\n",
    "SS0010\n",
    "SS0011\n",
    "SS0012\n",
    "SS0013\n",
    "SS0014\n",
    "SS0015\n",
    "SS0016\n",
    "SS0017\n",
    "SS0018\n",
    "SS0019\n",
    "SS0020\n",
    "SS0021\n",
    "SS0022\n",
    "SS0023\n",
    "SS0024\n",
    "SS0025\n",
    "SS0026\n",
    "SS0030\n",
    "SS0031\n",
    "SS0032\n",
    "SS0033\n",
    "SS0034\n",
    "SS0035\n",
    "SS0036\n",
    "SS0037\n",
    "SS0039\n",
    "SS0040\n",
    "SS0041\n",
    "SS0042\n",
    "SS0043\n",
    "SS0045\n",
    "SS0049\n",
    "SS0050\n",
    "SS0052\n",
    "SS0053\n",
    "SS0054\n",
    "SS0055\n",
    "SS0056\n",
    "SS0057\n",
    "SS0058\n",
    "SS0059\n",
    "SS0060\n",
    "SS0061\n",
    "SS0062\n",
    "SS0064\n",
    "SS0066\n",
    "SS0069\n",
    "SS0071\n",
    "SS0072\n",
    "SS0075\n",
    "SS0078\n",
    "SS0080\n",
    "SS0083\n",
    "SS0085\n",
    "SS0090\n",
    "SS0092\n",
    "SS0093\n",
    "SS0095\n",
    "SS0099\n",
    "SS0101\n",
    "SS0103\n",
    "SS0104\n",
    "SS0105\n",
    "SS0106\n",
    "SS0107\n",
    "SS0110\n",
    "SS0111\n",
    "SS0112\n",
    "SS0113\n",
    "SS0114\n",
    "SS0116\n",
    "SS0117\n",
    "SS0119\n",
    "SS0120\n",
    "SS0121\n",
    "SS0122\n",
    "SS0123\n",
    "SS0124\n",
    "SS0125\n",
    "SS0126\n",
    "SS0127\n",
    "SS0128\n",
    "SS0129\n",
    "SS0130\n",
    "SS0131\n",
    "SS0132\n",
    "SS0133\n",
    "SS0134\n",
    "SS0135\n",
    "SS0136\n",
    "SS0137\n",
    "SS0138\n",
    "SS0140\n",
    "SS0145\n",
    "SS0148\n",
    "SS0149\n",
    "SS0150\n",
    "SS0151\n",
    "SS0152\n",
    "SS0153\n",
    "SS0154\n",
    "SS0155\n",
    "SS0156\n",
    "SS0157\n",
    "SS0158\n",
    "SS0159\n",
    "SS0160\n",
    "SS0161\n",
    "SS0164\n",
    "SS0165\n",
    "SS0166\n",
    "SS0168\n",
    "SS0169\n",
    "SS0170\n",
    "SS0171\n",
    "SS0172\n",
    "SS0173\n",
    "SS0175\n",
    "SS0176\n",
    "SS0177\n",
    "SS0178\n",
    "SS0179\n",
    "SS0185\n",
    "SS0212\n",
    "SS0229\n",
    "SS0230\n",
    "SS0234\n",
    "SS0235\n",
    "SS0262\n",
    "SS0263\n",
    "SS0268\n",
    "SS0269\n",
    "SS0270\n",
    "SS0271\n",
    "SS0272\n",
    "SS0273\n",
    "SS0274\n",
    "SS0275\n",
    "SS0277\n",
    "SS0278\n",
    "SS0279\n",
    "SS0280\n",
    "SS0281\n",
    "SS0282\n",
    "SS0285\n",
    "SS0288\n",
    "SS0289\n",
    "SS0290\n",
    "SS0291\n",
    "SS0292\n",
    "SS0293\n",
    "SS0294\n",
    "SS0295\n",
    "SS0296\n",
    "SS0297\n",
    "SS0298\n",
    "SS0300\n",
    "SS0319\n",
    "SS0320\n",
    "SS0321\n",
    "SS0322\n",
    "SS0323\n",
    "SS0324\n",
    "SS0325\n",
    "SS0326\n",
    "SS0327\n",
    "SS0328\n",
    "SS0329\n",
    "SS0330\n",
    "SS0331\n",
    "SS0333\n",
    "SS0334\n",
    "SS0335\n",
    "SS0337\n",
    "SS0338\n",
    "SS0339\n",
    "SS0340\n",
    "SS0341\n",
    "SS0342\n",
    "SS0343\n",
    "SS0344\n",
    "SS0345\n",
    "SS0346\n",
    "SS0347\n",
    "SS0348\n",
    "SS0349\n",
    "SS0350\n",
    "SS0351\n",
    "SS0352\n",
    "SS0353\n",
    "SS0354\n",
    "SS0355\n",
    "SS0356\n",
    "SS0357\n",
    "SS0358\n",
    "SS0359\n",
    "SS0360\n",
    "SS0361\n",
    "SS0362\n",
    "SS0363\n",
    "SS0365\n",
    "SS0366\n",
    "SS0367\n",
    "SS0368\n",
    "SS0369\n",
    "SS0370\n",
    "SS0371\n",
    "SS0372\n",
    "SS0373\n",
    "SS0374\n",
    "SS0375\n",
    "SS0376\n",
    "SS0377\n",
    "SS0378\n",
    "SS0379\n",
    "SS0380\n",
    "SS0382\n",
    "SS0383\n",
    "SS0384\n",
    "SS0385\n",
    "SS0387\n",
    "SS0388\n",
    "SS0389\n",
    "SS0390\n",
    "SS0391\n",
    "SS0392\n",
    "SS0393\n",
    "SS0395\n",
    "SS0401\n",
    "SS0407\n",
    "SS0409\n",
    "SS0413\n",
    "SS0414\n",
    "SS0415\n",
    "SS0416\n",
    "SS0420\n",
    "SS0424\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uploaded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin-Know 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_think_block(text):   ## for Qwen3\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n",
    "    return text\n",
    "\n",
    "model_name = 'gpt-5'\n",
    "\n",
    "res_file = f\"/Users/jinym/Desktop/Desktop_AICenter✨/SFAIcenter/evaluation/fai_code/eval_fai/result/res_{model_name}_fai-fin-know-2504a.csv\"\n",
    "\n",
    "df_res = pd.read_csv(res_file)\n",
    "df_res = df_res.fillna(\" \")\n",
    "df_res['response_rev'] = df_res['response'].apply(remove_think_block)\n",
    "df_res['response_rev'] = df_res['response_rev'].map(lambda x: \"\".join(re.findall(r\"[a-zA-Z]\", x)))\n",
    "df_res['response_rev'] = df_res['response_rev'].map(lambda x: x.lower().strip()[-1] if x != \"\" else x)\n",
    "\n",
    "df_res['correct'] = (df_res['answer'] == df_res['response_rev'])\n",
    "\n",
    "# df_to_write = df_res[[\"answer\", \"response_rev\", \"response\", \"correct\"]]\n",
    "# df_to_write.to_csv(\"thinking_answer.csv\")\n",
    "precision = round(df_res['correct'].sum()/len(df_res)*100, 2)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[df_res['response'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[df_res['correct'] == False][['answer', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "model_name = 'res_gemini-2.5-flash_fai-fin-know-2504a'\n",
    "path = '/Users/jinym/Desktop/Desktop_AICenter✨/SFAIcenter/evaluation/fai_code/eval_fai/result'\n",
    "\n",
    "# JSON에서 파일 목록 로드\n",
    "merge_files = [file for file in os.listdir(path) if file.startswith(model_name)]\n",
    "print(merge_files[0])\n",
    "print(f\"[병합] 총 {len(merge_files)}개 파일을 병합합니다...\")\n",
    "\n",
    "dfs = []\n",
    "for i, file_path in enumerate(merge_files):\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(path, file_path), encoding='utf-8-sig')\n",
    "        print(f\"[병합] 그룹 {i}: {len(df)}개 행 로드 완료 - {file_path}\")\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[병합] 오류: {file_path} 로드 실패 - {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    print(\"[병합] 오류: 병합할 데이터가 없습니다!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"[병합] 총 {len(merged_df)}개 행으로 병합 완료\")\n",
    "\n",
    "# 병합된 파일 저장\n",
    "merged_df.to_csv(os.path.join(path, model_name+\".csv\"), index=False, encoding='utf-8-sig')\n",
    "print(f\"[병합] 병합된 파일 저장: {model_name}\")\n",
    "print(f\"[병합] ✓ 병합 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
