{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도서 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_dir = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "# base_dir = \"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "data_dir_org = os.path.join(base_dir, 'data', 'ORIGINAL', '1C')\n",
    "data_dir = os.path.join(base_dir, 'data', 'FINAL', '2C_0902')\n",
    "\n",
    "analysis = {1:'1차 분석', 2:'2차 분석', 3: '3차 분석'}\n",
    "buy = {1:'1차 구매', 2:'2차 구매', 3: '3차 구매'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "excel_analy = pd.read_excel(os.path.join(base_dir, '도서목록_전체통합.xlsx'), sheet_name=analysis[i], header=3)[['관리번호', 'ISBN', '도서명','분류']]\n",
    "excel_buy = pd.read_excel(os.path.join(base_dir, '도서목록_전체통합.xlsx'), sheet_name=buy[i], header=4)[['ISBN', '도서명', '출판일', '코퍼스 1분류', '코퍼스 2분류', '비고']]\n",
    "excel_buy.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lv2 파일명 변경, Lv3/4/5 폴더 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "Lv2_isbn_id = excel_analy[excel_analy['분류'] == 'Lv2']\n",
    "Lv3_isbn_id = excel_analy[excel_analy['분류'] == 'Lv3/4']\n",
    "Lv5_isbn_id = excel_analy[excel_analy['분류'] == 'Lv5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lv2_isbn_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lv2\n",
    "# 폴더 만들어서 옮기기\n",
    "for id in Lv2_isbn_id['관리번호']:\n",
    "    id = str(id)\n",
    "    try:\n",
    "        os.mkdir(os.path.join(data_dir, 'Lv2', id))\n",
    "        os.rename(os.path.join(data_dir, 'Lv2', id+'.json'), os.path.join(data_dir, 'Lv2', id, id+'.json'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (isbn, id) in zip(cycle1_isbn_id['ISBN'], cycle1_isbn_id['관리번호']):\n",
    "#     # print(isbn, id)\n",
    "#     # 복사할 파일 경로\n",
    "#     source_file = os.path.join(data_dir, '1C_0902', str(isbn)+'.json')\n",
    "#     # 복사할 목적지 경로\n",
    "#     destination_path = os.path.join(data_dir, '1C_0902')\n",
    "#     # 이름변경\n",
    "#     shutil.copy2(source_file, os.path.join(destination_path, str(id)+\".json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lv분류\n",
    "lv34 = os.path.join(data_dir, \"Lv3_4\")\n",
    "lv5 = os.path.join(data_dir, \"Lv5\")\n",
    "\n",
    "for id in os.listdir(data_dir):\n",
    "    if os.path.splitext(id)[0] in Lv3_isbn_id['관리번호'].astype(str).tolist():\n",
    "        shutil.move(os.path.join(data_dir, str(id)), lv34)\n",
    "    elif os.path.splitext(id)[0] in Lv5_isbn_id['관리번호'].astype(str).tolist():\n",
    "        shutil.move(os.path.join(data_dir, str(id)), lv5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.splitext(id)[0] in Lv5_isbn_id['관리번호'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id in Lv3_isbn_id['관리번호'].astype(str).tolist()\n",
    "# print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lv2 전체 형식 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_excel = pd.merge(excel_analy[excel_analy['분류'] == 'Lv2'], excel_buy, on=['ISBN', '도서명'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_excel = merge_excel[['관리번호', 'ISBN', '도서명', '출판일', '코퍼스 1분류', '코퍼스 2분류', '비고']].set_index('관리번호')\n",
    "merge_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_2 = os.path.join(data_dir, 'Lv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "for id in os.listdir(data_dir_2):\n",
    "    if id.startswith('.DS'):\n",
    "        continue\n",
    "    # id = 'SS0299'\n",
    "    name = str(id)+'.json'\n",
    "    # origin_name = str(merge_excel.loc[id, 'ISBN'])+'.json'\n",
    "    origin = json.load(open(os.path.join(data_dir_2, str(id), name), 'r', encoding='utf-8'))\n",
    "    revision = {\n",
    "        'file_id': str(merge_excel.loc[id, 'ISBN']),\n",
    "        'title': merge_excel.loc[id, '도서명'],\n",
    "        'cat1_domain': merge_excel.loc[id, '코퍼스 1분류'],\n",
    "        'cat2_sub': merge_excel.loc[id, '코퍼스 2분류'],\n",
    "        'cat3_specific': merge_excel.loc[id, '비고'],\n",
    "        'pub_date': str(merge_excel.loc[id, '출판일'])[:10],\n",
    "        'contents': [],\n",
    "    }\n",
    "    for i in range(len(origin)):\n",
    "        if len(origin[i]['content']) > 0:\n",
    "            contents_base = {\n",
    "            'page': f\"{int(origin[i]['page']):04d}\",\n",
    "            'chapter': \"\",\n",
    "            'page_contents': origin[i]['content'],\n",
    "                \"add_info\": []\n",
    "            }\n",
    "            revision['contents'].append(contents_base)\n",
    "    \n",
    "    json.dump(revision, open(os.path.join(data_dir_2, str(id), name), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lv3/4/5 분류 이름 변경\n",
    "코퍼스_1분류, 코퍼스_2분류, 비고, 출판일 -> cat1_domain, cat2_sub, cat3_specific, pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_excel = pd.merge(excel_analy[excel_analy['분류'] == 'Lv5'], excel_buy, on=['ISBN', '도서명'], how='inner')\n",
    "merge_excel = merge_excel[['관리번호', 'ISBN', '도서명', '출판일', '코퍼스 1분류', '코퍼스 2분류', '비고']].set_index('관리번호')\n",
    "merge_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merg=merge_excel[merge_excel['코퍼스 2분류'] != '수험서']\n",
    "merg = merge_excel\n",
    "merg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lv분류\n",
    "# # lv34 = os.path.join(data_dir, \"Lv3_4\")\n",
    "# lv5 = os.path.join(data_dir, \"Lv5\")\n",
    "\n",
    "# for id in merg.index:\n",
    "#     id = str(id)\n",
    "#     origin = json.load(open(os.path.join(lv5, id+'_workbook', id+\".json\"), 'r', encoding='utf-8'))\n",
    "#     try:\n",
    "#         origin['cat1_domain']\n",
    "#     except:\n",
    "#         revision = {\n",
    "#             'file_id': origin['file_id'],\n",
    "#             'title': origin['title'],\n",
    "#             'cat1_domain': origin['코퍼스_1분류'],\n",
    "#             'cat2_sub': origin['코퍼스_2분류'],\n",
    "#             'cat3_specific': origin['비고'],\n",
    "#             'pub_date': origin['출판일'],\n",
    "#             'contents': origin['contents']\n",
    "#         }\n",
    "    \n",
    "#         json.dump(revision, open(os.path.join(lv5, id+\"_workbook\", id+\"_name.json\"), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "290155840 in merg.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 개별 처리\n",
    "- 불필요 페이지 제거\n",
    "- 오타\n",
    "- 페이지 머리말/꼬리말"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "\n",
    "base_dir = \"data/FINAL/2C_0902\"\n",
    "data_dir = os.path.join(base_dir,'Lv2')\n",
    "\n",
    "# lv2\n",
    "# name = sorted(os.listdir(data_dir))[3]\n",
    "\n",
    "# lv3/4\n",
    "names = []\n",
    "for n in sorted(os.listdir(data_dir)):\n",
    "    if n.endswith('workbook'):\n",
    "        names.append(n.split('_')[0])\n",
    "\n",
    "name = names[0].split('_')[0]\n",
    "# origin = json.load(open(os.path.join(data_dir, name, name+'.json'), 'r', encoding='utf-8'))\n",
    "origin = json.load(open(os.path.join(data_dir, name+\"_workbook\", name+'.json'), 'r', encoding='utf-8'))\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1차 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_split(txt, sep):\n",
    "    result = re.sub(r'(?<![.?!①②③④⑤\\[\\]])\\n(?!\\n)', ' ', txt)\n",
    "    return result.split(sep)\n",
    "\n",
    "def remove_enter(txt):\n",
    "    # 문장 내 엔터 처리 (안함)\n",
    "    return re.sub(r'(?<![.?!\\]])\\n(?!\\n)', ' ', txt)\n",
    "\n",
    "def extract_options(txt):\n",
    "    pattern = r'([①②③④⑤]\\s*[^①②③④⑤]*)'\n",
    "    options = re.findall(pattern, txt)\n",
    "    return [opt.replace(\"\\n\", \" \").strip() for opt in options if opt.strip()]\n",
    "\n",
    "def replace_number(number):\n",
    "    circle_numbers = {'1': '①', '2': '②', '3': '③', '4': '④', '5': '⑤'}\n",
    "    return circle_numbers[str(number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chapter(c, n):\n",
    "    #  chapter 추출하기\n",
    "    page = int(c['page']) - n\n",
    "    regex = fr\"^(.*)\\n{page}\\n\"\n",
    "\n",
    "    chapter = re.findall(regex, c['page_contents'])\n",
    "\n",
    "    c['chapter'] = chapter[0].strip()\n",
    "    c['page_contents'] = re.sub(regex, \"\", c['page_contents'])\n",
    "    \n",
    "    # 248779244.json\n",
    "    # c['page_contents'] = re.sub(fr\"^{page}[가-힇]?\", \"\", c['page_contents'])\n",
    "    # c['page_contents'] = re.sub(title+str(page)+'\\n', \"\", c['page_contents'])\n",
    "    return c\n",
    "\n",
    "def fill_chapter(c):\n",
    "    # chapter 채우기\n",
    "    if (c['chapter'] == \"\") and i >= 1:\n",
    "        c['chapter'] = origin['contents'][i-1]['chapter']\n",
    "\n",
    "    # 284618539.json\n",
    "    # if c['page_contents'] == \"\": break\n",
    "    \n",
    "    # if (c['chapter'] == \"\") and i >= 1:\n",
    "    #     c['chapter'] = origin['contents'][i-1]['chapter']\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qna(c):\n",
    "    print('---------------------')\n",
    "    print(c['page'])\n",
    "# for i in range(len())\n",
    "    # 문제추출\n",
    "    # qn_re = r'\\[문제\\d+\\]\\s'\n",
    "    qn_re = r'\\s?문제 [0-9]+:'\n",
    "    # qn_re = r'\\s?[0-9][0-9]'\n",
    "    # info_re = r'\\[\\w+[.]?\\d+\\w\\]\\s'\n",
    "    # q_re = r\"\\s*[^?]*\\?\"\n",
    "    # ans_re = r\"\\[정답\\]\"\n",
    "    ans_re = r\"\\s?정답:\"\n",
    "    # exp_re = r\"\\s\\[해설\\]\"\n",
    "    exp_re = r\"\\s?해설:\"\n",
    "    option_re = r\"\\s?①\"\n",
    "    \n",
    "    base_add_info = {\n",
    "                        'tag': \"\",\n",
    "                        'type': \"question\",\n",
    "                        \"description\": {\n",
    "                            \"number\": \"\",\n",
    "                            \"question\": \"\",\n",
    "                            \"options\": [],\n",
    "                            'answer' : \"\",\n",
    "                            \"explanation\": \"\"\n",
    "                        },\n",
    "                        \"caption\":[],\n",
    "                        \"file_path\": 0,\n",
    "                        \"bbox\": 0\n",
    "                        }\n",
    "\n",
    "    # 문제는 있음\n",
    "    if re.search(qn_re, c['page_contents']) is not None:\n",
    "\n",
    "        base_add_info['tag'] = f\"q_{c['page']}_0001\"\n",
    "\n",
    "        start = re.search(qn_re, c['page_contents']).span()[0]\n",
    "        try:\n",
    "            end = re.search(ans_re, c['page_contents']).span()[1]\n",
    "            # 한 페이지에 문제~정답 모두 있음\n",
    "            # qna = [qa for qa in re.split(fr\"({qn_re}|{info_re}|{q_re}|{exp_re}|{ans_re})\", c['page_contents'][start:]) if qa !=\"\"]\n",
    "            qna = [qa for qa in re.split(fr\"({qn_re}|{option_re}|{exp_re}|{ans_re})\", c['page_contents'][start:]) if qa !=\"\"]\n",
    "            # 태그처리\n",
    "            c['page_contents'] = c['page_contents'].replace(c['page_contents'][start:end+2], \"\\n{\"+f\"q_{c['page']}_0001\"+\"}\")\n",
    "        \n",
    "        except:\n",
    "            # 다음페이지 살펴보기\n",
    "            c2 = origin['contents'][i+1]\n",
    "            if re.search(ans_re, c2['page_contents']) is not None:\n",
    "                end = re.search(ans_re, c2['page_contents']).span()[1]\n",
    "\n",
    "                qna = [qa \n",
    "                    # for qa in re.split(fr\"({qn_re}|{info_re}|{q_re}|{exp_re}|{ans_re})\", \n",
    "                    for qa in re.split(fr\"({qn_re}|{option_re}|{exp_re}|{ans_re})\", \n",
    "                                        c['page_contents'][start:]+c2['page_contents'][:end+2])\n",
    "                    if qa !=\"\"\n",
    "                    ]\n",
    "                \n",
    "                # 태그처리\n",
    "                c['page_contents'] = c['page_contents'].replace(c['page_contents'][start:], \"\\n{\"+f\"q_{c['page']}_0001\"+\"}\")\n",
    "                c2['page_contents'] = c2['page_contents'].replace(c2['page_contents'][:end+2], \"\")\n",
    "            \n",
    "            # 그 다음 페이지도 살펴보기\n",
    "            elif re.search(ans_re, c2['page_contents']) is None:\n",
    "                c3 = origin['contents'][i+2]\n",
    "                if re.search(ans_re, c3['page_contents']) is not None:\n",
    "                    end = re.search(ans_re, c3['page_contents']).span()[1]\n",
    "                    qna = [qa \n",
    "                        # for qa in re.split(fr\"({qn_re}|{info_re}|{q_re}|{exp_re}|{ans_re})\", \n",
    "                        for qa in re.split(fr\"({qn_re}|{option_re}|{exp_re}|{ans_re})\", \n",
    "                                            c['page_contents'][start:]+c2['page_contents']+c3['page_contents'][:end+2])\n",
    "                        if qa !=\"\"\n",
    "                        ]\n",
    "                    \n",
    "                    # 태그처리\n",
    "                    c['page_contents'] = c['page_contents'].replace(c['page_contents'][start:], \"\\n{\"+f\"q_{c['page']}_0001\"+\"}\")\n",
    "                    c2['page_contents'] = c2['page_contents'].replace(c2['page_contents'], \"\")\n",
    "                    c3['page_contents'] = c3['page_contents'].replace(c3['page_contents'][:end+2], \"\")\n",
    "            else:\n",
    "                print(\"못찾겟다..\")\n",
    "\n",
    "        print(qna)\n",
    "        for x in range(len(qna)):\n",
    "            if re.search(qn_re, qna[x]) is not None:\n",
    "                # number = re.search(r'\\[문제(\\d+)*\\]', qna[x]).group(1)\n",
    "                number = qna[x].strip()\n",
    "                print('number:', number)\n",
    "                # if int(number) < 10:\n",
    "                #     number = f\"{int(number):02}\"\n",
    "                base_add_info['description']['number'] = number\n",
    "                try:\n",
    "                    question = qna[x+1]\n",
    "                    question = question.strip()\n",
    "\n",
    "                    base_add_info['description']['question'] = question\n",
    "                except:\n",
    "                    print(\"(ERROR) question:\", question)\n",
    "\n",
    "            # if re.search(info_re, qna[x]) is not None:\n",
    "            #     base_add_info['caption'].append(re.search(r'\\[(\\w+[.]?\\d+\\w)\\]', qna[x]).group(1))\n",
    "            #     if base_add_info['caption'][0].startswith(\"문제\"):\n",
    "            #         base_add_info['caption'].pop(0)\n",
    "\n",
    "            # if re.search(q_re, qna[x]) is not None:\n",
    "            #     question = re.search(r'\\s*([^?]*\\?)', qna[x]).group(1)\n",
    "            #     options = [ca.strip() for ca in n_split(qna[x+1].replace(\"❑\",\"\"), \"\\n\") if ca != \"\"]\n",
    "            #     for ct in range(len(options)):\n",
    "            #         if '①' not in options[0]:\n",
    "            #             question += \" \"+options[0]\n",
    "                        # options = options[1:]\n",
    "                \n",
    "            if re.search(option_re, qna[x]) is not None:\n",
    "                try:\n",
    "                    options = [qna[x]+qna[x+1]]\n",
    "                except:\n",
    "                    print(\"(ERROR) options:\", options)\n",
    "                if len(options) == 1:\n",
    "                    options = extract_options(options[0])\n",
    "                print(\"options_re:\", options)\n",
    "                base_add_info['description']['options'] = options\n",
    "\n",
    "            if re.search(exp_re, qna[x]) is not None:\n",
    "                explanation = qna[x+1].strip()\n",
    "                print(\"explanation:\", explanation)\n",
    "                base_add_info['description']['explanation'] = explanation\n",
    "\n",
    "            if re.search(ans_re, qna[x]) is not None:\n",
    "                answer = qna[x+1].strip()\n",
    "                print(\"answer:\", answer)\n",
    "                if answer in \"①②③④⑤\":\n",
    "                    base_add_info['description']['answer'] = answer\n",
    "                elif answer.isnumeric():\n",
    "                    answer = replace_number(answer)\n",
    "                    base_add_info['description']['answer'] = answer\n",
    "                else:\n",
    "                    print(c['page'], end, answer)\n",
    "            \n",
    "            # base_add_info['caption'].append(\"매경 TEST 기출 문제\")\n",
    "\n",
    "        c['add_info'].append(base_add_info)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_footnote(c):\n",
    "    # 각주 처리\n",
    "    for fn in range(1, 43):\n",
    "        if f\"\\n{fn} \" in c['page_contents']:\n",
    "            start = c['page_contents'].find(f\"\\n{fn} \")\n",
    "            tag = f\"note_{c['page']}_{len(c['add_info'])+1:04}\"\n",
    "\n",
    "            c['add_info'].append(\n",
    "                {\n",
    "                    \"tag\": tag,\n",
    "                    \"type\": \"footnote\",\n",
    "                    \"description\": c['page_contents'][start+1:], # 두개 겹쳐있으면 그대로..\n",
    "                    \"caption\": 0,\n",
    "                    \"file_path\": 0,\n",
    "                    \"bbox\": 0\n",
    "                }\n",
    "            )\n",
    "            c['page_contents'] = c['page_contents'].replace(c['page_contents'][start:], \"{\"+tag+\"}\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "base_dir = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "data_dir = os.path.join(base_dir, 'data', 'FINAL', '2C_0902')\n",
    "\n",
    "json_files = []\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\") and ('_' not in f) and ('_workbook' in root):\n",
    "        #  and ('Lv5' not in root):\n",
    "            json_files.append(os.path.join(root, f))\n",
    "            # json_files.append(int(os.path.splitext(f)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = json.load(open(json_files[0], 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0119_workbook/SS0119.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0121_workbook/SS0121.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0122_workbook/SS0122.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0124_workbook/SS0124.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0125_workbook/SS0125.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0126_workbook/SS0126.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0127_workbook/SS0127.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0128_workbook/SS0128.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0129_workbook/SS0129.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0134_workbook/SS0134.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0136_workbook/SS0136.json',\n",
       " '/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/2C_0902/Lv2/SS0137_workbook/SS0137.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files = sorted(json_files)\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.process_quizbook as quiz\n",
    "\n",
    "quiz.main(json_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in json_files:\n",
    "if True:\n",
    "    j = json_files[1]\n",
    "    origin = json.load(open(j, 'r', encoding='utf-8'))\n",
    "    \n",
    "    new = {\n",
    "        'file_id': origin['file_id'],\n",
    "        'title': origin['title'],\n",
    "        'cat1_domain': origin['cat1_domain'],\n",
    "        'cat2_sub': origin['cat2_sub'],\n",
    "        'cat3_specific': origin['cat3_specific'],\n",
    "        'pub_date': origin['pub_date'],\n",
    "        'contents': [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(origin['contents'])):\n",
    "        contents = origin['contents'][i]\n",
    "        \n",
    "        # c = extract_qna(contents)\n",
    "        # page_contents = remove_enter(contents['page_contents'])\n",
    "        # contents['page_contents'] = page_contents\n",
    "        c = fill_chapter(contents)\n",
    "        # c = contents\n",
    "        # print(contents)\n",
    "        \n",
    "        if len(c['page_contents']) > 0:\n",
    "            new['contents'].append(c)\n",
    "        \n",
    "\n",
    "    json.dump(new, open(j, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    # json.dump(new, open(j.replace(\".json\", \"_new.json\"), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    # json.dump(new, open(os.path.join(data_dir, name+'_workbook', name+'.json'), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in enumerate(json_files):\n",
    "    if '276709608' in j:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "\n",
    "base_dir = \"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL\"\n",
    "data_dir = os.path.join(base_dir, '1C_0902', 'Lv2')\n",
    "\n",
    "name = sorted(os.listdir(data_dir))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = json.load(open(os.path.join(data_dir, name, name+'.json'), 'r', encoding='utf-8'))\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5분 손해사정사 제3보험 (321494387) - 기출/답안 추출\n",
    "new = {\n",
    "    'file_id': origin['file_id'],\n",
    "    'title': origin['title'],\n",
    "    'cat1_domain': origin['cat1_domain'],\n",
    "    'cat2_sub': origin['cat2_sub'],\n",
    "    'cat3_specific': origin['cat3_specific'],\n",
    "    'pub_date': origin['pub_date'],\n",
    "    'contents': [],\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(origin['contents'])):\n",
    "    # i = 126\n",
    "    c = origin['contents'][i]\n",
    "\n",
    "    base_add_info = {\n",
    "    'tag': \"\",\n",
    "    'type': \"question\",\n",
    "    \"description\": {\n",
    "        \"number\": \"\",\n",
    "        \"question\": \"\",\n",
    "        \"options\": 0,\n",
    "        'answer' : \"\",\n",
    "        \"explanation\": \"\"\n",
    "    },\n",
    "    \"caption\":[],\n",
    "    \"file_path\": 0,\n",
    "    \"bbox\": 0\n",
    "    }\n",
    "    # 문제추출\n",
    "    q_re = r'\\[[0-9]{4} 기출\\]'\n",
    "    ans_re = r'\\[[0-9]{4} 답안\\]'\n",
    "    key_re = r'\\s키워드'\n",
    "\n",
    "    # break\n",
    "    try:\n",
    "    # if 1:\n",
    "        # [기출] ~ [답안]꼴\n",
    "        q_start = re.search(q_re, c['page_contents']).span()[0]\n",
    "        q_end = re.search(ans_re, c['page_contents'][q_start:]).span()[0] # 그꼴이 아니면 여기서 에러남\n",
    "        a_start = q_end + q_start\n",
    "        \n",
    "        question = c['page_contents'][q_start:q_start+q_end]\n",
    "\n",
    "       \n",
    "        # caption 달기\n",
    "        caption = re.findall(r'\\[([0-9]{4} 기출)\\]', question)[0]\n",
    "        base_add_info['caption'].append(caption)\n",
    "\n",
    "        question = re.sub(q_re, \"\", question).strip()      \n",
    "\n",
    "         # 문제번호 뽑기\n",
    "        number = re.findall(r\"\\D?(\\d+)\\D?\\s\", question)[0]\n",
    "        base_add_info['description']['number'] = number.strip()\n",
    "        question = re.sub(r\"\\D?\\d+\\D?\\s\", \"\", question).strip()\n",
    "\n",
    "\n",
    "        # 태그 처리\n",
    "        tag = f\"q_{c['page']}_0001\"\n",
    "        base_add_info['tag'] = tag\n",
    "\n",
    "        \n",
    "                \n",
    "        # 한 페이지에 [답안] 없으면\n",
    "        if re.search(key_re, c['page_contents'][a_start:]) is None:\n",
    "            print(\"in two pages\")\n",
    "            c2 = origin['contents'][i+1]\n",
    "            a_end = re.search(key_re, c2['page_contents']).span()[0]\n",
    "            # print(c2)\n",
    "            answer = c['page_contents'][a_start:] + '\\n' + c2['page_contents'][:a_end]\n",
    "\n",
    "            c['page_contents'] += '\\n' + c2['page_contents'][:a_end]\n",
    "            c2['page_contents'] = c2['page_contents'].replace(c2['page_contents'][:a_end], \"\")\n",
    "\n",
    "            c['page_contents'] = c['page_contents'].replace(c['page_contents'][q_start:], \"{\"+tag+\"}\")\n",
    "\n",
    "            if c2['page_contents'].strip().startswith('키워드'):\n",
    "                keyword_org = c2['page_contents'].strip().split(\"\\n\")[0]\n",
    "                keyword = keyword_org.replace(\"키워드\", \"키워드: \").replace(\"  \", \" \")\n",
    "                base_add_info['caption'].append(keyword)\n",
    "                c2['page_contents'] = c2['page_contents'].replace(\"\\n\"+keyword_org, \"\")\n",
    "            # print(c2)\n",
    "        # 한 페이지에 [답안]까지 있음\n",
    "        else:\n",
    "            print(\"in one page\")\n",
    "            a_end = re.search(key_re, c['page_contents'][a_start:]).span()[0]\n",
    "            answer = c['page_contents'][a_start:a_start+a_end]\n",
    "            if c['page_contents'][a_start+a_end:].strip().startswith('키워드'):\n",
    "                keyword_org = c['page_contents'][a_start+a_end:].strip().split(\"\\n\")[0]\n",
    "                keyword = keyword_org.replace(\"키워드\", \"키워드: \").replace(\"  \", \" \")\n",
    "                base_add_info['caption'].append(keyword)\n",
    "\n",
    "            c['page_contents'] = c['page_contents'].replace(c['page_contents'][q_start:a_start+a_end], \"{\"+tag+\"}\")\n",
    "            c['page_contents'] = c['page_contents'].replace(\"\\n\"+keyword_org, \"\")\n",
    "\n",
    "        answer = re.sub(ans_re, \"\", answer)\n",
    "        \n",
    "\n",
    "        base_add_info['description']['question'] = question.strip()\n",
    "        base_add_info['description']['answer'] = answer.strip()\n",
    "\n",
    "        c['add_info'].append(base_add_info)\n",
    "\n",
    "    except Exception as e:\n",
    "    # else:\n",
    "        if re.search(q_re, c['page_contents']) is not None:\n",
    "            print(c['page'])\n",
    "        else:\n",
    "            # print(e, c['page'])\n",
    "            pass\n",
    "\n",
    "    # break\n",
    "    if len(c['page_contents']) > 0:\n",
    "        new['contents'].append(c)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # break\n",
    "\n",
    "# json.dump(origin, open(os.path.join(data_dir, name, name+'_new.json'), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = [\n",
    "  \"◆ ① (O) [상법 제317조 제2항 제8호, 제9호]\",\n",
    "                            \"◆ ② (O) [상법 제342조의2 제1항]\",\n",
    "                            \"◆ ③ (O) [상법 제382조 제3항 제1호]\",\n",
    "                            \"◆ ④ (X) 두지 못한다. [상법 제408조의2 제1항] / ⑤ (O) [상법 제408조의2 제2항]\",\n",
    "                            \"✓ 제408조의2(집행임원 설치회사, 집행임원과 회사의 관계) ① 회사는 집행임원을 둘 수 있다. 이 경우 집행임원을 둔 회사(이하 \\\"집행임원 설치회사\\\"라 한다) 는 대표이사를 두지 못한다.\",\n",
    "                            \"✓ ② 집행임원 설치회사와 집행임원의 관계는 「민법」 중 위임에 관한 규정을 준용한다.\"\n",
    "]\n",
    "\n",
    "exp = \"\\n\".join(exp).replace('◆ ', \"\")\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# add_info\n",
    "```json\n",
    "{\n",
    "    \"tag\": \"q_0000_0001\",\n",
    "    \"type\": \"question\",\n",
    "    \"description\": {\n",
    "        \"number\": \"\",\n",
    "        \"question\": \"\",\n",
    "        \"options\": [],\n",
    "        \"options\": null,\n",
    "        \"answer\": \"\",\n",
    "        \"explanation\": \"\"\n",
    "    },\n",
    "    \"caption\": [\n",
    "        \" 기출\",\n",
    "        \"키워드: \"\n",
    "    ],\n",
    "    \"file_path\": null,\n",
    "    \"bbox\": null\n",
    "},\n",
    "{\n",
    "    \"tag\": \"note_0000_0001\",\n",
    "    \"type\": \"footnote\",\n",
    "    \"description\": \"1 이러한 계약을 법적으로는 금전소비대차계약이라고 한다. 차용증서를 영어로는 I owe you.의 소리를 따라 IOU 라고 한다.\",\n",
    "    \"caption\": null,\n",
    "    \"file_path\": null,\n",
    "    \"bbox\": null\n",
    "},\n",
    "{\n",
    "    \"tag\": \"tb_0000_0001\",\n",
    "    \"type\": \"table\",\n",
    "    \"description\": \"\\\\begin{tabular}{|c|c|c|c|}\\n\\\\hline\\n나라 이름 & 지수 이름 & 포괄 종목 & 작성 기관 \\\\\\\\\\\\hline\\n\\\\multirow{2}{*}{한 국} & KOSPI & \\\\makecell[l]{유가증권시장 상장\\\\\\\\전종목} & 한국거래소 \\\\\\\\\\\\cline{2-4}\\n& KOSDAQ & \\\\makecell[l]{코스닥 상장 전종목} & 한국거래소 \\\\\\\\\\\\hline\\n\\\\multirow{4}{*}{미 국} & DJIA & \\\\makecell[l]{뉴욕거래소, 나스닥\\\\\\\\상장 30 개 종목} & 다우존스사 \\\\\\\\\\\\cline{2-4}\\n& S\\\\&P 500 & \\\\makecell[l]{뉴욕거래소,\\\\\\\\미국거래소, 나스닥\\\\\\\\상장 500 개 종목} & \\\\makecell[c]{Standard \\\\&\\\\\\\\Poors 사} \\\\\\\\\\\\hline\\n\\\\multirow{3}{*}{일 본} & TOPIX & \\\\makecell[l]{동경거래소 상장\\\\\\\\전종목} & 동경거래소 \\\\\\\\\\\\cline{2-4}\\n& NIKKEI 225 & \\\\makecell[l]{동경거래소 상장\\\\\\\\225 개 종목} & 일본경제신문사 \\\\\\\\\\\\hline\\n영 국 & FTSE 100 & \\\\makecell[l]{런던거래소 상장\\\\\\\\100 개 종목} & FTSE \\\\\\\\\\\\hline\\n독 일 & DAX 30 & \\\\makecell[l]{프랑크푸르트 거래소\\\\\\\\상장 30 개 종목} & Deutsche Boerse \\\\\\\\\\\\hline\\n프랑스 & CAC 40 & \\\\makecell[l]{Euronext Paris 상장\\\\\\\\40 개 종목} & Euronext \\\\\\\\\\\\hline\\n중 국 & 상해종합지수 & \\\\makecell[l]{상해거래소 상장\\\\\\\\전종목} & 상해거래소 \\\\\\\\\\\\hline\\n\\\\end{tabular}\",\n",
    "    \"caption\": [\n",
    "        \"표2. 주가지수 개요\"\n",
    "    ],\n",
    "    \"file_path\": \"??????/crop/tb_0000_0001.png\",\n",
    "    \"bbox\": null\n",
    "},\n",
    "{\n",
    "    \"tag\": \"img_0000_0001\",\n",
    "    \"type\": \"image\",\n",
    "    \"description\": null,\n",
    "    \"caption\": [\n",
    "        \"도표1: 경제주체간 상품, 생산요소 및 자금의 흐름\"\n",
    "    ],\n",
    "    \"file_path\": \"?????/crop/img_0000_0001.png\",\n",
    "    \"bbox\": null\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "\n",
    "base_dir = \"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL\"\n",
    "data_dir = os.path.join(base_dir, '1C_0902', 'Lv2')\n",
    "\n",
    "json_files = []\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(root, f))\n",
    "\n",
    "# json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for js in json_files:\n",
    "    origin = json.load(open(js, 'r', encoding='utf-8'))\n",
    "    if str(origin).find('\": 0,') != -1:\n",
    "        print(js)\n",
    "        new = str(origin)\n",
    "        new = new.replace('\": 0,', '\": null,')\n",
    "        json.dump(new, open(js, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(origin).find(\": 0,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = str(origin)\n",
    "print(new[791:800])\n",
    "new = new.replace(\": 0,\", \": null,\")\n",
    "print(new[791:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
