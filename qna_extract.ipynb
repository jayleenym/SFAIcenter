{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_dir = \"/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "# base_dir = \"/Users/yejin/Library/CloudStorage/OneDrive-개인/데이터L/selectstar\"\n",
    "data_dir = os.path.join(base_dir, 'data', 'FINAL', '1C_0910')\n",
    "new_data_dir = os.path.join(base_dir, 'data', 'FIN_workbook', '1C')\n",
    "\n",
    "analysis = {1:'1차 분석', 2:'2차 분석', 3: '3차 분석'}\n",
    "buy = {1:'1차 구매', 2:'2차 구매', 3: '3차 구매'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "excel_analy = pd.read_excel(os.path.join(base_dir, '도서목록_전체통합.xlsx'), sheet_name=analysis[i], header=3)[['관리번호', 'ISBN', '도서명','분류']]\n",
    "excel_buy = pd.read_excel(os.path.join(base_dir, '도서목록_전체통합.xlsx'), sheet_name=buy[i], header=4)[['ISBN', '도서명', '출판일', '코퍼스 1분류', '코퍼스 2분류', '비고']]\n",
    "excel_buy.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_excel = pd.merge(excel_analy, excel_buy, on=['ISBN', '도서명'], how='inner').set_index('관리번호')\n",
    "\n",
    "Lv2_isbn_id = merge_excel[merge_excel['분류'] == 'Lv2']\n",
    "Lv3_isbn_id = merge_excel[merge_excel['분류'] == 'Lv3/4']\n",
    "Lv5_isbn_id = merge_excel[merge_excel['분류'] == 'Lv5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 56, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Lv2_isbn_id),len(Lv3_isbn_id), len(Lv5_isbn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workbook = merge_excel[merge_excel['코퍼스 2분류'] == '수험서']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "json_files = []\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\") and ('_' not in f) and ('Lv5' in root):\n",
    "            json_files.append(os.path.join(root, f))\n",
    "            # json_files.append(int(os.path.splitext(f)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_extracted_qna(qna_info: dict):\n",
    "    # qna_info = qna_data['qna_data']\n",
    "    # print(qna_info)\n",
    "    try:\n",
    "        if 'description' in qna_info and 'options' in qna_info['description']:\n",
    "            options = qna_info['description']['options']\n",
    "            answer = qna_info['description']['answer']\n",
    "            if (len(options) >= 4 and len(options) <= 5) and (len(answer) == 4 or len(answer) == 1):\n",
    "                # 객관식\n",
    "                return 'multiple-choice'\n",
    "            else:\n",
    "                # 주관식 - 답변의 문장 수로 단답형/서술형 구분\n",
    "                sentence_count = answer.count('.') + answer.count('!') + answer.count('?')\n",
    "                if sentence_count <= 1:\n",
    "                    # 한 문장 또는 한 단어 (단답형)\n",
    "                    return 'short-answer'\n",
    "                else:\n",
    "                    # 2문장 이상 (서술형)\n",
    "                    return 'essay'\n",
    "    except Exception as e:\n",
    "        print(\"분석 오류:\", e)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 수정된 extract_qna_tags 함수 (정규식 패턴 수정)\n",
    "def extract_qna_tags(json_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    page_contents에서 {q_0000_0000} 형태의 태그를 추출하고,\n",
    "    add_info에서 해당 태그를 찾아서 별도 리스트로 분리하는 함수\n",
    "    Q&A 내용 안의 tb, img, f 태그도 함께 추출하여 제거 (수정된 정규식)\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON 데이터\n",
    "        \n",
    "    Returns:\n",
    "        수정된 JSON 데이터와 추출된 Q&A 리스트\n",
    "    \"\"\"\n",
    "    # 추출된 Q&A를 저장할 리스트\n",
    "    extracted_qna = []\n",
    "\n",
    "    # 각 페이지를 순회\n",
    "    for page_data in json_data.get('contents', []):\n",
    "        page_contents = page_data.get('page_contents', '')\n",
    "        if page_contents != \"\":\n",
    "            add_info = page_data.get('add_info', [])\n",
    "            \n",
    "            # page_contents에서 {q_0000_0000} 형태의 태그 추출\n",
    "            qna_tags = re.findall(r'\\{q_\\d{4}_\\d{4}\\}', page_contents)\n",
    "            \n",
    "            # 제거할 인덱스들을 수집\n",
    "            indices_to_remove = set()\n",
    "            qna_items_to_extract = []\n",
    "            \n",
    "            # 각 태그에 대해 add_info에서 해당하는 항목 찾기\n",
    "            for tag in qna_tags:\n",
    "                # add_info에서 해당 태그를 가진 항목 찾기\n",
    "                qna_item_index = None\n",
    "                qna_item = None\n",
    "                tag = tag.removeprefix('{').removesuffix('}')\n",
    "                \n",
    "                for i, info_item in enumerate(add_info):\n",
    "                    if info_item.get('tag') == tag:\n",
    "                        qna_item_index = i\n",
    "                        qna_item = info_item\n",
    "                        break\n",
    "                \n",
    "                if qna_item is not None:\n",
    "                    # Q&A 내용에서 추가 태그들 추출\n",
    "                    qna_content = \"\"\n",
    "                    if 'description' in qna_item:\n",
    "                        desc = qna_item['description']\n",
    "                        # question, answer, explanation, options에서 태그 추출\n",
    "                        for field in ['question', 'answer', 'explanation', 'options']:\n",
    "                            if field in desc and desc[field]:\n",
    "                                if field == 'options' and isinstance(desc[field], list):\n",
    "                                    # options는 리스트이므로 각 항목을 합침\n",
    "                                    for option in desc[field]:\n",
    "                                        qna_content += str(option) + \" \"\n",
    "                                else:\n",
    "                                    qna_content += str(desc[field]) + \" \"\n",
    "                    \n",
    "                    # Q&A 내용에서 tb, img, f 태그 추출 (수정된 정규식)\n",
    "                    tb_tags = re.findall(r'\\{tb_\\d{4}_\\d{4}\\}', qna_content)\n",
    "                    img_tags = re.findall(r'\\{img_\\d{4}_\\d{4}\\}', qna_content)\n",
    "                    f_tags = re.findall(r'\\{f_\\d{4}_\\d{4}\\}', qna_content)\n",
    "                    additional_tags = tb_tags + img_tags + f_tags\n",
    "                    \n",
    "                    # 디버깅: 추가 태그 발견 시 출력\n",
    "                    # if additional_tags:\n",
    "                    #     print(f\"  추가 태그 발견 - Q&A: {tag}\")\n",
    "                    #     print(f\"    TB: {tb_tags}, IMG: {img_tags}, F: {f_tags}\")\n",
    "                    \n",
    "                    # 추가 태그들의 실제 데이터 수집\n",
    "                    additional_tag_data = []\n",
    "                    \n",
    "                    # 추가 태그들도 add_info에서 찾아서 인덱스 수집 및 데이터 저장\n",
    "                    for additional_tag in additional_tags:\n",
    "                        tag_without_braces = additional_tag[1:-1]  # {tag} -> tag\n",
    "                        for j, additional_info_item in enumerate(add_info):\n",
    "                            if additional_info_item.get('tag') == tag_without_braces:\n",
    "                                indices_to_remove.add(j)\n",
    "                                # 추가 태그의 실제 데이터도 저장\n",
    "                                additional_tag_data.append({\n",
    "                                    'tag': additional_tag,\n",
    "                                    # 'tag_type': additional_tag.split('_')[0][1:],  # {tb_0000_0000} -> tb\n",
    "                                    'data': additional_info_item\n",
    "                                })\n",
    "                                # print(f\"    -> 추가 태그 데이터 수집: {additional_tag}\")\n",
    "                                break\n",
    "                    \n",
    "                    # Q&A 항목도 제거 대상에 추가\n",
    "                    indices_to_remove.add(qna_item_index)\n",
    "        \n",
    "                    # 추출할 Q&A 정보 저장\n",
    "                    qna_items_to_extract.append({\n",
    "                        'file_id': json_data.get(\"file_id\"),\n",
    "                        'title': json_data.get('title'),\n",
    "                        'chapter': page_data.get('chapter'),\n",
    "                        'page': page_data.get('page'),\n",
    "                        \"qna_type\": analyze_extracted_qna(qna_item),\n",
    "                        \"qna_domain\": \"\",\n",
    "                        'qna_data': qna_item,\n",
    "                        'additional_tags_found': additional_tags,\n",
    "                        'additional_tag_data': additional_tag_data\n",
    "                    })\n",
    "            \n",
    "            # 인덱스를 역순으로 정렬하여 제거 (뒤에서부터 제거)\n",
    "            sorted_indices = sorted(indices_to_remove, reverse=True)\n",
    "            for idx in sorted_indices:\n",
    "                if 0 <= idx < len(add_info):\n",
    "                    add_info.pop(idx)\n",
    "            \n",
    "            # 추출된 Q&A들을 리스트에 추가\n",
    "            extracted_qna.extend(qna_items_to_extract)\n",
    "\n",
    "            # 수정된 add_info로 업데이트\n",
    "            page_data['add_info'] = add_info\n",
    "            page_data['page_contents'] = re.sub(r'\\{q_\\d{4}_\\d{4}\\}', \"\", page_contents)\n",
    "            page_data['page_contents'] = page_data['page_contents'].replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # 정제 끝나고 빈 페이지 삭제\n",
    "    pages_to_remove = []\n",
    "    for i, page_data in enumerate(json_data.get('contents', [])):\n",
    "        page_contents = page_data.get('page_contents', '')\n",
    "        if page_contents.strip() == \"\":\n",
    "            pages_to_remove.append(i)\n",
    "    \n",
    "    # 역순으로 제거\n",
    "    for i in reversed(pages_to_remove):\n",
    "        json_data['contents'].pop(i)\n",
    "    \n",
    "    return {\n",
    "        'modified_json': json_data,\n",
    "        'extracted_qna': extracted_qna\n",
    "    }\n",
    "\n",
    "def process_json_file(file_path: str, output_path: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    JSON 파일을 처리하여 Q&A 태그를 추출하고 분리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        file_path: 입력 JSON 파일 경로\n",
    "        output_path: 출력 JSON 파일 경로 (None이면 원본 파일 덮어쓰기)\n",
    "        \n",
    "    Returns:\n",
    "        처리 결과\n",
    "    \"\"\"\n",
    "    # JSON 파일 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Q&A 태그 추출 및 분리\n",
    "    result = extract_qna_tags(json_data)\n",
    "    \n",
    "    # 수정된 JSON 저장\n",
    "    output_file = output_path if output_path else file_path\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result['modified_json'], f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "\n",
    "    # 추출된 Q&A를 별도 파일로 저장\n",
    "    if len(result['extracted_qna']) != 0:\n",
    "        qna_output_path = output_path.replace('.json', '_extracted_qna.json')\n",
    "        with open(qna_output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result['extracted_qna'], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # analyze_extracted_qna(qna_output_path)\n",
    "        return result\n",
    "    else:\n",
    "        qna_output_path = \"\"\n",
    "    \n",
    "    print(f\"처리 완료:\")\n",
    "    print(f\"- 수정된 JSON: {output_file}\")\n",
    "    # print(f\"- 추출된 Q&A: {qna_output_path}\")\n",
    "    if len(result['extracted_qna']) > 0:\n",
    "        print(f\"- 추출된 Q&A 개수: {len(result['extracted_qna'])}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 25개의 JSON 파일을 찾았습니다.\n",
      "\n",
      "[1/25] 처리 중: 356500908.json\n",
      "\n",
      "[2/25] 처리 중: 351876680.json\n",
      "\n",
      "[3/25] 처리 중: 362874088.json\n",
      "\n",
      "[4/25] 처리 중: 342761174.json\n",
      "\n",
      "[5/25] 처리 중: 361988234.json\n",
      "\n",
      "[6/25] 처리 중: 342761055.json\n",
      "\n",
      "[7/25] 처리 중: 360314281.json\n",
      "\n",
      "[8/25] 처리 중: 339516636.json\n",
      "\n",
      "[9/25] 처리 중: 349719489.json\n",
      "\n",
      "[10/25] 처리 중: 349719549.json\n",
      "\n",
      "[11/25] 처리 중: 343261092.json\n",
      "\n",
      "[12/25] 처리 중: 353738691.json\n",
      "\n",
      "[13/25] 처리 중: 349857344.json\n",
      "\n",
      "[14/25] 처리 중: 360844783.json\n",
      "\n",
      "[15/25] 처리 중: 349995715.json\n",
      "\n",
      "[16/25] 처리 중: 290155840.json\n",
      "\n",
      "[17/25] 처리 중: 337872889.json\n",
      "\n",
      "[18/25] 처리 중: 359837661.json\n",
      "\n",
      "[19/25] 처리 중: 343630674.json\n",
      "\n",
      "[20/25] 처리 중: 343001569.json\n",
      "\n",
      "[21/25] 처리 중: 357094267.json\n",
      "\n",
      "[22/25] 처리 중: 342460463.json\n",
      "\n",
      "[23/25] 처리 중: 357314428.json\n",
      "\n",
      "[24/25] 처리 중: 328073930.json\n",
      "\n",
      "[25/25] 처리 중: 364690338.json\n",
      "\n",
      "=== 전체 처리 결과 ===\n",
      "- 처리된 파일: 25/25\n",
      "- 총 추출된 Q&A: 22601개\n"
     ]
    }
   ],
   "source": [
    "# 모든 JSON 파일 일괄 처리\n",
    "print(f\"총 {len(json_files)}개의 JSON 파일을 찾았습니다.\")\n",
    "\n",
    "total_extracted = 0\n",
    "processed_files = 0\n",
    "\n",
    "for i, json_file in enumerate(json_files):\n",
    "    try:\n",
    "        print(f\"\\n[{i+1}/{len(json_files)}] 처리 중: {os.path.basename(json_file)}\")\n",
    "        \n",
    "        # 파일 처리\n",
    "        name = os.path.basename(json_file)\n",
    "        result = process_json_file(json_file, os.path.join(new_data_dir, name))\n",
    "        \n",
    "        total_extracted += len(result['extracted_qna'])\n",
    "        processed_files += 1\n",
    "        \n",
    "        # print(f\"  - 추출된 Q&A: {len(result['extracted_qna'])}개\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - 오류 발생: {e}\")        \n",
    "\n",
    "print(f\"\\n=== 전체 처리 결과 ===\")\n",
    "print(f\"- 처리된 파일: {processed_files}/{len(json_files)}\")\n",
    "print(f\"- 총 추출된 Q&A: {total_extracted}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinym/Library/CloudStorage/OneDrive-개인/데이터L/selectstar/data/FINAL/1C_0910/Lv3_4/353302817/353302817.json\n"
     ]
    }
   ],
   "source": [
    "for j in json_files:\n",
    "    if '353302817' in j:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 24, 56)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_files), len(Lv2_isbn_id), len(Lv3_isbn_id)\n",
    "\n",
    "# set(list(Lv2_isbn_id.index) + list(Lv3_isbn_id.index)) - set(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 단일 파일 처리\n",
    "# json_file = j\n",
    "# name = os.path.basename(json_file)\n",
    "# process_json_file(json_file, os.path.join(new_data_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
